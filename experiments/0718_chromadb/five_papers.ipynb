{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8b55ea",
   "metadata": {},
   "source": [
    "### Example Code for Creating and Querying a ChromaDB with Chunks Generated from Yourbench\n",
    "Adapted from UDA-benchmark. Using /tmp/zlu39/.conda_envs/UDA-benchmark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 5/5 [00:00<00:00, 573.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document_id', 'document_text', 'document_filename', 'document_metadata', 'raw_chunk_summaries', 'chunk_summaries', 'raw_document_summary', 'document_summary', 'summarization_model', 'chunks', 'multihop_chunks', 'chunk_info_metrics', 'chunking_model'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "dataset = load_dataset('zhichul/0718_yourbench_five_papers', name='chunked', download_mode=DownloadMode.FORCE_REDOWNLOAD)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6abfeeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "text_chunks = []\n",
    "for chunks_by_doc in dataset['train']['chunks']:\n",
    "    for chunk in chunks_by_doc:\n",
    "        text_chunks.append(chunk['chunk_text'])\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70963743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# Create the vector_db collection \n",
    "# and store the embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "chroma_client = chromadb.Client()\n",
    "device_info = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=model_name, device=device_info\n",
    ")\n",
    "collection = chroma_client.create_collection(\n",
    "    \"demo_vdb\", embedding_function=ef, metadata={\"hnsw:space\": \"cosine\"}, get_or_create=True\n",
    ")\n",
    "id_list = [str(i) for i in range(len(text_chunks))]\n",
    "collection.add(documents=text_chunks, ids=id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0128e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant contexts to the question: What is the goal of synthetic continued pretraining?\n",
      "===== Context 1 =======\n",
      "; Schumann & Rehbein, 2019). Contemporary works employ co-training (Lang et al. , 2022) and self-training to improve language model performance, often on mathematical reasoning tasks (Huang et al. , 2023; Gulcehre et al. , 2023; Zhang et al. , 2024a), or synthesize input-output pairs for instruction tuning, usually by con- ditioning on a curated seed set (Wang et al. , 2023b; Honovich et al. , 2023; Taori et al. , 2023; Peng et al. , 2023; Yuan et al. , 2024b; Li et al. , 2024). Continual learning and pretraining. Continual learning is rooted in historical work on connec- tionist networks (McCloskey & Cohen, 1989; Ratcliff, 1990) and considers learning with tasks ar- riving in an online manner (Schlimmer & Fisher, 1986; Grossberg, 2012). The main focus is on mitigating a neural net’s “catastrophic forgetting” of previously encountered tasks (Robins, \n",
      "===== Context 2 =======\n",
      " 2. Related Works  Post-training. Post-training is crucial for enhancing model performance (Zhang et al. , 2022; Hoffmann et al. , 2023; OpenAI, 2023b; Google, 2023; Touvron et al. , 2023). This stage commonly utilizes large-scale supervised fine- tuning (SFT) (Radford et al. , 2018; Brown et al. , 2020; Radford et al. , 2021; Wei et al. , 2022a; Chung et al. , 2022; Zhou et al. , 2024a) and/or reinforcement learning (RL) (Ziegler et al. , 2019; Ouyang et al. , 2022; Sun et al. , 2024; Abdulhai et al. , 2023; Zhou et al. , 2024b; Zhai et al. ,  2  In-DistributionOut-of-DistributionSFTRL\fSFT Memorizes, RL Generalizes  models to generate intermediate reasoning steps and extend the responses before producing a final answer. Subsequent work (Zelikman et al. , 2022; Feng\n",
      "===== Context 3 =======\n",
      " learning without incurring the high compute costs of pretraining from scratch. Specifically, we assume access to a collection of 265 books totaling 1. 3M tokens. Our task is to synthesize a corpus such that continued pretraining on it enables a model to answer queries (e. g. , multiple-choice QA or user instructions related to the book content) without access to the source texts. In our main experiments (§5), we use EntiGraph to generate 455M synthetic tokens from 1. 3M real tokens using GPT-4 (OpenAI et al. , 2024). Then, we continually pretrain Llama 3 8B (Dubey et al. , 2024) on the synthetic tokens and evaluate its QA accuracy on the QuALITY questions. We observe log-linear scaling in the accuracy as synthetic token count increases, up to 455M (§4. 2). At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy gain of having the source documents available at inference time (§5). Beyond QA, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions\n",
      "===== Context 4 =======\n",
      "  Articles & Books  8B  Table 1: Comparing the scale of modern continued pretraining (CPT) works with our small corpus setting. Prior work adapts LMs to broad domains with diverse, large-scale corpora. We aim to downscale CPT to small corpora; we use a corpus that is 10,000× smaller than the smallest modern corpus for domain-adaptive CPT. pretraining on the synthetic corpus. In this section, we first outline this problem setting and our evaluation approach in more detail (§2. 1). Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph (§2. 2). 2. 1 PROBLEM SETUP  Continued pretraining on small corpora. We focus on approaches that continually pretrain an LM to teach it the knowledge of a small source corpus Dsource. These approaches acquire “parametric knowledge”—the knowledge of Dsource is learned in the LM’s parameters, as in pretraining. Synthetic continued pretraining (synthetic CPT). First, we apply a synthetic data generation algorithm Asynth to convert a small corpus Dsource into a synthetic corpus Dsynth:  Asynth : Dsource (cid:\n",
      "===== Context 5 =======\n",
      " Graph. To test this, we randomly subsam- ple without replacement the EntiGraph corpus with varying sample sizes, continually pretrain Llama 3 8B Base on each subsample, and plot accuracy versus sample size in Figure 2. We observe log-linear scaling of the accuracy in the number of synthetic tokens used for CPT, up to 455M tokens. We mathematically investigate the scaling properties of EntiGraph in §6. In broad strokes, we postulate that QuALITY ac- curacy follows a mixture-of-exponential shape with three stages: (i) linear growth, (ii) log- linear growth, and (iii) asymptotic plateau. Figure 2: Accuracy on the QuALITY question set Qtest (y-axis) as a function of the synthetic token count (x- axis). The accuracy of synthetic continued pretraining using the EntiGraph data augmentation algorithm (Enti- Graph CPT) scales log-linearly up to 455M tokens. Comparison with baselines. Raw CPT (green line) underperforms even Llama 3 8B (dashed black line). We postulate two explanations: (i) The Raw corpus follows a narrower, different distribution than the L\n"
     ]
    }
   ],
   "source": [
    "# Fetch the top_k most similar chunks according to the query\n",
    "top_k = 5\n",
    "question = \"What is the goal of synthetic continued pretraining?\"\n",
    "fetct_res = collection.query(query_texts=[question], n_results=top_k, include=['metadatas', 'documents', 'distances', 'embeddings'])\n",
    "contexts = fetct_res[\"documents\"][0]\n",
    "\n",
    "# Show a snapshot of the context\n",
    "print(f\"The most relevant contexts to the question: {question}\")\n",
    "for idx,context in enumerate(contexts):\n",
    "    print(f\"===== Context {idx+1} =======\")\n",
    "    print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9099aec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['163', '395', '53', '61', '71']],\n",
       " 'embeddings': [array([[-0.04351589, -0.06309311, -0.02049338, ...,  0.05304797,\n",
       "           0.0011077 ,  0.00092487],\n",
       "         [-0.05412519, -0.06544636,  0.01237015, ..., -0.01653033,\n",
       "          -0.04229851,  0.01795014],\n",
       "         [-0.09530476, -0.07220636, -0.02631474, ..., -0.06363436,\n",
       "          -0.00936357,  0.02696936],\n",
       "         [-0.03810095, -0.08134458,  0.05342281, ..., -0.03218824,\n",
       "           0.00582854,  0.04326789],\n",
       "         [-0.0706144 , -0.09598251,  0.0337091 , ..., -0.09405132,\n",
       "          -0.04735962, -0.07118285]], shape=(5, 384))],\n",
       " 'documents': [['; Schumann & Rehbein, 2019). Contemporary works employ co-training (Lang et al. , 2022) and self-training to improve language model performance, often on mathematical reasoning tasks (Huang et al. , 2023; Gulcehre et al. , 2023; Zhang et al. , 2024a), or synthesize input-output pairs for instruction tuning, usually by con- ditioning on a curated seed set (Wang et al. , 2023b; Honovich et al. , 2023; Taori et al. , 2023; Peng et al. , 2023; Yuan et al. , 2024b; Li et al. , 2024). Continual learning and pretraining. Continual learning is rooted in historical work on connec- tionist networks (McCloskey & Cohen, 1989; Ratcliff, 1990) and considers learning with tasks ar- riving in an online manner (Schlimmer & Fisher, 1986; Grossberg, 2012). The main focus is on mitigating a neural net’s “catastrophic forgetting” of previously encountered tasks (Robins, ',\n",
       "   ' 2. Related Works  Post-training. Post-training is crucial for enhancing model performance (Zhang et al. , 2022; Hoffmann et al. , 2023; OpenAI, 2023b; Google, 2023; Touvron et al. , 2023). This stage commonly utilizes large-scale supervised fine- tuning (SFT) (Radford et al. , 2018; Brown et al. , 2020; Radford et al. , 2021; Wei et al. , 2022a; Chung et al. , 2022; Zhou et al. , 2024a) and/or reinforcement learning (RL) (Ziegler et al. , 2019; Ouyang et al. , 2022; Sun et al. , 2024; Abdulhai et al. , 2023; Zhou et al. , 2024b; Zhai et al. ,  2  In-DistributionOut-of-DistributionSFTRL\\x0cSFT Memorizes, RL Generalizes  models to generate intermediate reasoning steps and extend the responses before producing a final answer. Subsequent work (Zelikman et al. , 2022; Feng',\n",
       "   ' learning without incurring the high compute costs of pretraining from scratch. Specifically, we assume access to a collection of 265 books totaling 1. 3M tokens. Our task is to synthesize a corpus such that continued pretraining on it enables a model to answer queries (e. g. , multiple-choice QA or user instructions related to the book content) without access to the source texts. In our main experiments (§5), we use EntiGraph to generate 455M synthetic tokens from 1. 3M real tokens using GPT-4 (OpenAI et al. , 2024). Then, we continually pretrain Llama 3 8B (Dubey et al. , 2024) on the synthetic tokens and evaluate its QA accuracy on the QuALITY questions. We observe log-linear scaling in the accuracy as synthetic token count increases, up to 455M (§4. 2). At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy gain of having the source documents available at inference time (§5). Beyond QA, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions',\n",
       "   '  Articles & Books  8B  Table 1: Comparing the scale of modern continued pretraining (CPT) works with our small corpus setting. Prior work adapts LMs to broad domains with diverse, large-scale corpora. We aim to downscale CPT to small corpora; we use a corpus that is 10,000× smaller than the smallest modern corpus for domain-adaptive CPT. pretraining on the synthetic corpus. In this section, we first outline this problem setting and our evaluation approach in more detail (§2. 1). Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph (§2. 2). 2. 1 PROBLEM SETUP  Continued pretraining on small corpora. We focus on approaches that continually pretrain an LM to teach it the knowledge of a small source corpus Dsource. These approaches acquire “parametric knowledge”—the knowledge of Dsource is learned in the LM’s parameters, as in pretraining. Synthetic continued pretraining (synthetic CPT). First, we apply a synthetic data generation algorithm Asynth to convert a small corpus Dsource into a synthetic corpus Dsynth:  Asynth : Dsource (cid:',\n",
       "   ' Graph. To test this, we randomly subsam- ple without replacement the EntiGraph corpus with varying sample sizes, continually pretrain Llama 3 8B Base on each subsample, and plot accuracy versus sample size in Figure 2. We observe log-linear scaling of the accuracy in the number of synthetic tokens used for CPT, up to 455M tokens. We mathematically investigate the scaling properties of EntiGraph in §6. In broad strokes, we postulate that QuALITY ac- curacy follows a mixture-of-exponential shape with three stages: (i) linear growth, (ii) log- linear growth, and (iii) asymptotic plateau. Figure 2: Accuracy on the QuALITY question set Qtest (y-axis) as a function of the synthetic token count (x- axis). The accuracy of synthetic continued pretraining using the EntiGraph data augmentation algorithm (Enti- Graph CPT) scales log-linearly up to 455M tokens. Comparison with baselines. Raw CPT (green line) underperforms even Llama 3 8B (dashed black line). We postulate two explanations: (i) The Raw corpus follows a narrower, different distribution than the L']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances', 'embeddings'],\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None, None, None]],\n",
       " 'distances': [[0.5951083302497864,\n",
       "   0.6163771748542786,\n",
       "   0.6247628331184387,\n",
       "   0.6263489723205566,\n",
       "   0.6367581486701965]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetct_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db5366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
