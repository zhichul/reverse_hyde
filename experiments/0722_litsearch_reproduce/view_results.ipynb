{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0eb3de",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Basically there's two important things needed to reproduce the numbers\n",
    "* use the same chunk format\n",
    "* use the same instruction prompt for gritlm\n",
    "* deduplicate the corpus, taking last entry as canonical!\n",
    "* use exact NN search (no chromadb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d38b9556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57657\n",
      "6197\n",
      "{1, 2, 3, 4, 5, 6, 8, 6197}\n",
      "BM25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3ab0e5ff404b0790de067e1a0a7ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall\n",
      "recall@5: 0.4380792853154662\n",
      "recall@20: 0.5793969849246231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c27592d45b46abb8e75ea4c68f2f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inline specific\n",
      "recall@5: 0.3852813852813853\n",
      "recall@20: 0.5584415584415584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9594b0b86c74190bf5959facaa50955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inline broad\n",
      "recall@5: 0.22944444444444442\n",
      "recall@20: 0.37416666666666665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bb61b448964437b2dfaad8117864d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author specific\n",
      "recall@5: 0.6255924170616114\n",
      "recall@20: 0.7345971563981043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d76f2772544a639978b995ba772002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author broad\n",
      "recall@5: 0.37142857142857144\n",
      "recall@20: 0.4857142857142857\n",
      "GRITLM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c795873e8442fbb8719f5b1b8d32a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall\n",
      "recall@5: 0.6913456169737577\n",
      "recall@20: 0.8001395868230039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24749ed6a7041cdad1d6a45ead35690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inline specific\n",
      "recall@5: 0.6774891774891775\n",
      "recall@20: 0.7792207792207793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435531d786bd4d93bbdd7694d9450ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inline broad\n",
      "recall@5: 0.5269444444444444\n",
      "recall@20: 0.6973611111111111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98000788f4d4472aa757e246177e49bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author specific\n",
      "recall@5: 0.8246445497630331\n",
      "recall@20: 0.8909952606635071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c2ee488a8d4afe9003ff68b8f6f701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author broad\n",
      "recall@5: 0.5428571428571428\n",
      "recall@20: 0.7428571428571429\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "corpus_clean_data = load_dataset('princeton-nlp/LitSearch', \"corpus_clean\", split=\"full\")\n",
    "corpus_clean_data_with_assembled_title_abstract = corpus_clean_data.map(lambda x: {'chunk': f\"Title: {x['title']}\\nAbstract: {x['abstract']}\"})\n",
    "\n",
    "doc_2_ids = defaultdict(list)\n",
    "for line in corpus_clean_data_with_assembled_title_abstract:\n",
    "    doc_2_ids[line['chunk']].append(line['corpusid'])\n",
    "\n",
    "multiplicity = {val[-1]: len(val) for val in doc_2_ids.values()}\n",
    "print(len(multiplicity))\n",
    "print(max(multiplicity.values()))\n",
    "print(set(len(val) for val in doc_2_ids.values()))\n",
    "def recall(predictions, relevant_set, expand_multiplicity=False):\n",
    "    if expand_multiplicity:\n",
    "        total = len(predictions)\n",
    "        predictions = [pred for pred in predictions for _ in range(multiplicity[pred])]\n",
    "        if len(predictions) > total:\n",
    "            pass\n",
    "        predictions = predictions[:total]\n",
    "    relevant_count = 0\n",
    "    recall_scores = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        relevant_count += (pred in relevant_set)\n",
    "        recall_scores.append(relevant_count / len(relevant_set))\n",
    "    return np.array(recall_scores)\n",
    "\n",
    "def print_recall_scores(recall_scores, ks=None):\n",
    "    if ks is None:\n",
    "        ks = list(range(1, len(recall_scores) + 1))\n",
    "    for k in ks:\n",
    "        print(f'recall@{k}: {recall_scores[k-1]}')\n",
    "\n",
    "retrieval_results = load_dataset('json', data_files='LitSearch/results/retrieval/LitSearch.title_abstract.bm25.jsonl', split='train')\n",
    "\n",
    "print(\"BM25\")\n",
    "# overall\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"overall\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 0 or 'inline' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"inline specific\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 1 or 'inline' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"inline broad\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 0 or 'manual' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"author specific\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 1 or 'manual' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"author broad\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "\n",
    "\n",
    "retrieval_results = load_dataset('json', data_files='LitSearch/results/retrieval/LitSearch.title_abstract.grit.jsonl', split='train')\n",
    "print(\"GRITLM\")\n",
    "# overall\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"overall\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 0 or 'inline' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"inline specific\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 1 or 'inline' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"inline broad\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 0 or 'manual' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"author specific\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n",
    "\n",
    "recall_scores_all = []\n",
    "for i in tqdm(range(len(retrieval_results))):\n",
    "    if retrieval_results[i]['quality'] == 0 or retrieval_results[i]['specificity'] == 1 or 'manual' not in retrieval_results[i]['query_set']: continue\n",
    "    result = retrieval_results[i]\n",
    "    corpus_ids = retrieval_results[i]['retrieved']\n",
    "    recall_scores = recall(corpus_ids, set(retrieval_results[i]['corpusids']))\n",
    "    recall_scores_all.append(recall_scores)\n",
    "recall_scores_all = np.vstack(recall_scores_all).mean(axis=0)\n",
    "print(\"author broad\")\n",
    "print_recall_scores(recall_scores_all, ks=[5,20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7ce36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
