<!doctype html>
<html lang="en"><head></head><body>```html



<meta charset="utf-8">
<title>Prompt‑HyDE playground</title>
<style>
  body{font-family:system-ui,sans-serif;margin:0;padding:1rem 2rem;}
  h2{margin-top:2rem;}
  label{display:block;margin-top:.75rem;font-weight:600;}
  select,input[type=text],input[type=number],textarea{width:100%;padding:.4rem;}
  textarea{height:120px;font-family:monospace;}
  button{margin-top:1rem;padding:.5rem 1rem;cursor:pointer;}
  table{border-collapse:collapse;width:100%;margin-top:.5rem;}
  th,td{border:1px solid #ccc;padding:.25rem .5rem;text-align:left;vertical-align:top;}
  em.rev{font-style:italic;color:#9146ff;}
  strong.rel{color:#0b63ff;font-weight:700;}
  .flex{display:flex;gap:2rem;flex-wrap:wrap;}
  .card{flex:1 1 320px;border:1px solid #ddd;padding:1rem;}
  .small{font-size:.85rem;color:#555;margin:.25rem 0;}

  /* tooltip for abstracts */
  .has-abs{position:relative;}
  .has-abs:hover::after{
    content:attr(data-abs);
    position:absolute;left:0;top:100%;z-index:999;
    max-width:420px;white-space:pre-wrap;font-size:.8rem;
    background:#333;color:#fff;padding:.5rem;border-radius:.25rem;
    box-shadow:0 2px 6px rgba(0,0,0,.35);
  }

  /* tab styling */
  .tabs{display:flex;gap:.5rem;margin-bottom:.5rem;}
  .tabs button{padding:.25rem .75rem;border:1px solid #aaa;background:#eee;}
  .tabs button.active{background:#fff;border-bottom:none;font-weight:600;}
  .tab-body{display:none;}
  .tab-body.active{display:block;}
</style>


<h1>Prompt‑HyDE UI (vanilla&nbsp;JS)</h1>

<!-- CONFIG -------------------------------------------------------------->
<section class="card">
  <h2>Server configuration</h2>
  <pre id="cfg" class="small">{
  "embed_model": "grit",
  "index_path": "../0721_litsearch_example/faiss/litsearch.index",
  "backend": "faiss",
  "query_dataset": "../0721_litsearch_example/query_with_score.parquet",
  "corpus_dataset": "../0721_litsearch_example/corpus_clean_dedup.parquet",
  "prompt_dir": "prompts",
  "extractor_dir": "extractors",
  "annotation_dir": "annotations",
  "ui_config": "configs/ui_config.json",
  "host": "0.0.0.0",
  "port": 8200,
  "id_field": "corpusid",
  "relevant_documents_field": "corpusids"
}</pre>
</section>

<section class="flex">
  <!-- DATA CARD -->
  <div class="card" style="max-width:440px;">
    <h2>Data</h2>

    <label>Query</label>
    <select id="querySel"><option value="0">0: [inline_acl] sp=0 qual=2 gr=0 — Are there any research papers on methods to compre…</option><option value="3">3: [inline_acl] sp=1 qual=2 gr=0 — Are there any tools or studies that have focused o…</option><option value="4">4: [inline_acl] sp=1 qual=2 gr=0 — Are there papers that propose contextualized calib…</option><option value="10">10: [inline_acl] sp=1 qual=2 gr=0 — Can you point me to a work that uses diagnostic to…</option><option value="11">11: [inline_acl] sp=0 qual=1 gr=0 — Can you point me to studies discussing methods for…</option><option value="17">17: [inline_acl] sp=1 qual=2 gr=0 — Can you recommend a paper that uses an NLI model f…</option><option value="24">24: [inline_acl] sp=0 qual=1 gr=0 — Can you suggest recent studies that have integrate…</option><option value="30">30: [inline_acl] sp=0 qual=1 gr=0 — Could you point me to studies that have investigat…</option><option value="35">35: [inline_acl] sp=1 qual=2 gr=0 — Could you suggest a paper that introduces an appro…</option><option value="39">39: [inline_acl] sp=1 qual=2 gr=0 — I am looking for research that has explored topic …</option><option value="44">44: [inline_acl] sp=1 qual=2 gr=0 — I'm exploring research that utilizes large dataset…</option><option value="49">49: [inline_acl] sp=1 qual=2 gr=0 — I'm looking for innovative approaches to data anno…</option><option value="55">55: [inline_acl] sp=1 qual=2 gr=0 — In discourse parsing literature, which works have …</option><option value="56">56: [inline_acl] sp=1 qual=2 gr=0 — In researching metrics for human-interaction with …</option><option value="58">58: [inline_acl] sp=1 qual=2 gr=0 — In the context of Named Entity Recognition tasks a…</option><option value="59">59: [inline_acl] sp=0 qual=1 gr=0 — In the context of machine translation, can you poi…</option><option value="61">61: [inline_acl] sp=1 qual=2 gr=0 — In the context of simultaneous machine translation…</option><option value="62">62: [inline_acl] sp=1 qual=2 gr=0 — In the field of reinforcement learning models for …</option><option value="63">63: [inline_acl] sp=1 qual=2 gr=0 — What approaches have been used to address the limi…</option><option value="71">71: [inline_acl] sp=1 qual=2 gr=0 — What are the recent developments in evaluating the…</option><option value="75">75: [inline_acl] sp=0 qual=1 gr=0 — What research could I reference to understand the …</option><option value="84">84: [inline_acl] sp=1 qual=2 gr=0 — Where can I find a multilingual corpus that includ…</option><option value="109">109: [inline_nonacl] sp=1 qual=2 gr=0 — Can you recommend research that uses an LLM to gen…</option><option value="110">110: [inline_nonacl] sp=0 qual=1 gr=0 — Can you show me a paper that built a large structu…</option><option value="111">111: [inline_nonacl] sp=1 qual=2 gr=0 — Can you suggest research that deals with the multi…</option><option value="119">119: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that examines how inco…</option><option value="123">123: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that explores how lang…</option><option value="124">124: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores mitigati…</option><option value="126">126: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores the diff…</option><option value="128">128: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that initializes embed…</option><option value="132">132: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that investigates enha…</option><option value="135">135: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="136">136: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates how …</option><option value="137">137: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="142">142: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates the …</option><option value="143">143: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates the …</option><option value="145">145: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that uses feedback-dri…</option><option value="149">149: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research papers that investiga…</option><option value="155">155: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that evaluates the pe…</option><option value="157">157: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that examines how dec…</option><option value="163">163: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that explores how the…</option><option value="168">168: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates app…</option><option value="169">169: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates gen…</option><option value="176">176: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates the…</option><option value="179">179: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research that proposed enhanci…</option><option value="194">194: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest a study that explores data annot…</option><option value="195">195: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that explores employing …</option><option value="197">197: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest a study that explores the idea o…</option><option value="199">199: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that investigates the in…</option><option value="205">205: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest papers that tackle conversationa…</option><option value="208">208: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that concentrates on pi…</option><option value="209">209: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines a system …</option><option value="213">213: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that examines how stran…</option><option value="221">221: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines the effec…</option><option value="224">224: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores generatin…</option><option value="226">226: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores the idea …</option><option value="228">228: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that includes an online…</option><option value="231">231: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that investigates apply…</option><option value="238">238: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that investigates how c…</option><option value="244">244: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest research that investigates the u…</option><option value="251">251: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest studies focused on emotion-class…</option><option value="252">252: [inline_nonacl] sp=1 qual=2 gr=0 — Has any research explored using other off-the-shel…</option><option value="260">260: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research efforts been made to gather dial…</option><option value="261">261: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers been published on models …</option><option value="265">265: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers examined whether using la…</option><option value="269">269: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research papers investigated the creation…</option><option value="272">272: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers tried to create conversat…</option><option value="278">278: [inline_nonacl] sp=1 qual=2 gr=0 — I know about prompt tuning, but have any works tri…</option><option value="297">297: [inline_nonacl] sp=1 qual=2 gr=0 — What are some scholarly articles that explore the …</option><option value="306">306: [inline_nonacl] sp=0 qual=1 gr=0 — What papers discuss the effect of false negatives …</option><option value="307">307: [inline_nonacl] sp=1 qual=2 gr=0 — What papers explore replacing schema linking with …</option><option value="311">311: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists comparing adapter-based tunin…</option><option value="312">312: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists on employing generative model…</option><option value="325">325: [inline_nonacl] sp=0 qual=1 gr=0 — What research is available on acquiring sentence e…</option><option value="326">326: [inline_nonacl] sp=0 qual=2 gr=0 — What research is available on hybrid approaches th…</option><option value="331">331: [inline_nonacl] sp=0 qual=1 gr=0 — What techniques and frameworks have been suggested…</option><option value="346">346: [inline_nonacl] sp=1 qual=1 gr=0 — Which work shows that only emplying instance-level…</option><option value="347">347: [inline_nonacl] sp=1 qual=2 gr=0 — Which work shows that reducing the number of train…</option><option value="353">353: [manual_acl] sp=1 qual=2 gr=0 — Are there any papers that build dense retrievers w…</option><option value="360">360: [manual_acl] sp=1 qual=2 gr=0 — If one would like to train (or evaluate) a helpful…</option><option value="364">364: [manual_acl] sp=1 qual=2 gr=0 — Is there a decoder-only language model that does n…</option><option value="366">366: [manual_acl] sp=1 qual=2 gr=0 — Is there a method for measuring the critical error…</option><option value="369">369: [manual_acl] sp=0 qual=1 gr=0 — Is there a paper exploring the curse of multilingu…</option><option value="372">372: [manual_acl] sp=0 qual=2 gr=0 — Is there a paper that links exposure bias to disti…</option><option value="378">378: [manual_acl] sp=1 qual=2 gr=0 — Is there a paper that uses similarity scores to ch…</option><option value="382">382: [manual_acl] sp=1 qual=2 gr=0 — Is there a tool that can automatically segment spe…</option><option value="389">389: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that aligns speech and text emb…</option><option value="395">395: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that constructs augmented train…</option><option value="404">404: [manual_acl] sp=0 qual=2 gr=0 — Is there any paper that proposes a set of criteria…</option><option value="411">411: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that utilizes Gaussian processe…</option><option value="414">414: [manual_acl] sp=1 qual=1 gr=0 — Is there any research paper that can extract attri…</option><option value="417">417: [manual_acl] sp=1 qual=2 gr=0 — Is there any works that explores how to achieve ba…</option><option value="423">423: [manual_acl] sp=0 qual=2 gr=0 — What are some methods for solving the class-increm…</option><option value="426">426: [manual_acl] sp=0 qual=1 gr=0 — What is the performance of large language models i…</option><option value="433">433: [manual_acl] sp=1 qual=2 gr=0 — Which is the first multimodal model combining text…</option><option value="451">451: [manual_acl] sp=1 qual=2 gr=0 — Which paper first constructed a structured knowled…</option><option value="458">458: [manual_acl] sp=1 qual=1 gr=0 — Which paper first proposed shared adapter module a…</option><option value="467">467: [manual_acl] sp=0 qual=1 gr=0 — Which paper found that mutual learning benefits mu…</option><option value="473">473: [manual_acl] sp=1 qual=1 gr=0 — Which paper is among the earliest to train on exte…</option><option value="475">475: [manual_acl] sp=0 qual=1 gr=0 — Which paper makes sure that the questions used in …</option><option value="498">498: [manual_acl] sp=1 qual=2 gr=0 — Which research paper leverages event structure inf…</option><option value="502">502: [manual_acl] sp=1 qual=2 gr=0 — Which work discusses an analysis of source and tar…</option><option value="510">510: [manual_iclr] sp=1 qual=2 gr=0 — Are there any papers that use a world model for pl…</option><option value="522">522: [manual_iclr] sp=1 qual=2 gr=0 — Is there a parameter-efficient fine-tuning method …</option><option value="550">550: [manual_iclr] sp=1 qual=1 gr=0 — What paper first showed that you can score the cod…</option><option value="557">557: [manual_iclr] sp=0 qual=1 gr=0 — What paper proposes breaking down programming prob…</option><option value="560">560: [manual_iclr] sp=1 qual=1 gr=0 — What research first proposed a new kind of cascade…</option><option value="564">564: [manual_iclr] sp=1 qual=2 gr=0 — Which backdoor paper first used the CLIP to suppre…</option><option value="566">566: [manual_iclr] sp=1 qual=1 gr=0 — Which is one of the first papers to highlight and …</option><option value="595">595: [manual_iclr] sp=0 qual=2 gr=0 — What paper provides generalization bounds for self…</option><option value="18">18: [inline_acl] sp=0 qual=2 gr=0.25 — Can you recommend some literature that focuses on …</option><option value="2">2: [inline_acl] sp=0 qual=2 gr=0.5 — Are there any studies that explore post-hoc techni…</option><option value="12">12: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me to studies that explore the impac…</option><option value="13">13: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me towards research on contrastive l…</option><option value="53">53: [inline_acl] sp=1 qual=2 gr=0.5 — I'm researching on the efficacy of recurrent netwo…</option><option value="74">74: [inline_acl] sp=1 qual=2 gr=0.5 — What prior works suggested that exposure bias coul…</option><option value="92">92: [inline_acl] sp=0 qual=1 gr=0.5 — Where might I find research on the evaluation of c…</option><option value="102">102: [inline_nonacl] sp=1 qual=2 gr=0.5 — Are there any studies investigating example-based …</option><option value="113">113: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you direct me to studies investigating the e…</option><option value="150">150: [inline_nonacl] sp=0 qual=1 gr=0.5 — Could you recommend research that analyses prompt …</option><option value="156">156: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you recommend research that examines how an …</option><option value="235">235: [inline_nonacl] sp=1 qual=1 gr=0.5 — Could you suggest research that investigates enhan…</option><option value="257">257: [inline_nonacl] sp=0 qual=2 gr=0.5 — Have any new metrics been developed to assess the …</option><option value="328">328: [inline_nonacl] sp=0 qual=2 gr=0.5 — What research should I consult regarding the appli…</option><option value="334">334: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques exist to enhance the few-shot fine…</option><option value="335">335: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques have been investigated to enhance …</option><option value="96">96: [inline_acl] sp=0 qual=2 gr=0.6 — Which studies should I look into that have explore…</option><option value="8">8: [inline_acl] sp=0 qual=1 gr=0.6666666666666666 — Can you list some publications that discuss the ev…</option><option value="23">23: [inline_acl] sp=0 qual=2 gr=0.6666666666666666 — Can you suggest literature on enhanced semantic pa…</option><option value="1">1: [inline_acl] sp=1 qual=2 gr=1 — Are there any resources available for translating …</option><option value="5">5: [inline_acl] sp=1 qual=2 gr=1 — Are there studies that combine convolutional and r…</option><option value="6">6: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to research that explores method…</option><option value="7">7: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to studies that explore techniqu…</option><option value="9">9: [inline_acl] sp=0 qual=2 gr=1 — Can you point me to a paper that discussed transfo…</option><option value="14">14: [inline_acl] sp=0 qual=1 gr=1 — Can you point to studies or tasks focused on detec…</option><option value="15">15: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a conversational QA dataset wher…</option><option value="16">16: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a foundational paper that provid…</option><option value="19">19: [inline_acl] sp=1 qual=2 gr=1 — Can you refer me to research that adapts the conce…</option><option value="20">20: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest a corpus that contains French ency…</option><option value="21">21: [inline_acl] sp=0 qual=2 gr=1 — Can you suggest any literature that explores the i…</option><option value="22">22: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest literature on a dataset that categ…</option><option value="25">25: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some literature that evaluates the…</option><option value="26">26: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some recent datasets that have bee…</option><option value="27">27: [inline_acl] sp=1 qual=2 gr=1 — Could you direct me towards a study that explores …</option><option value="28">28: [inline_acl] sp=1 qual=2 gr=1 — Could you point me to research on binary classific…</option><option value="29">29: [inline_acl] sp=0 qual=2 gr=1 — Could you point me to studies that discuss the dev…</option><option value="31">31: [inline_acl] sp=1 qual=1 gr=1 — Could you point me toward some large-scale multili…</option><option value="32">32: [inline_acl] sp=1 qual=2 gr=1 — Could you provide me with a reference that discuss…</option><option value="33">33: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend datasets that include SQL anno…</option><option value="34">34: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend studies that provide a baselin…</option><option value="36">36: [inline_acl] sp=0 qual=1 gr=1 — Could you suggest studies that employ novel method…</option><option value="37">37: [inline_acl] sp=1 qual=2 gr=1 — Has there been any recent work or competitions foc…</option><option value="38">38: [inline_acl] sp=0 qual=1 gr=1 — I am exploring state-of-the-art techniques in lang…</option><option value="40">40: [inline_acl] sp=0 qual=2 gr=1 — I am looking to understand more about sequence-to-…</option><option value="41">41: [inline_acl] sp=1 qual=2 gr=1 — I would like to understand the theoretical basis f…</option><option value="42">42: [inline_acl] sp=1 qual=2 gr=1 — I'm conducting research on computational humor and…</option><option value="43">43: [inline_acl] sp=1 qual=1 gr=1 — I'm exploring efficient transformer architectures …</option><option value="45">45: [inline_acl] sp=0 qual=2 gr=1 — I'm exploring ways to enhance question answering s…</option><option value="46">46: [inline_acl] sp=1 qual=2 gr=1 — I'm interested in understanding how perplexity is …</option><option value="47">47: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a comprehensive dataset that has b…</option><option value="48">48: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a paper that discusses improvement…</option><option value="50">50: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into morphological embedding algorithm…</option><option value="51">51: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into the distillation process of langu…</option><option value="52">52: [inline_acl] sp=0 qual=1 gr=1 — I'm researching insertion-based decoding methods f…</option><option value="54">54: [inline_acl] sp=1 qual=1 gr=1 — I'm searching for studies that explore advancement…</option><option value="57">57: [inline_acl] sp=1 qual=2 gr=1 — In the area of argument mining, could you point to…</option><option value="60">60: [inline_acl] sp=1 qual=2 gr=1 — In the context of natural language processing, I a…</option><option value="64">64: [inline_acl] sp=1 qual=1 gr=1 — What are some approaches to generating sports news…</option><option value="65">65: [inline_acl] sp=1 qual=2 gr=1 — What are some good datasets for conversational que…</option><option value="66">66: [inline_acl] sp=1 qual=2 gr=1 — What are some of the key papers to look at for und…</option><option value="67">67: [inline_acl] sp=0 qual=2 gr=1 — What are some recent advancements in training syst…</option><option value="68">68: [inline_acl] sp=1 qual=2 gr=1 — What are some soft-constrained methods proposed in…</option><option value="69">69: [inline_acl] sp=1 qual=2 gr=1 — What are some studies that leverage statistical ma…</option><option value="70">70: [inline_acl] sp=0 qual=2 gr=1 — What are some techniques or tools used in machine …</option><option value="72">72: [inline_acl] sp=1 qual=2 gr=1 — What paper should I look at if I am interested in …</option><option value="73">73: [inline_acl] sp=1 qual=2 gr=1 — What papers should I refer to if I want to explore…</option><option value="76">76: [inline_acl] sp=1 qual=2 gr=1 — What research has been done on annotating user com…</option><option value="77">77: [inline_acl] sp=0 qual=1 gr=1 — What research has been done on improving named ent…</option><option value="78">78: [inline_acl] sp=1 qual=1 gr=1 — What research should I explore to understand metho…</option><option value="79">79: [inline_acl] sp=1 qual=2 gr=1 — When using pretrained transformer models for gener…</option><option value="80">80: [inline_acl] sp=0 qual=1 gr=1 — Where can I find a corpus of CCG annotations for n…</option><option value="81">81: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a detailed discussion on automati…</option><option value="82">82: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a discourse treebank tailored to …</option><option value="83">83: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a large corpus of annotated socia…</option><option value="85">85: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a paper that discusses annotating…</option><option value="86">86: [inline_acl] sp=0 qual=1 gr=1 — Where can I find guidelines on standard practices …</option><option value="87">87: [inline_acl] sp=0 qual=1 gr=1 — Where can I find information on self-attentive par…</option><option value="88">88: [inline_acl] sp=0 qual=1 gr=1 — Where can I find interdisciplinary research that i…</option><option value="89">89: [inline_acl] sp=1 qual=2 gr=1 — Where can I find multilingual datasets used for th…</option><option value="90">90: [inline_acl] sp=1 qual=2 gr=1 — Where can I find research about automatic evaluati…</option><option value="91">91: [inline_acl] sp=0 qual=1 gr=1 — Where might I find a dataset annotated specificall…</option><option value="93">93: [inline_acl] sp=1 qual=2 gr=1 — Which corpora are frequently used in research to b…</option><option value="94">94: [inline_acl] sp=1 qual=2 gr=1 — Which paper specifies the typical configurations u…</option><option value="95">95: [inline_acl] sp=0 qual=1 gr=1 — Which papers should I refer to for learning about …</option><option value="97">97: [inline_acl] sp=1 qual=2 gr=1 — Which work should I explore to understand the tech…</option><option value="98">98: [inline_nonacl] sp=1 qual=1 gr=1 — *Could you suggest a dataset with legally or ethic…</option><option value="99">99: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any papers on training video-language mo…</option><option value="100">100: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any recent papers investigating the use …</option><option value="101">101: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any research papers investigating the im…</option><option value="103">103: [inline_nonacl] sp=1 qual=2 gr=1 — Are there any studies investigating sentiment anal…</option><option value="104">104: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any studies on incorporating external co…</option><option value="105">105: [inline_nonacl] sp=1 qual=2 gr=1 — Are there studies examining how well question answ…</option><option value="106">106: [inline_nonacl] sp=1 qual=1 gr=1 — Are there studies that investigate debiasing langu…</option><option value="107">107: [inline_nonacl] sp=0 qual=1 gr=1 — Can you give me a paper that does self-supervised …</option><option value="108">108: [inline_nonacl] sp=0 qual=1 gr=1 — Can you recommend a dialogue summarization dataset…</option><option value="112">112: [inline_nonacl] sp=1 qual=2 gr=1 — Could you direct me to research that evaluates few…</option><option value="114">114: [inline_nonacl] sp=1 qual=1 gr=1 — Could you point me to research that tackles the is…</option><option value="115">115: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a contemporary research paper …</option><option value="116">116: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a paper that builds a writing …</option><option value="117">117: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that does data-augment…</option><option value="118">118: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that examines how cros…</option><option value="120">120: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that examines the intr…</option><option value="121">121: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores a pre-tr…</option><option value="122">122: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that explores employin…</option><option value="125">125: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that explores strategi…</option><option value="127">127: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores the impr…</option><option value="129">129: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates empl…</option><option value="130">130: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates empl…</option><option value="131">131: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates enha…</option><option value="133">133: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates grap…</option><option value="134">134: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates guid…</option><option value="138">138: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates inco…</option><option value="139">139: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates know…</option><option value="140">140: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates repr…</option><option value="141">141: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates text…</option><option value="144">144: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates the …</option><option value="146">146: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend articles that explore the role…</option><option value="147">147: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research articles that explore…</option><option value="148">148: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research papers that explore a…</option><option value="151">151: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that assesses how wel…</option><option value="152">152: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that assesses how wel…</option><option value="153">153: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that assesses techniq…</option><option value="154">154: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that employs a relaxe…</option><option value="158">158: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how mul…</option><option value="159">159: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how opt…</option><option value="160">160: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that examines how syn…</option><option value="161">161: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the cha…</option><option value="162">162: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the eff…</option><option value="164">164: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that explores identif…</option><option value="165">165: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that has introduced a…</option><option value="166">166: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that improves knowled…</option><option value="167">167: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend research that introduces a met…</option><option value="170">170: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="171">171: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="172">172: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates mer…</option><option value="173">173: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates met…</option><option value="174">174: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates tec…</option><option value="175">175: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="177">177: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="178">178: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates usi…</option><option value="180">180: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend scholarly articles that invest…</option><option value="181">181: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend studies on hierarchical modeli…</option><option value="182">182: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that concentrate on an…</option><option value="183">183: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that investigate fine-…</option><option value="184">184: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend studies that tackle the issue …</option><option value="185">185: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies which explore how to o…</option><option value="186">186: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a dataset containing diverse, in…</option><option value="187">187: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a dataset for question-answering…</option><option value="188">188: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a research article that explores…</option><option value="189">189: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study examining how transforme…</option><option value="190">190: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that evaluates cross-enc…</option><option value="191">191: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that examines how well c…</option><option value="192">192: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a cohesive…</option><option value="193">193: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a compress…</option><option value="196">196: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores improved t…</option><option value="198">198: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores the use of…</option><option value="200">200: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a study that proposes high-param…</option><option value="201">201: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a thorough comparative analysis …</option><option value="202">202: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a triplet-formatted structured d…</option><option value="203">203: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest an article that leverages the sp…</option><option value="204">204: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest datasets that can benchmark LLM …</option><option value="206">206: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research on detecting common err…</option><option value="207">207: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that assesses if langua…</option><option value="210">210: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how coref…</option><option value="211">211: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="212">212: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="214">214: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that examines how the o…</option><option value="215">215: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="216">216: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines how well …</option><option value="217">217: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="218">218: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines the appli…</option><option value="219">219: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines the chall…</option><option value="220">220: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines the diffi…</option><option value="222">222: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores a pre-tra…</option><option value="223">223: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores employing…</option><option value="225">225: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores the drawb…</option><option value="227">227: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores the impro…</option><option value="229">229: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates a clu…</option><option value="230">230: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates acros…</option><option value="232">232: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates aspec…</option><option value="233">233: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates effic…</option><option value="234">234: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates emplo…</option><option value="236">236: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates expan…</option><option value="237">237: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates graph…</option><option value="239">239: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how h…</option><option value="240">240: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how m…</option><option value="241">241: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that investigates how n…</option><option value="242">242: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates how u…</option><option value="243">243: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates impro…</option><option value="245">245: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates train…</option><option value="246">246: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that offers an in-depth…</option><option value="247">247: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that shows multilingual…</option><option value="248">248: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that trains language mo…</option><option value="249">249: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that tries to interpret…</option><option value="250">250: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest some work that develops multimod…</option><option value="253">253: [inline_nonacl] sp=1 qual=2 gr=1 — Has any research tried to mitigate overfitting in …</option><option value="254">254: [inline_nonacl] sp=1 qual=1 gr=1 — Has any study explored the zero-shot extraction of…</option><option value="255">255: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any research that uses multiple mod…</option><option value="256">256: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any work that improves the work on …</option><option value="258">258: [inline_nonacl] sp=1 qual=2 gr=1 — Have any papers tried to address the background-sh…</option><option value="259">259: [inline_nonacl] sp=1 qual=2 gr=1 — Have any recent publications explored the use of n…</option><option value="262">262: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers collected feedback from r…</option><option value="263">263: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers critically analyzed the p…</option><option value="264">264: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers examined the efficacy of …</option><option value="266">266: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers explored methods to impro…</option><option value="267">267: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers introduced a dedicated pr…</option><option value="268">268: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers investigated human capaci…</option><option value="270">270: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers suggested methods for sum…</option><option value="271">271: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers suggested techniques for …</option><option value="273">273: [inline_nonacl] sp=1 qual=1 gr=1 — Have any studies explored the creation of memory m…</option><option value="274">274: [inline_nonacl] sp=1 qual=2 gr=1 — Have there been any advancements in language model…</option><option value="275">275: [inline_nonacl] sp=1 qual=2 gr=1 — How can I locate a dataset containing toxic senten…</option><option value="276">276: [inline_nonacl] sp=1 qual=2 gr=1 — How can SQL-to-text be utilized to improve text-to…</option><option value="277">277: [inline_nonacl] sp=0 qual=1 gr=1 — How can dense retrieval models for open-domain que…</option><option value="279">279: [inline_nonacl] sp=1 qual=2 gr=1 — In multi-hop question answering, is there a paper …</option><option value="280">280: [inline_nonacl] sp=1 qual=2 gr=1 — Is it possible to adatp named entity recognition s…</option><option value="281">281: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a benchmark designed to assess language m…</option><option value="282">282: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a comprehensive dataset available for sum…</option><option value="283">283: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a dataset available for open-domain targe…</option><option value="284">284: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a dataset containing question-answer pair…</option><option value="285">285: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a research paper that has developed a cus…</option><option value="286">286: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a specialized question answering dataset …</option><option value="287">287: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a study that investigates if large langua…</option><option value="288">288: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any paper that tried fine-tuning mBERT to…</option><option value="289">289: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any research that investigates how to use…</option><option value="290">290: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research examining if multilingual pre-tr…</option><option value="291">291: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research on a specialized language model …</option><option value="292">292: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research that argues for transparency and…</option><option value="293">293: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research that investigates embedding mult…</option><option value="294">294: [inline_nonacl] sp=1 qual=2 gr=1 — Is there work on text classification that explores…</option><option value="295">295: [inline_nonacl] sp=1 qual=2 gr=1 — What approaches have been suggested to lower the c…</option><option value="296">296: [inline_nonacl] sp=1 qual=1 gr=1 — What are some scholarly articles that explore scal…</option><option value="298">298: [inline_nonacl] sp=0 qual=2 gr=1 — What are some studies that explore data-poisoning …</option><option value="299">299: [inline_nonacl] sp=1 qual=1 gr=1 — What are the latest advancements in predicting sui…</option><option value="300">300: [inline_nonacl] sp=0 qual=1 gr=1 — What are the latest developments in conversational…</option><option value="301">301: [inline_nonacl] sp=0 qual=1 gr=1 — What benchmarks have prior research utilized to as…</option><option value="302">302: [inline_nonacl] sp=1 qual=2 gr=1 — What concerns or key points have been highlighted …</option><option value="303">303: [inline_nonacl] sp=0 qual=2 gr=1 — What difficulties do neural conversational models …</option><option value="304">304: [inline_nonacl] sp=1 qual=2 gr=1 — What literature is available on training semantic …</option><option value="305">305: [inline_nonacl] sp=1 qual=2 gr=1 — What methods exist for tailoring news suggestions …</option><option value="308">308: [inline_nonacl] sp=1 qual=1 gr=1 — What recent developments in transformer architectu…</option><option value="309">309: [inline_nonacl] sp=1 qual=1 gr=1 — What recent research has been conducted on improvi…</option><option value="310">310: [inline_nonacl] sp=1 qual=1 gr=1 — What research articles should I consult to underst…</option><option value="313">313: [inline_nonacl] sp=0 qual=1 gr=1 — What research exists on incorporating knowledge gr…</option><option value="314">314: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on leveraging syntactic roles…</option><option value="315">315: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on the impact of scaling on p…</option><option value="316">316: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on using reinforcement learni…</option><option value="317">317: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on applying contr…</option><option value="318">318: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on creating neura…</option><option value="319">319: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on determining th…</option><option value="320">320: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on enhancing conv…</option><option value="321">321: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on incorporating …</option><option value="322">322: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on news recommend…</option><option value="323">323: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on scaling within…</option><option value="324">324: [inline_nonacl] sp=0 qual=2 gr=1 — What research has been conducted on the impact of …</option><option value="327">327: [inline_nonacl] sp=1 qual=1 gr=1 — What research is available on the concept of using…</option><option value="329">329: [inline_nonacl] sp=1 qual=2 gr=1 — What resources or toolkits are available to facili…</option><option value="330">330: [inline_nonacl] sp=1 qual=1 gr=1 — What sources offer research on maintaining factual…</option><option value="332">332: [inline_nonacl] sp=0 qual=1 gr=1 — What techniques exist for efficiently fine-tuning …</option><option value="333">333: [inline_nonacl] sp=0 qual=2 gr=1 — What techniques exist for incorporating context in…</option><option value="336">336: [inline_nonacl] sp=1 qual=1 gr=1 — Where can I find a database of good prompts to use…</option><option value="337">337: [inline_nonacl] sp=0 qual=1 gr=1 — Where can I read about the using soft embeddings t…</option><option value="338">338: [inline_nonacl] sp=1 qual=2 gr=1 — Which method involves training additional prompt t…</option><option value="339">339: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper has conducted a thorough analysis of h…</option><option value="340">340: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper introduced the task of creating extend…</option><option value="341">341: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper presents a platform that emphasizes ev…</option><option value="342">342: [inline_nonacl] sp=0 qual=1 gr=1 — Which paper shows that generated captions of model…</option><option value="343">343: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper shows that human experts and non-exper…</option><option value="344">344: [inline_nonacl] sp=0 qual=1 gr=1 — Which work introduces sparse attention modules and…</option><option value="345">345: [inline_nonacl] sp=1 qual=1 gr=1 — Which work pushes the limit of model quantization …</option><option value="348">348: [inline_nonacl] sp=1 qual=1 gr=1 — Which work suggests that machine translation model…</option><option value="349">349: [inline_nonacl] sp=1 qual=1 gr=1 — Which works shows that training large language mod…</option><option value="350">350: [inline_nonacl] sp=1 qual=2 gr=1 — ould you direct me to research that shows that the…</option><option value="351">351: [manual_acl] sp=1 qual=1 gr=1 — Are there any examples of using dense phrase retri…</option><option value="352">352: [manual_acl] sp=1 qual=1 gr=1 — Are there any large-scale and open-source text sim…</option><option value="354">354: [manual_acl] sp=1 qual=1 gr=1 — Could you recommend a dataset paper which presents…</option><option value="355">355: [manual_acl] sp=1 qual=1 gr=1 — Find the NLP paper that focuses on dialogue genera…</option><option value="356">356: [manual_acl] sp=0 qual=1 gr=1 — Give me a paper proposing to circumvent a single-t…</option><option value="357">357: [manual_acl] sp=1 qual=2 gr=1 — How to achieve zero-shot lip reading?…</option><option value="358">358: [manual_acl] sp=1 qual=2 gr=1 — How to better attract readers to news articles by …</option><option value="359">359: [manual_acl] sp=0 qual=1 gr=1 — How to faithfully and explicitly measure the helpf…</option><option value="361">361: [manual_acl] sp=1 qual=1 gr=1 — In multimodal (multilingual) abstractive summariza…</option><option value="362">362: [manual_acl] sp=1 qual=1 gr=1 — Is there a Chinese hate speech paper that construc…</option><option value="363">363: [manual_acl] sp=0 qual=1 gr=1 — Is there a dataset that allows to perform aspect-b…</option><option value="365">365: [manual_acl] sp=1 qual=1 gr=1 — Is there a dialogue dataset where a speaker's utte…</option><option value="367">367: [manual_acl] sp=1 qual=2 gr=1 — Is there a method that measures the information pr…</option><option value="368">368: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper comparing knowledge distillation …</option><option value="370">370: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that applies large language model…</option><option value="371">371: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that connects the basic elements …</option><option value="373">373: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that shows that language models' …</option><option value="374">374: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that supports the use of automate…</option><option value="375">375: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that uses Explainable AI techniqu…</option><option value="376">376: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses an app for a popular ta…</option><option value="377">377: [manual_acl] sp=0 qual=1 gr=1 — Is there a paper that uses evolutionary algorithms…</option><option value="379">379: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses the tree structure of m…</option><option value="380">380: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that utilizes the characteristics…</option><option value="381">381: [manual_acl] sp=1 qual=2 gr=1 — Is there a study that shows how to help the demons…</option><option value="383">383: [manual_acl] sp=1 qual=1 gr=1 — Is there an evaluation metric for natural language…</option><option value="384">384: [manual_acl] sp=1 qual=1 gr=1 — Is there any dataset that contains minimally-contr…</option><option value="385">385: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper about style transfer for storie…</option><option value="386">386: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper exploring real speakers and thu…</option><option value="387">387: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper leverages knowledge distillatio…</option><option value="388">388: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that address attacks on code mo…</option><option value="390">390: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that applies curriculum learnin…</option><option value="391">391: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that applies symbolic distillat…</option><option value="392">392: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that attempts to evaluate the s…</option><option value="393">393: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that automatically creates a da…</option><option value="394">394: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that combines causal inference …</option><option value="396">396: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that employs code LLMs to itera…</option><option value="397">397: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores and annotates the…</option><option value="398">398: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores using only an enc…</option><option value="399">399: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that investigates backdoor atta…</option><option value="400">400: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that leverages graph neural net…</option><option value="401">401: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that leverages syntactic rules …</option><option value="402">402: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that performs adversarial train…</option><option value="403">403: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that proposes a new multimodal …</option><option value="405">405: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that reveals annotation problem…</option><option value="406">406: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that studies a teacher AI infer…</option><option value="407">407: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that tries to investigate LLMs’…</option><option value="408">408: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that uses data collected from t…</option><option value="409">409: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses prompt tuning in mult…</option><option value="410">410: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses token-level loss to e…</option><option value="412">412: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that utilizes graph structure t…</option><option value="413">413: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that utilizes masked language m…</option><option value="415">415: [manual_acl] sp=1 qual=2 gr=1 — Is there any work that allows large numbers of mod…</option><option value="416">416: [manual_acl] sp=1 qual=1 gr=1 — Is there any work that attacks language models in …</option><option value="418">418: [manual_acl] sp=1 qual=2 gr=1 — Is there commonsense reasoning dataset which gener…</option><option value="419">419: [manual_acl] sp=1 qual=2 gr=1 — Is there such a factuality evaluation dataset that…</option><option value="420">420: [manual_acl] sp=1 qual=2 gr=1 — Is there such a reading comprehension dataset in u…</option><option value="421">421: [manual_acl] sp=1 qual=1 gr=1 — Provide an example of a paper which proposes a met…</option><option value="422">422: [manual_acl] sp=1 qual=2 gr=1 — What are some data-efficient ways to learn text em…</option><option value="424">424: [manual_acl] sp=0 qual=1 gr=1 — What is a large event-coverage general-domain even…</option><option value="425">425: [manual_acl] sp=1 qual=1 gr=1 — What is the first paper to address the problem of …</option><option value="427">427: [manual_acl] sp=1 qual=2 gr=1 — What limitations do large language models have in …</option><option value="428">428: [manual_acl] sp=1 qual=2 gr=1 — What paper compares humans' and language models' n…</option><option value="429">429: [manual_acl] sp=1 qual=2 gr=1 — What work attempts to explore multi-hop reasoning …</option><option value="430">430: [manual_acl] sp=1 qual=1 gr=1 — Which article first proposed shuffled-group-whiten…</option><option value="431">431: [manual_acl] sp=1 qual=1 gr=1 — Which dataset supports narration generation and te…</option><option value="432">432: [manual_acl] sp=0 qual=1 gr=1 — Which family of model generally perform the best f…</option><option value="434">434: [manual_acl] sp=1 qual=2 gr=1 — Which knowledge graph completion method focuses on…</option><option value="435">435: [manual_acl] sp=1 qual=1 gr=1 — Which language model distillation paper that first…</option><option value="436">436: [manual_acl] sp=1 qual=2 gr=1 — Which numerical reasoning paper first published a …</option><option value="437">437: [manual_acl] sp=1 qual=2 gr=1 — Which paper about parameter-efficient finetuning f…</option><option value="438">438: [manual_acl] sp=1 qual=1 gr=1 — Which paper combines the advantages of different f…</option><option value="439">439: [manual_acl] sp=0 qual=1 gr=1 — Which paper did a comprehensive survey of the code…</option><option value="440">440: [manual_acl] sp=1 qual=2 gr=1 — Which paper employs a two-stage approach in genera…</option><option value="441">441: [manual_acl] sp=1 qual=1 gr=1 — Which paper enables interactive semantic parsing b…</option><option value="442">442: [manual_acl] sp=1 qual=1 gr=1 — Which paper explored training a GPT-2 for automati…</option><option value="443">443: [manual_acl] sp=1 qual=1 gr=1 — Which paper first aggregates statements to represe…</option><option value="444">444: [manual_acl] sp=1 qual=2 gr=1 — Which paper first applied the chain-of-thought tec…</option><option value="445">445: [manual_acl] sp=1 qual=2 gr=1 — Which paper first apply mixture of experts idea to…</option><option value="446">446: [manual_acl] sp=1 qual=2 gr=1 — Which paper first attempts to take potential depen…</option><option value="447">447: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines different methods for u…</option><option value="448">448: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines rewriting and expansion…</option><option value="449">449: [manual_acl] sp=1 qual=1 gr=1 — Which paper first conducted the positioned error t…</option><option value="450">450: [manual_acl] sp=1 qual=1 gr=1 — Which paper first construct large-scale corpus to …</option><option value="452">452: [manual_acl] sp=1 qual=1 gr=1 — Which paper first explored In-context learning in …</option><option value="453">453: [manual_acl] sp=1 qual=1 gr=1 — Which paper first found that multilingual models c…</option><option value="454">454: [manual_acl] sp=1 qual=2 gr=1 — Which paper first introduced document content as a…</option><option value="455">455: [manual_acl] sp=1 qual=1 gr=1 — Which paper first propose to mask positions to pre…</option><option value="456">456: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed a cross-domain language…</option><option value="457">457: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed extracting the pair of …</option><option value="459">459: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed to combine pretrained m…</option><option value="460">460: [manual_acl] sp=0 qual=1 gr=1 — Which paper first proposed to only update some ori…</option><option value="461">461: [manual_acl] sp=1 qual=1 gr=1 — Which paper first published a real-world Chinese-E…</option><option value="462">462: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that it is possible to mai…</option><option value="463">463: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that large language models…</option><option value="464">464: [manual_acl] sp=1 qual=2 gr=1 — Which paper first studied the efficiency robustnes…</option><option value="465">465: [manual_acl] sp=1 qual=2 gr=1 — Which paper first use the attention weights to gui…</option><option value="466">466: [manual_acl] sp=0 qual=1 gr=1 — Which paper first used structural information for …</option><option value="468">468: [manual_acl] sp=0 qual=1 gr=1 — Which paper highlights the need for leveraging all…</option><option value="469">469: [manual_acl] sp=1 qual=2 gr=1 — Which paper introduce a DRO (distribution robust o…</option><option value="470">470: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduced the human-evaluated timelin…</option><option value="471">471: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduces the R-GCN technique into do…</option><option value="472">472: [manual_acl] sp=1 qual=1 gr=1 — Which paper investigates the influence of the dive…</option><option value="474">474: [manual_acl] sp=0 qual=1 gr=1 — Which paper is the first to comprehensively review…</option><option value="476">476: [manual_acl] sp=1 qual=2 gr=1 — Which paper measured how well the source-translati…</option><option value="477">477: [manual_acl] sp=1 qual=2 gr=1 — Which paper presents an easy to implement and high…</option><option value="478">478: [manual_acl] sp=1 qual=2 gr=1 — Which paper produces a dataset for text simplifica…</option><option value="479">479: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed a learning-based data augment…</option><option value="480">480: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed decomposing the logit update …</option><option value="481">481: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed dictionary-based Bayesian inf…</option><option value="482">482: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed the integration of human tran…</option><option value="483">483: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes a memory-efficient optimizer …</option><option value="484">484: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes the two-stage training method…</option><option value="485">485: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes to use rewriting based approa…</option><option value="486">486: [manual_acl] sp=1 qual=2 gr=1 — Which paper showed that social relationships were …</option><option value="487">487: [manual_acl] sp=1 qual=2 gr=1 — Which paper shows assessment of training instabili…</option><option value="488">488: [manual_acl] sp=1 qual=1 gr=1 — Which paper shows that in instruction tuning, the …</option><option value="489">489: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies how current retrieval systems …</option><option value="490">490: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies the concept of enhancing the c…</option><option value="491">491: [manual_acl] sp=1 qual=2 gr=1 — Which paper surveyed the datasets and tasks of ask…</option><option value="492">492: [manual_acl] sp=1 qual=2 gr=1 — Which paper used both automatically generated and …</option><option value="493">493: [manual_acl] sp=1 qual=2 gr=1 — Which paper utilizes language models to generate s…</option><option value="494">494: [manual_acl] sp=1 qual=1 gr=1 — Which paper was the first to propose combining hum…</option><option value="495">495: [manual_acl] sp=0 qual=2 gr=1 — Which papers develop methods to make in-context le…</option><option value="496">496: [manual_acl] sp=0 qual=1 gr=1 — Which papers were among the first to explore the t…</option><option value="497">497: [manual_acl] sp=1 qual=1 gr=1 — Which pre-trained model is specifically designed f…</option><option value="499">499: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model can demonstrate that v…</option><option value="500">500: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model paper in 2023 develope…</option><option value="501">501: [manual_acl] sp=1 qual=2 gr=1 — Which was the first paper to explore the online ad…</option><option value="503">503: [manual_acl] sp=1 qual=1 gr=1 — Which work proposes an approach to improve candida…</option><option value="504">504: [manual_acl] sp=1 qual=2 gr=1 — what's the first paper that manages to handle KBQA…</option><option value="505">505: [manual_acl] sp=1 qual=1 gr=1 — which paper first focuses on addressing the over-s…</option><option value="506">506: [manual_iclr] sp=1 qual=1 gr=1 — Can we reduce visual tokens in vision transformers…</option><option value="507">507: [manual_iclr] sp=1 qual=1 gr=1 — Can we learn to represent an image with arbitary n…</option><option value="508">508: [manual_iclr] sp=1 qual=2 gr=1 — Are there any papers that construct convolutional …</option><option value="509">509: [manual_iclr] sp=1 qual=1 gr=1 — Are there any papers that study whether you can id…</option><option value="511">511: [manual_iclr] sp=1 qual=1 gr=1 — Are there datasets and benchmarks available for me…</option><option value="512">512: [manual_iclr] sp=1 qual=2 gr=1 — Are there sequential learning guarantees for confi…</option><option value="513">513: [manual_iclr] sp=1 qual=2 gr=1 — Can we find the solution of the Bilevel optimizati…</option><option value="514">514: [manual_iclr] sp=0 qual=2 gr=1 — Can you find a dataset that shows LLM-based evalua…</option><option value="515">515: [manual_iclr] sp=1 qual=2 gr=1 — Can you find a research paper that discusses using…</option><option value="516">516: [manual_iclr] sp=1 qual=1 gr=1 — I'm using Local SGD with a decaying learning rate …</option><option value="517">517: [manual_iclr] sp=1 qual=1 gr=1 — In video diffusion models, is there any paper that…</option><option value="518">518: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper illustrating that pre-trained tra…</option><option value="519">519: [manual_iclr] sp=1 qual=2 gr=1 — Is there a paper that takes a mixed machine learni…</option><option value="520">520: [manual_iclr] sp=1 qual=1 gr=1 — Is there a paper which applies Bayesian optimizati…</option><option value="521">521: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper which proposes a general data sel…</option><option value="523">523: [manual_iclr] sp=0 qual=1 gr=1 — Is there a single GNN model that can inductively g…</option><option value="524">524: [manual_iclr] sp=1 qual=2 gr=1 — Is there a theory paper that explains why sometime…</option><option value="525">525: [manual_iclr] sp=1 qual=1 gr=1 — Is there an existing dataset of images with alt-te…</option><option value="526">526: [manual_iclr] sp=1 qual=1 gr=1 — Is there any generalizable NeRF paper that disenta…</option><option value="527">527: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper applies off-shelf GPT-2 model i…</option><option value="528">528: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper improves adversarial training b…</option><option value="529">529: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that explores ways to parameter…</option><option value="530">530: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that previously proposed to con…</option><option value="531">531: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that seamlessly integrates the …</option><option value="532">532: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that theoretically explains why…</option><option value="533">533: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that uses Lipschitz continuity …</option><option value="534">534: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper trying to improve MLE for auto-…</option><option value="535">535: [manual_iclr] sp=1 qual=1 gr=1 — Name a paper which proposes a probabilsitic formul…</option><option value="536">536: [manual_iclr] sp=1 qual=2 gr=1 — What are some evaluation benchmarks for LLM privac…</option><option value="537">537: [manual_iclr] sp=1 qual=1 gr=1 — What are the key advantages of coupling neural SDE…</option><option value="538">538: [manual_iclr] sp=1 qual=2 gr=1 — What is a paper studying data being collected in b…</option><option value="539">539: [manual_iclr] sp=1 qual=1 gr=1 — What is the first paper that theoretically studies…</option><option value="540">540: [manual_iclr] sp=1 qual=2 gr=1 — What is the first paper that uses the generalized …</option><option value="541">541: [manual_iclr] sp=1 qual=1 gr=1 — What molecular representation learning paper intro…</option><option value="542">542: [manual_iclr] sp=1 qual=2 gr=1 — What open-source dataset combined knowledge retrie…</option><option value="543">543: [manual_iclr] sp=0 qual=1 gr=1 — What paper considers sensitive data issue when pro…</option><option value="544">544: [manual_iclr] sp=0 qual=1 gr=1 — What paper evaluated the ability of visual few-sho…</option><option value="545">545: [manual_iclr] sp=1 qual=1 gr=1 — What paper first adapted ControlNet to generate co…</option><option value="546">546: [manual_iclr] sp=1 qual=1 gr=1 — What paper first associate the modeling frequency …</option><option value="547">547: [manual_iclr] sp=1 qual=2 gr=1 — What paper first extends rotary positional encodin…</option><option value="548">548: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposed a robust perceptual simi…</option><option value="549">549: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposes that simply reversing th…</option><option value="551">551: [manual_iclr] sp=1 qual=1 gr=1 — What paper first used the technique of prompt engi…</option><option value="552">552: [manual_iclr] sp=1 qual=1 gr=1 — What paper first uses decoupled workers in distrib…</option><option value="553">553: [manual_iclr] sp=1 qual=2 gr=1 — What paper investigated the effect of the relative…</option><option value="554">554: [manual_iclr] sp=1 qual=1 gr=1 — What paper is the first to prove finetuned LLM can…</option><option value="555">555: [manual_iclr] sp=1 qual=1 gr=1 — What paper mitigates language model sampling error…</option><option value="556">556: [manual_iclr] sp=1 qual=2 gr=1 — What paper mitigates the vocabulary size limitatio…</option><option value="558">558: [manual_iclr] sp=1 qual=1 gr=1 — What paper showed first that one can build a fully…</option><option value="559">559: [manual_iclr] sp=0 qual=1 gr=1 — What paper shows that RLAIF can fully replace RLHF…</option><option value="561">561: [manual_iclr] sp=1 qual=1 gr=1 — What work first uses LLM to code robotic simulatio…</option><option value="562">562: [manual_iclr] sp=1 qual=2 gr=1 — What work proposes a model to learn a latent regul…</option><option value="563">563: [manual_iclr] sp=1 qual=1 gr=1 — What work proposes to combine video foundation mod…</option><option value="565">565: [manual_iclr] sp=1 qual=1 gr=1 — Which foundation model paper first proposed a time…</option><option value="567">567: [manual_iclr] sp=1 qual=1 gr=1 — Which machine learning paper proposed certified ro…</option><option value="568">568: [manual_iclr] sp=0 qual=1 gr=1 — Which multimodal large language model represents v…</option><option value="569">569: [manual_iclr] sp=1 qual=1 gr=1 — Which neural theorem proving paper first attempted…</option><option value="570">570: [manual_iclr] sp=1 qual=2 gr=1 — Which paper considers both weights and activations…</option><option value="571">571: [manual_iclr] sp=1 qual=1 gr=1 — Which paper contains quantitative results demonstr…</option><option value="572">572: [manual_iclr] sp=1 qual=1 gr=1 — Which paper examined the scalability of instructio…</option><option value="573">573: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first applied the chain of thought con…</option><option value="574">574: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first derived online occupany estimati…</option><option value="575">575: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first found that REINFORCE works bette…</option><option value="576">576: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first found that when transformers are…</option><option value="577">577: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first investigates the knowledge prefe…</option><option value="578">578: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first proposes a unified framework for…</option><option value="579">579: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first proved that wide-enough transfor…</option><option value="580">580: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first showed that task-specific knowle…</option><option value="581">581: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first studied differential privacy for…</option><option value="582">582: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first study POMDP with enhanced feedba…</option><option value="583">583: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first tried to fine-tune LLMs with cha…</option><option value="584">584: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first used language models to emulate …</option><option value="585">585: [manual_iclr] sp=1 qual=1 gr=1 — Which paper formally defines the problem of model …</option><option value="586">586: [manual_iclr] sp=1 qual=2 gr=1 — Which paper found that using common character enco…</option><option value="587">587: [manual_iclr] sp=1 qual=1 gr=1 — Which paper in human motion generation can control…</option><option value="588">588: [manual_iclr] sp=1 qual=2 gr=1 — Which paper is the first to model the helpfulness …</option><option value="589">589: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes an alignment framework that s…</option><option value="590">590: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes to integrate black-box LLMs w…</option><option value="591">591: [manual_iclr] sp=1 qual=1 gr=1 — Which paper studies how difficult is a policy lear…</option><option value="592">592: [manual_iclr] sp=1 qual=2 gr=1 — Which paper trains on linear regression to hypothe…</option><option value="593">593: [manual_iclr] sp=1 qual=1 gr=1 — Which paper uses the latent diffusion model for th…</option><option value="594">594: [manual_iclr] sp=1 qual=2 gr=1 — Which paper utilized MMD flows with Riesz kernels …</option><option value="596">596: [manual_iclr] sp=1 qual=2 gr=1 — Which paper systematically examed the input mismat…</option></select>
    <p id="queryText" class="small has-abs" data-abs="">Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?</p>
    <p id="queryMeta" class="small">set:inline_acl | spec:0 | qual:2 | gr:0</p>

    <label>Relevant document (rank&nbsp;+&nbsp;title)</label>
    <select id="docSel"><option value="0" data-id="202719327" title="Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large &quot;teacher&quot; BERT can be effectively transferred to a small &quot;student&quot; Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.TinyBERT 4 1 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT 4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE . * Authors contribute equally. † This work is done when Xiaoqi Jiao is an intern at Huawei Noah's Ark Lab. ‡ Corresponding authors. 1  The code and models are publicly available at https: //github.com/huawei-noah/Pretrained-Language-Model/tree/ master/TinyBERT">0: rank=∞  TinyBERT: Distilling BERT for Natural Language Understanding</option></select>
    <p id="docTitle" class="small has-abs" data-abs="Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large &quot;teacher&quot; BERT can be effectively transferred to a small &quot;student&quot; Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.TinyBERT 4 1 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT 4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE . * Authors contribute equally. † This work is done when Xiaoqi Jiao is an intern at Huawei Noah's Ark Lab. ‡ Corresponding authors. 1  The code and models are publicly available at https: //github.com/huawei-noah/Pretrained-Language-Model/tree/ master/TinyBERT">0: rank=∞  TinyBERT: Distilling BERT for Natural Language Understanding</p>
  </div>

  <!-- PROMPT / LLM / ANNOTATION CARD -->
  <div class="card" style="flex:2 1 520px;">
    <h2>Prompt &amp; LLM setup</h2>

    <label>Prompt</label>
    <div style="display:flex;gap:.5rem;">
      <select id="promptSel" style="flex:1;"><option value="dummy">dummy</option><option value="full_text">full_text</option><option value="title_abstract">title_abstract</option></select>
      <button id="newPromptBtn">+&nbsp;New</button>
    </div>

    <label>Extractor</label>
    <select id="extSel"><option value="dummy">dummy</option><option value="json_list_extractor">json_list_extractor</option></select>

    <label>k (top‑k retrieval)</label>
    <input id="kInp" type="number" value="50" min="1">

    <label>Prompt text</label>
    <textarea id="promptBox"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="savePromptBtn">💾&nbsp;Save</button>
      <button id="reloadPromptBtn">⟳&nbsp;Reload</button>
    </div>

    <details style="margin-top:.75rem;">
      <summary><strong>LLM config</strong></summary>
      <label>API key</label><input id="apiKey" type="text" placeholder="sk‑…">
      <label>Model</label><input id="model" type="text" value="gpt-4o-mini">
      <label>Temperature</label><input id="temp" type="number" value="0" step=".1">
      <label>Max tokens</label><input id="maxTok" type="number" value="2048">
      <label style="display:flex;align-items:center;gap:.5rem;margin-top:.5rem;">
        <input id="wantJson" type="checkbox"> Expect JSON object response
      </label>
    </details>

    <label>Your annotation</label>
    <textarea id="noteBox" placeholder="Add notes about this run…"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="saveNoteBtn" style="background:#0b63ff;color:#fff;">Save&nbsp;annotation</button>
      <button id="runBtn" style="background:#14a44d;color:#fff;">Run</button>
    </div>
  </div>
</section>

<!-- BEFORE / AFTER TABLES ---------------------------------------------->
<section class="flex">
  <div class="card">
    <h2>Before&nbsp;(original)</h2>
    <table id="beforeTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody><tr class="has-abs" data-abs="Existing language model compression methods mostly use a simple L 2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CODIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pretraining and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods. 1"><td>1</td><td>Contrastive Distillation on Intermediate Representations for Language Model Compression</td><td>0.346</td></tr><tr class="has-abs" data-abs="Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that Ho-moDistil achieves significant improvements on existing baselines 1 ."><td>2</td><td>HOMODISTIL: HOMOTOPIC TASK-AGNOSTIC DISTIL- LATION OF PRE-TRAINED TRANSFORMERS</td><td>0.349</td></tr><tr class="has-abs" data-abs="Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods. * Corresponding authors: Chongyang Tao and Dongyan Zhao."><td>3</td><td>Multi-Granularity Structural Knowledge Distillation for Language Model Compression</td><td>0.353</td></tr><tr class="has-abs" data-abs="A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes, but also in combination with DynaBERT, the recently proposed adaptive size pruning method."><td>4</td><td>Distilling Linguistic Context for Language Model Compression</td><td>0.354</td></tr><tr class="has-abs" data-abs="The billions, and sometimes even trillions, of parameters involved in pre-trained language models significantly hamper their deployment in resource-constrained devices and real-time applications. Knowledge distillation (KD) can transfer knowledge from the original model (i.e., teacher) into a compact model (i.e., student) to achieve model compression. However, previous KD methods have usually frozen the teacher and applied its immutable output feature maps as soft labels to guide the student's training. Moreover, the goal of the teacher is to achieve the best performance on downstream tasks rather than knowledge transfer. Such a fixed architecture may limit the teacher's teaching and student's learning abilities. Herein, a knowledge distillation method with reptile meta-learning is proposed to facilitate the transfer of knowledge from the teacher to the student. The teacher can continuously meta-learn the student's learning objective to adjust its parameters for maximizing the student's performance throughout the distillation process. In this way, the teacher learns to teach, produces more suitable soft labels, and transfers more appropriate knowledge to the student, resulting in improved performance. Unlike previous KD using meta-learning, the proposed method only needs to calculate the first-order derivatives to update the teacher, leading to lower computational cost but better convergence. Extensive experiments on the GLUE benchmark show the competitive performance achieved by the proposed method. For reproducibility, the code for this paper is available at: https://github. com/maxinge8698/ReptileDistil."><td>5</td><td>Knowledge Distillation with Reptile Meta-Learning for Pretrained Language Model Compression</td><td>0.371</td></tr><tr class="has-abs" data-abs="Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multilayer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy. 1"><td>6</td><td>Patient Knowledge Distillation for BERT Model Compression</td><td>0.373</td></tr><tr class="has-abs" data-abs="Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher's behavior on the linear interpolation of example pairs as well. We prove, from a theoretical perspective, that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach. * Equal contribution 1 arXiv:2011.00593v1 [cs.CL] 1 Nov 2020"><td>7</td><td>Pre-print MIXKD: TOWARDS EFFICIENT DISTILLATION OF LARGE-SCALE LANGUAGE MODELS</td><td>0.377</td></tr><tr class="has-abs" data-abs="The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, timesensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model."><td>8</td><td>Autoregressive Knowledge Distillation through Imitation Learning</td><td>0.377</td></tr><tr class="has-abs" data-abs="Knowledge distillation (KD) is a wellknown method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5× smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. Overall, we conclude that compressing MNMT models via KD is challenging, indicating immense scope for further research."><td>9</td><td>An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models</td><td>0.383</td></tr><tr class="has-abs" data-abs="The development of over-parameterized pretrained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7× and 21× outperforms state-of-theart compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks."><td>10</td><td>KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation</td><td>0.385</td></tr><tr class="has-abs" data-abs="We generalize deep self-attention distillation in MINILM (Wang et al., 2020) by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers. In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models 1 distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art."><td>11</td><td>MINILMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</td><td>0.386</td></tr><tr class="has-abs" data-abs="Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies wordlevel KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher.Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG."><td>12</td><td>A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training</td><td>0.390</td></tr><tr class="has-abs" data-abs="Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs. 1  We focus on task-specific knowledge distillation."><td>13</td><td>One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers</td><td>0.400</td></tr><tr class="has-abs" data-abs="Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher's training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out-ofdomain data and adversarial training to learn the teacher's output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher's classification score (accuracy or F1) while compressing the model 30 times.ReferencesLuisa Bentivogli, Peter Clark, Ido Dagan, and DaniloGiampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. In TAC. Amodei. 2020. Language models are few-shot learners."><td>14</td><td>Towards Zero-Shot Knowledge Distillation for Natural Language Processing</td><td>0.400</td></tr><tr class="has-abs" data-abs="On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of metalearning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compareaggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https: //github.com/cheneydon/hrkd."><td>15</td><td>HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression</td><td>0.402</td></tr><tr class="has-abs" data-abs="Knowledge Distillation (KD)(Hinton et al., 2015)is one of the most effective approaches for deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the largescale models to smaller student models. Previous KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher's soft labels and predictions can further enhance student capacity and improve generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in teacher and student embedding spaces. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for taskspecific knowledge distillation on the GLUE benchmark(Wang et al., 2018a)."><td>16</td><td>ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models</td><td>0.404</td></tr><tr class="has-abs" data-abs="Knowledge distillation (KD) is a highly promising method for mitigating the computational problems of pre-trained language models (PLMs). Among various KD approaches, Intermediate Layer Distillation (ILD) has been a de facto standard KD method with its performance efficacy in the NLP field. In this paper, we find that existing ILD methods are prone to overfitting to training datasets, although these methods transfer more information than the original KD. Next, we present the simple observations to mitigate the overfitting of ILD: distilling only the last Transformer layer and conducting ILD on supplementary tasks. Based on our two findings, we propose a simple yet effective consistency-regularized ILD (CR-ILD), which prevents the student model from overfitting the training dataset. Substantial experiments on distilling BERT on the GLUE benchmark and several synthetic datasets demonstrate that our proposed ILD method outperforms other KD techniques. Our code is available at https: //github.com/jongwooko/CR-ILD."><td>17</td><td>Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective</td><td>0.405</td></tr><tr class="has-abs" data-abs="Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long inference time, limit the deployment of such models in realtime applications. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn singledomain student models with guidance from the meta-teacher. Experiments on public multidomain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce."><td>18</td><td>Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains</td><td>0.405</td></tr><tr class="has-abs" data-abs="Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F 1 -score for NER over 41 languages."><td>19</td><td>XtremeDistil: Multi-stage Distillation for Massive Multilingual Models</td><td>0.409</td></tr><tr class="has-abs" data-abs="Knowledge Distillation (KD) offers a natural way to reduce the latency and memory/energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets/tasks, such as the data augmentation policy, the loss function, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component's contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MIα objective functions with better bias/variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following: 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-α performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks. Moreover, we find that different datasets/tasks prefer different KD algorithms, and thus propose a simple AutoDistiller algorithm that can recommend a good KD pipeline for a new dataset."><td>20</td><td>Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing</td><td>0.411</td></tr></tbody></table>
  </div>
  <div class="card">
    <h2>After&nbsp;(augmented)</h2>
    <table id="afterTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody></tbody></table>
  </div>
</section>

<!-- METRICS ------------------------------------------------------------->
<section id="metricsCard" class="card" style="">
  <h2>Metrics</h2>
  <div id="recallLine" class="small">Recall </div>

  <div class="tabs">
    <button id="tab1" class="active">Δ recall</button>
    <button id="tab2">Rank maps</button>
  </div>

  <div id="body1" class="tab-body active">
    <p id="deltaLine" class="small"></p>
  </div>
  <div id="body2" class="tab-body">
    <h3 class="small" style="margin-top:0;">ranks_before</h3>
    <pre id="rb" class="small">{
  "202719327": 9999999999
}</pre>
    <h3 class="small">ranks_after</h3>
    <pre id="ra" class="small"></pre>
  </div>
</section>




```
</body></html>