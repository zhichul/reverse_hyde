<!doctype html>
<html lang="en"><head></head><body>```html



<meta charset="utf-8">
<title>Prompt‑HyDE playground</title>
<style>
  body{font-family:system-ui,sans-serif;margin:0;padding:1rem 2rem;}
  h2{margin-top:2rem;}
  label{display:block;margin-top:.75rem;font-weight:600;}
  select,input[type=text],input[type=number],textarea{width:100%;padding:.4rem;}
  textarea{height:120px;font-family:monospace;}
  button{margin-top:1rem;padding:.5rem 1rem;cursor:pointer;}
  table{border-collapse:collapse;width:100%;margin-top:.5rem;}
  th,td{border:1px solid #ccc;padding:.25rem .5rem;text-align:left;vertical-align:top;}
  em.rev{font-style:italic;color:#9146ff;}
  strong.rel{color:#0b63ff;font-weight:700;}
  .flex{display:flex;gap:2rem;flex-wrap:wrap;}
  .card{flex:1 1 320px;border:1px solid #ddd;padding:1rem;}
  .small{font-size:.85rem;color:#555;margin:.25rem 0;}

  /* tooltip for abstracts */
  .has-abs{position:relative;}
  .has-abs:hover::after{
    content:attr(data-abs);
    position:absolute;left:0;top:100%;z-index:999;
    max-width:420px;white-space:pre-wrap;font-size:.8rem;
    background:#333;color:#fff;padding:.5rem;border-radius:.25rem;
    box-shadow:0 2px 6px rgba(0,0,0,.35);
  }

  /* tab styling */
  .tabs{display:flex;gap:.5rem;margin-bottom:.5rem;}
  .tabs button{padding:.25rem .75rem;border:1px solid #aaa;background:#eee;}
  .tabs button.active{background:#fff;border-bottom:none;font-weight:600;}
  .tab-body{display:none;}
  .tab-body.active{display:block;}
</style>


<h1>Prompt‑HyDE UI (vanilla&nbsp;JS)</h1>

<!-- CONFIG -------------------------------------------------------------->
<section class="card">
  <h2>Server configuration</h2>
  <pre id="cfg" class="small">{
  "embed_model": "grit",
  "index_path": "../0721_litsearch_example/faiss/litsearch.index",
  "backend": "faiss",
  "query_dataset": "../0721_litsearch_example/query_with_score.parquet",
  "corpus_dataset": "../0721_litsearch_example/corpus_clean_dedup.parquet",
  "prompt_dir": "prompts",
  "extractor_dir": "extractors",
  "annotation_dir": "annotations",
  "ui_config": "configs/ui_config.json",
  "host": "0.0.0.0",
  "port": 8200,
  "id_field": "corpusid",
  "relevant_documents_field": "corpusids"
}</pre>
</section>

<section class="flex">
  <!-- DATA CARD -->
  <div class="card" style="max-width:440px;">
    <h2>Data</h2>

    <label>Query</label>
    <select id="querySel"><option value="0">0: [inline_acl] sp=0 qual=2 gr=0 — Are there any research papers on methods to compre…</option><option value="3">3: [inline_acl] sp=1 qual=2 gr=0 — Are there any tools or studies that have focused o…</option><option value="4">4: [inline_acl] sp=1 qual=2 gr=0 — Are there papers that propose contextualized calib…</option><option value="10">10: [inline_acl] sp=1 qual=2 gr=0 — Can you point me to a work that uses diagnostic to…</option><option value="11">11: [inline_acl] sp=0 qual=1 gr=0 — Can you point me to studies discussing methods for…</option><option value="17">17: [inline_acl] sp=1 qual=2 gr=0 — Can you recommend a paper that uses an NLI model f…</option><option value="24">24: [inline_acl] sp=0 qual=1 gr=0 — Can you suggest recent studies that have integrate…</option><option value="30">30: [inline_acl] sp=0 qual=1 gr=0 — Could you point me to studies that have investigat…</option><option value="35">35: [inline_acl] sp=1 qual=2 gr=0 — Could you suggest a paper that introduces an appro…</option><option value="39">39: [inline_acl] sp=1 qual=2 gr=0 — I am looking for research that has explored topic …</option><option value="44">44: [inline_acl] sp=1 qual=2 gr=0 — I'm exploring research that utilizes large dataset…</option><option value="49">49: [inline_acl] sp=1 qual=2 gr=0 — I'm looking for innovative approaches to data anno…</option><option value="55">55: [inline_acl] sp=1 qual=2 gr=0 — In discourse parsing literature, which works have …</option><option value="56">56: [inline_acl] sp=1 qual=2 gr=0 — In researching metrics for human-interaction with …</option><option value="58">58: [inline_acl] sp=1 qual=2 gr=0 — In the context of Named Entity Recognition tasks a…</option><option value="59">59: [inline_acl] sp=0 qual=1 gr=0 — In the context of machine translation, can you poi…</option><option value="61">61: [inline_acl] sp=1 qual=2 gr=0 — In the context of simultaneous machine translation…</option><option value="62">62: [inline_acl] sp=1 qual=2 gr=0 — In the field of reinforcement learning models for …</option><option value="63">63: [inline_acl] sp=1 qual=2 gr=0 — What approaches have been used to address the limi…</option><option value="71">71: [inline_acl] sp=1 qual=2 gr=0 — What are the recent developments in evaluating the…</option><option value="75">75: [inline_acl] sp=0 qual=1 gr=0 — What research could I reference to understand the …</option><option value="84">84: [inline_acl] sp=1 qual=2 gr=0 — Where can I find a multilingual corpus that includ…</option><option value="109">109: [inline_nonacl] sp=1 qual=2 gr=0 — Can you recommend research that uses an LLM to gen…</option><option value="110">110: [inline_nonacl] sp=0 qual=1 gr=0 — Can you show me a paper that built a large structu…</option><option value="111">111: [inline_nonacl] sp=1 qual=2 gr=0 — Can you suggest research that deals with the multi…</option><option value="119">119: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that examines how inco…</option><option value="123">123: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that explores how lang…</option><option value="124">124: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores mitigati…</option><option value="126">126: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores the diff…</option><option value="128">128: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that initializes embed…</option><option value="132">132: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that investigates enha…</option><option value="135">135: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="136">136: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates how …</option><option value="137">137: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="142">142: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates the …</option><option value="143">143: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates the …</option><option value="145">145: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that uses feedback-dri…</option><option value="149">149: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research papers that investiga…</option><option value="155">155: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that evaluates the pe…</option><option value="157">157: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that examines how dec…</option><option value="163">163: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that explores how the…</option><option value="168">168: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates app…</option><option value="169">169: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates gen…</option><option value="176">176: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates the…</option><option value="179">179: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research that proposed enhanci…</option><option value="194">194: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest a study that explores data annot…</option><option value="195">195: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that explores employing …</option><option value="197">197: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest a study that explores the idea o…</option><option value="199">199: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that investigates the in…</option><option value="205">205: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest papers that tackle conversationa…</option><option value="208">208: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that concentrates on pi…</option><option value="209">209: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines a system …</option><option value="213">213: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that examines how stran…</option><option value="221">221: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines the effec…</option><option value="224">224: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores generatin…</option><option value="226">226: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores the idea …</option><option value="228">228: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that includes an online…</option><option value="231">231: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that investigates apply…</option><option value="238">238: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that investigates how c…</option><option value="244">244: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest research that investigates the u…</option><option value="251">251: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest studies focused on emotion-class…</option><option value="252">252: [inline_nonacl] sp=1 qual=2 gr=0 — Has any research explored using other off-the-shel…</option><option value="260">260: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research efforts been made to gather dial…</option><option value="261">261: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers been published on models …</option><option value="265">265: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers examined whether using la…</option><option value="269">269: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research papers investigated the creation…</option><option value="272">272: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers tried to create conversat…</option><option value="278">278: [inline_nonacl] sp=1 qual=2 gr=0 — I know about prompt tuning, but have any works tri…</option><option value="297">297: [inline_nonacl] sp=1 qual=2 gr=0 — What are some scholarly articles that explore the …</option><option value="306">306: [inline_nonacl] sp=0 qual=1 gr=0 — What papers discuss the effect of false negatives …</option><option value="307">307: [inline_nonacl] sp=1 qual=2 gr=0 — What papers explore replacing schema linking with …</option><option value="311">311: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists comparing adapter-based tunin…</option><option value="312">312: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists on employing generative model…</option><option value="325">325: [inline_nonacl] sp=0 qual=1 gr=0 — What research is available on acquiring sentence e…</option><option value="326">326: [inline_nonacl] sp=0 qual=2 gr=0 — What research is available on hybrid approaches th…</option><option value="331">331: [inline_nonacl] sp=0 qual=1 gr=0 — What techniques and frameworks have been suggested…</option><option value="346">346: [inline_nonacl] sp=1 qual=1 gr=0 — Which work shows that only emplying instance-level…</option><option value="347">347: [inline_nonacl] sp=1 qual=2 gr=0 — Which work shows that reducing the number of train…</option><option value="353">353: [manual_acl] sp=1 qual=2 gr=0 — Are there any papers that build dense retrievers w…</option><option value="360">360: [manual_acl] sp=1 qual=2 gr=0 — If one would like to train (or evaluate) a helpful…</option><option value="364">364: [manual_acl] sp=1 qual=2 gr=0 — Is there a decoder-only language model that does n…</option><option value="366">366: [manual_acl] sp=1 qual=2 gr=0 — Is there a method for measuring the critical error…</option><option value="369">369: [manual_acl] sp=0 qual=1 gr=0 — Is there a paper exploring the curse of multilingu…</option><option value="372">372: [manual_acl] sp=0 qual=2 gr=0 — Is there a paper that links exposure bias to disti…</option><option value="378">378: [manual_acl] sp=1 qual=2 gr=0 — Is there a paper that uses similarity scores to ch…</option><option value="382">382: [manual_acl] sp=1 qual=2 gr=0 — Is there a tool that can automatically segment spe…</option><option value="389">389: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that aligns speech and text emb…</option><option value="395">395: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that constructs augmented train…</option><option value="404">404: [manual_acl] sp=0 qual=2 gr=0 — Is there any paper that proposes a set of criteria…</option><option value="411">411: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that utilizes Gaussian processe…</option><option value="414">414: [manual_acl] sp=1 qual=1 gr=0 — Is there any research paper that can extract attri…</option><option value="417">417: [manual_acl] sp=1 qual=2 gr=0 — Is there any works that explores how to achieve ba…</option><option value="423">423: [manual_acl] sp=0 qual=2 gr=0 — What are some methods for solving the class-increm…</option><option value="426">426: [manual_acl] sp=0 qual=1 gr=0 — What is the performance of large language models i…</option><option value="433">433: [manual_acl] sp=1 qual=2 gr=0 — Which is the first multimodal model combining text…</option><option value="451">451: [manual_acl] sp=1 qual=2 gr=0 — Which paper first constructed a structured knowled…</option><option value="458">458: [manual_acl] sp=1 qual=1 gr=0 — Which paper first proposed shared adapter module a…</option><option value="467">467: [manual_acl] sp=0 qual=1 gr=0 — Which paper found that mutual learning benefits mu…</option><option value="473">473: [manual_acl] sp=1 qual=1 gr=0 — Which paper is among the earliest to train on exte…</option><option value="475">475: [manual_acl] sp=0 qual=1 gr=0 — Which paper makes sure that the questions used in …</option><option value="498">498: [manual_acl] sp=1 qual=2 gr=0 — Which research paper leverages event structure inf…</option><option value="502">502: [manual_acl] sp=1 qual=2 gr=0 — Which work discusses an analysis of source and tar…</option><option value="510">510: [manual_iclr] sp=1 qual=2 gr=0 — Are there any papers that use a world model for pl…</option><option value="522">522: [manual_iclr] sp=1 qual=2 gr=0 — Is there a parameter-efficient fine-tuning method …</option><option value="550">550: [manual_iclr] sp=1 qual=1 gr=0 — What paper first showed that you can score the cod…</option><option value="557">557: [manual_iclr] sp=0 qual=1 gr=0 — What paper proposes breaking down programming prob…</option><option value="560">560: [manual_iclr] sp=1 qual=1 gr=0 — What research first proposed a new kind of cascade…</option><option value="564">564: [manual_iclr] sp=1 qual=2 gr=0 — Which backdoor paper first used the CLIP to suppre…</option><option value="566">566: [manual_iclr] sp=1 qual=1 gr=0 — Which is one of the first papers to highlight and …</option><option value="595">595: [manual_iclr] sp=0 qual=2 gr=0 — What paper provides generalization bounds for self…</option><option value="18">18: [inline_acl] sp=0 qual=2 gr=0.25 — Can you recommend some literature that focuses on …</option><option value="2">2: [inline_acl] sp=0 qual=2 gr=0.5 — Are there any studies that explore post-hoc techni…</option><option value="12">12: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me to studies that explore the impac…</option><option value="13">13: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me towards research on contrastive l…</option><option value="53">53: [inline_acl] sp=1 qual=2 gr=0.5 — I'm researching on the efficacy of recurrent netwo…</option><option value="74">74: [inline_acl] sp=1 qual=2 gr=0.5 — What prior works suggested that exposure bias coul…</option><option value="92">92: [inline_acl] sp=0 qual=1 gr=0.5 — Where might I find research on the evaluation of c…</option><option value="102">102: [inline_nonacl] sp=1 qual=2 gr=0.5 — Are there any studies investigating example-based …</option><option value="113">113: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you direct me to studies investigating the e…</option><option value="150">150: [inline_nonacl] sp=0 qual=1 gr=0.5 — Could you recommend research that analyses prompt …</option><option value="156">156: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you recommend research that examines how an …</option><option value="235">235: [inline_nonacl] sp=1 qual=1 gr=0.5 — Could you suggest research that investigates enhan…</option><option value="257">257: [inline_nonacl] sp=0 qual=2 gr=0.5 — Have any new metrics been developed to assess the …</option><option value="328">328: [inline_nonacl] sp=0 qual=2 gr=0.5 — What research should I consult regarding the appli…</option><option value="334">334: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques exist to enhance the few-shot fine…</option><option value="335">335: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques have been investigated to enhance …</option><option value="96">96: [inline_acl] sp=0 qual=2 gr=0.6 — Which studies should I look into that have explore…</option><option value="8">8: [inline_acl] sp=0 qual=1 gr=0.6666666666666666 — Can you list some publications that discuss the ev…</option><option value="23">23: [inline_acl] sp=0 qual=2 gr=0.6666666666666666 — Can you suggest literature on enhanced semantic pa…</option><option value="1">1: [inline_acl] sp=1 qual=2 gr=1 — Are there any resources available for translating …</option><option value="5">5: [inline_acl] sp=1 qual=2 gr=1 — Are there studies that combine convolutional and r…</option><option value="6">6: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to research that explores method…</option><option value="7">7: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to studies that explore techniqu…</option><option value="9">9: [inline_acl] sp=0 qual=2 gr=1 — Can you point me to a paper that discussed transfo…</option><option value="14">14: [inline_acl] sp=0 qual=1 gr=1 — Can you point to studies or tasks focused on detec…</option><option value="15">15: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a conversational QA dataset wher…</option><option value="16">16: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a foundational paper that provid…</option><option value="19">19: [inline_acl] sp=1 qual=2 gr=1 — Can you refer me to research that adapts the conce…</option><option value="20">20: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest a corpus that contains French ency…</option><option value="21">21: [inline_acl] sp=0 qual=2 gr=1 — Can you suggest any literature that explores the i…</option><option value="22">22: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest literature on a dataset that categ…</option><option value="25">25: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some literature that evaluates the…</option><option value="26">26: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some recent datasets that have bee…</option><option value="27">27: [inline_acl] sp=1 qual=2 gr=1 — Could you direct me towards a study that explores …</option><option value="28">28: [inline_acl] sp=1 qual=2 gr=1 — Could you point me to research on binary classific…</option><option value="29">29: [inline_acl] sp=0 qual=2 gr=1 — Could you point me to studies that discuss the dev…</option><option value="31">31: [inline_acl] sp=1 qual=1 gr=1 — Could you point me toward some large-scale multili…</option><option value="32">32: [inline_acl] sp=1 qual=2 gr=1 — Could you provide me with a reference that discuss…</option><option value="33">33: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend datasets that include SQL anno…</option><option value="34">34: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend studies that provide a baselin…</option><option value="36">36: [inline_acl] sp=0 qual=1 gr=1 — Could you suggest studies that employ novel method…</option><option value="37">37: [inline_acl] sp=1 qual=2 gr=1 — Has there been any recent work or competitions foc…</option><option value="38">38: [inline_acl] sp=0 qual=1 gr=1 — I am exploring state-of-the-art techniques in lang…</option><option value="40">40: [inline_acl] sp=0 qual=2 gr=1 — I am looking to understand more about sequence-to-…</option><option value="41">41: [inline_acl] sp=1 qual=2 gr=1 — I would like to understand the theoretical basis f…</option><option value="42">42: [inline_acl] sp=1 qual=2 gr=1 — I'm conducting research on computational humor and…</option><option value="43">43: [inline_acl] sp=1 qual=1 gr=1 — I'm exploring efficient transformer architectures …</option><option value="45">45: [inline_acl] sp=0 qual=2 gr=1 — I'm exploring ways to enhance question answering s…</option><option value="46">46: [inline_acl] sp=1 qual=2 gr=1 — I'm interested in understanding how perplexity is …</option><option value="47">47: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a comprehensive dataset that has b…</option><option value="48">48: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a paper that discusses improvement…</option><option value="50">50: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into morphological embedding algorithm…</option><option value="51">51: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into the distillation process of langu…</option><option value="52">52: [inline_acl] sp=0 qual=1 gr=1 — I'm researching insertion-based decoding methods f…</option><option value="54">54: [inline_acl] sp=1 qual=1 gr=1 — I'm searching for studies that explore advancement…</option><option value="57">57: [inline_acl] sp=1 qual=2 gr=1 — In the area of argument mining, could you point to…</option><option value="60">60: [inline_acl] sp=1 qual=2 gr=1 — In the context of natural language processing, I a…</option><option value="64">64: [inline_acl] sp=1 qual=1 gr=1 — What are some approaches to generating sports news…</option><option value="65">65: [inline_acl] sp=1 qual=2 gr=1 — What are some good datasets for conversational que…</option><option value="66">66: [inline_acl] sp=1 qual=2 gr=1 — What are some of the key papers to look at for und…</option><option value="67">67: [inline_acl] sp=0 qual=2 gr=1 — What are some recent advancements in training syst…</option><option value="68">68: [inline_acl] sp=1 qual=2 gr=1 — What are some soft-constrained methods proposed in…</option><option value="69">69: [inline_acl] sp=1 qual=2 gr=1 — What are some studies that leverage statistical ma…</option><option value="70">70: [inline_acl] sp=0 qual=2 gr=1 — What are some techniques or tools used in machine …</option><option value="72">72: [inline_acl] sp=1 qual=2 gr=1 — What paper should I look at if I am interested in …</option><option value="73">73: [inline_acl] sp=1 qual=2 gr=1 — What papers should I refer to if I want to explore…</option><option value="76">76: [inline_acl] sp=1 qual=2 gr=1 — What research has been done on annotating user com…</option><option value="77">77: [inline_acl] sp=0 qual=1 gr=1 — What research has been done on improving named ent…</option><option value="78">78: [inline_acl] sp=1 qual=1 gr=1 — What research should I explore to understand metho…</option><option value="79">79: [inline_acl] sp=1 qual=2 gr=1 — When using pretrained transformer models for gener…</option><option value="80">80: [inline_acl] sp=0 qual=1 gr=1 — Where can I find a corpus of CCG annotations for n…</option><option value="81">81: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a detailed discussion on automati…</option><option value="82">82: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a discourse treebank tailored to …</option><option value="83">83: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a large corpus of annotated socia…</option><option value="85">85: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a paper that discusses annotating…</option><option value="86">86: [inline_acl] sp=0 qual=1 gr=1 — Where can I find guidelines on standard practices …</option><option value="87">87: [inline_acl] sp=0 qual=1 gr=1 — Where can I find information on self-attentive par…</option><option value="88">88: [inline_acl] sp=0 qual=1 gr=1 — Where can I find interdisciplinary research that i…</option><option value="89">89: [inline_acl] sp=1 qual=2 gr=1 — Where can I find multilingual datasets used for th…</option><option value="90">90: [inline_acl] sp=1 qual=2 gr=1 — Where can I find research about automatic evaluati…</option><option value="91">91: [inline_acl] sp=0 qual=1 gr=1 — Where might I find a dataset annotated specificall…</option><option value="93">93: [inline_acl] sp=1 qual=2 gr=1 — Which corpora are frequently used in research to b…</option><option value="94">94: [inline_acl] sp=1 qual=2 gr=1 — Which paper specifies the typical configurations u…</option><option value="95">95: [inline_acl] sp=0 qual=1 gr=1 — Which papers should I refer to for learning about …</option><option value="97">97: [inline_acl] sp=1 qual=2 gr=1 — Which work should I explore to understand the tech…</option><option value="98">98: [inline_nonacl] sp=1 qual=1 gr=1 — *Could you suggest a dataset with legally or ethic…</option><option value="99">99: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any papers on training video-language mo…</option><option value="100">100: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any recent papers investigating the use …</option><option value="101">101: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any research papers investigating the im…</option><option value="103">103: [inline_nonacl] sp=1 qual=2 gr=1 — Are there any studies investigating sentiment anal…</option><option value="104">104: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any studies on incorporating external co…</option><option value="105">105: [inline_nonacl] sp=1 qual=2 gr=1 — Are there studies examining how well question answ…</option><option value="106">106: [inline_nonacl] sp=1 qual=1 gr=1 — Are there studies that investigate debiasing langu…</option><option value="107">107: [inline_nonacl] sp=0 qual=1 gr=1 — Can you give me a paper that does self-supervised …</option><option value="108">108: [inline_nonacl] sp=0 qual=1 gr=1 — Can you recommend a dialogue summarization dataset…</option><option value="112">112: [inline_nonacl] sp=1 qual=2 gr=1 — Could you direct me to research that evaluates few…</option><option value="114">114: [inline_nonacl] sp=1 qual=1 gr=1 — Could you point me to research that tackles the is…</option><option value="115">115: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a contemporary research paper …</option><option value="116">116: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a paper that builds a writing …</option><option value="117">117: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that does data-augment…</option><option value="118">118: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that examines how cros…</option><option value="120">120: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that examines the intr…</option><option value="121">121: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores a pre-tr…</option><option value="122">122: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that explores employin…</option><option value="125">125: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that explores strategi…</option><option value="127">127: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores the impr…</option><option value="129">129: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates empl…</option><option value="130">130: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates empl…</option><option value="131">131: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates enha…</option><option value="133">133: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates grap…</option><option value="134">134: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates guid…</option><option value="138">138: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates inco…</option><option value="139">139: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates know…</option><option value="140">140: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates repr…</option><option value="141">141: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates text…</option><option value="144">144: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates the …</option><option value="146">146: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend articles that explore the role…</option><option value="147">147: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research articles that explore…</option><option value="148">148: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research papers that explore a…</option><option value="151">151: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that assesses how wel…</option><option value="152">152: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that assesses how wel…</option><option value="153">153: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that assesses techniq…</option><option value="154">154: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that employs a relaxe…</option><option value="158">158: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how mul…</option><option value="159">159: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how opt…</option><option value="160">160: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that examines how syn…</option><option value="161">161: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the cha…</option><option value="162">162: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the eff…</option><option value="164">164: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that explores identif…</option><option value="165">165: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that has introduced a…</option><option value="166">166: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that improves knowled…</option><option value="167">167: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend research that introduces a met…</option><option value="170">170: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="171">171: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="172">172: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates mer…</option><option value="173">173: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates met…</option><option value="174">174: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates tec…</option><option value="175">175: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="177">177: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="178">178: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates usi…</option><option value="180">180: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend scholarly articles that invest…</option><option value="181">181: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend studies on hierarchical modeli…</option><option value="182">182: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that concentrate on an…</option><option value="183">183: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that investigate fine-…</option><option value="184">184: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend studies that tackle the issue …</option><option value="185">185: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies which explore how to o…</option><option value="186">186: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a dataset containing diverse, in…</option><option value="187">187: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a dataset for question-answering…</option><option value="188">188: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a research article that explores…</option><option value="189">189: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study examining how transforme…</option><option value="190">190: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that evaluates cross-enc…</option><option value="191">191: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that examines how well c…</option><option value="192">192: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a cohesive…</option><option value="193">193: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a compress…</option><option value="196">196: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores improved t…</option><option value="198">198: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores the use of…</option><option value="200">200: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a study that proposes high-param…</option><option value="201">201: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a thorough comparative analysis …</option><option value="202">202: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a triplet-formatted structured d…</option><option value="203">203: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest an article that leverages the sp…</option><option value="204">204: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest datasets that can benchmark LLM …</option><option value="206">206: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research on detecting common err…</option><option value="207">207: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that assesses if langua…</option><option value="210">210: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how coref…</option><option value="211">211: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="212">212: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="214">214: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that examines how the o…</option><option value="215">215: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="216">216: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines how well …</option><option value="217">217: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="218">218: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines the appli…</option><option value="219">219: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines the chall…</option><option value="220">220: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines the diffi…</option><option value="222">222: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores a pre-tra…</option><option value="223">223: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores employing…</option><option value="225">225: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores the drawb…</option><option value="227">227: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores the impro…</option><option value="229">229: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates a clu…</option><option value="230">230: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates acros…</option><option value="232">232: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates aspec…</option><option value="233">233: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates effic…</option><option value="234">234: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates emplo…</option><option value="236">236: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates expan…</option><option value="237">237: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates graph…</option><option value="239">239: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how h…</option><option value="240">240: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how m…</option><option value="241">241: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that investigates how n…</option><option value="242">242: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates how u…</option><option value="243">243: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates impro…</option><option value="245">245: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates train…</option><option value="246">246: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that offers an in-depth…</option><option value="247">247: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that shows multilingual…</option><option value="248">248: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that trains language mo…</option><option value="249">249: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that tries to interpret…</option><option value="250">250: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest some work that develops multimod…</option><option value="253">253: [inline_nonacl] sp=1 qual=2 gr=1 — Has any research tried to mitigate overfitting in …</option><option value="254">254: [inline_nonacl] sp=1 qual=1 gr=1 — Has any study explored the zero-shot extraction of…</option><option value="255">255: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any research that uses multiple mod…</option><option value="256">256: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any work that improves the work on …</option><option value="258">258: [inline_nonacl] sp=1 qual=2 gr=1 — Have any papers tried to address the background-sh…</option><option value="259">259: [inline_nonacl] sp=1 qual=2 gr=1 — Have any recent publications explored the use of n…</option><option value="262">262: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers collected feedback from r…</option><option value="263">263: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers critically analyzed the p…</option><option value="264">264: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers examined the efficacy of …</option><option value="266">266: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers explored methods to impro…</option><option value="267">267: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers introduced a dedicated pr…</option><option value="268">268: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers investigated human capaci…</option><option value="270">270: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers suggested methods for sum…</option><option value="271">271: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers suggested techniques for …</option><option value="273">273: [inline_nonacl] sp=1 qual=1 gr=1 — Have any studies explored the creation of memory m…</option><option value="274">274: [inline_nonacl] sp=1 qual=2 gr=1 — Have there been any advancements in language model…</option><option value="275">275: [inline_nonacl] sp=1 qual=2 gr=1 — How can I locate a dataset containing toxic senten…</option><option value="276">276: [inline_nonacl] sp=1 qual=2 gr=1 — How can SQL-to-text be utilized to improve text-to…</option><option value="277">277: [inline_nonacl] sp=0 qual=1 gr=1 — How can dense retrieval models for open-domain que…</option><option value="279">279: [inline_nonacl] sp=1 qual=2 gr=1 — In multi-hop question answering, is there a paper …</option><option value="280">280: [inline_nonacl] sp=1 qual=2 gr=1 — Is it possible to adatp named entity recognition s…</option><option value="281">281: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a benchmark designed to assess language m…</option><option value="282">282: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a comprehensive dataset available for sum…</option><option value="283">283: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a dataset available for open-domain targe…</option><option value="284">284: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a dataset containing question-answer pair…</option><option value="285">285: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a research paper that has developed a cus…</option><option value="286">286: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a specialized question answering dataset …</option><option value="287">287: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a study that investigates if large langua…</option><option value="288">288: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any paper that tried fine-tuning mBERT to…</option><option value="289">289: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any research that investigates how to use…</option><option value="290">290: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research examining if multilingual pre-tr…</option><option value="291">291: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research on a specialized language model …</option><option value="292">292: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research that argues for transparency and…</option><option value="293">293: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research that investigates embedding mult…</option><option value="294">294: [inline_nonacl] sp=1 qual=2 gr=1 — Is there work on text classification that explores…</option><option value="295">295: [inline_nonacl] sp=1 qual=2 gr=1 — What approaches have been suggested to lower the c…</option><option value="296">296: [inline_nonacl] sp=1 qual=1 gr=1 — What are some scholarly articles that explore scal…</option><option value="298">298: [inline_nonacl] sp=0 qual=2 gr=1 — What are some studies that explore data-poisoning …</option><option value="299">299: [inline_nonacl] sp=1 qual=1 gr=1 — What are the latest advancements in predicting sui…</option><option value="300">300: [inline_nonacl] sp=0 qual=1 gr=1 — What are the latest developments in conversational…</option><option value="301">301: [inline_nonacl] sp=0 qual=1 gr=1 — What benchmarks have prior research utilized to as…</option><option value="302">302: [inline_nonacl] sp=1 qual=2 gr=1 — What concerns or key points have been highlighted …</option><option value="303">303: [inline_nonacl] sp=0 qual=2 gr=1 — What difficulties do neural conversational models …</option><option value="304">304: [inline_nonacl] sp=1 qual=2 gr=1 — What literature is available on training semantic …</option><option value="305">305: [inline_nonacl] sp=1 qual=2 gr=1 — What methods exist for tailoring news suggestions …</option><option value="308">308: [inline_nonacl] sp=1 qual=1 gr=1 — What recent developments in transformer architectu…</option><option value="309">309: [inline_nonacl] sp=1 qual=1 gr=1 — What recent research has been conducted on improvi…</option><option value="310">310: [inline_nonacl] sp=1 qual=1 gr=1 — What research articles should I consult to underst…</option><option value="313">313: [inline_nonacl] sp=0 qual=1 gr=1 — What research exists on incorporating knowledge gr…</option><option value="314">314: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on leveraging syntactic roles…</option><option value="315">315: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on the impact of scaling on p…</option><option value="316">316: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on using reinforcement learni…</option><option value="317">317: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on applying contr…</option><option value="318">318: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on creating neura…</option><option value="319">319: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on determining th…</option><option value="320">320: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on enhancing conv…</option><option value="321">321: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on incorporating …</option><option value="322">322: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on news recommend…</option><option value="323">323: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on scaling within…</option><option value="324">324: [inline_nonacl] sp=0 qual=2 gr=1 — What research has been conducted on the impact of …</option><option value="327">327: [inline_nonacl] sp=1 qual=1 gr=1 — What research is available on the concept of using…</option><option value="329">329: [inline_nonacl] sp=1 qual=2 gr=1 — What resources or toolkits are available to facili…</option><option value="330">330: [inline_nonacl] sp=1 qual=1 gr=1 — What sources offer research on maintaining factual…</option><option value="332">332: [inline_nonacl] sp=0 qual=1 gr=1 — What techniques exist for efficiently fine-tuning …</option><option value="333">333: [inline_nonacl] sp=0 qual=2 gr=1 — What techniques exist for incorporating context in…</option><option value="336">336: [inline_nonacl] sp=1 qual=1 gr=1 — Where can I find a database of good prompts to use…</option><option value="337">337: [inline_nonacl] sp=0 qual=1 gr=1 — Where can I read about the using soft embeddings t…</option><option value="338">338: [inline_nonacl] sp=1 qual=2 gr=1 — Which method involves training additional prompt t…</option><option value="339">339: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper has conducted a thorough analysis of h…</option><option value="340">340: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper introduced the task of creating extend…</option><option value="341">341: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper presents a platform that emphasizes ev…</option><option value="342">342: [inline_nonacl] sp=0 qual=1 gr=1 — Which paper shows that generated captions of model…</option><option value="343">343: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper shows that human experts and non-exper…</option><option value="344">344: [inline_nonacl] sp=0 qual=1 gr=1 — Which work introduces sparse attention modules and…</option><option value="345">345: [inline_nonacl] sp=1 qual=1 gr=1 — Which work pushes the limit of model quantization …</option><option value="348">348: [inline_nonacl] sp=1 qual=1 gr=1 — Which work suggests that machine translation model…</option><option value="349">349: [inline_nonacl] sp=1 qual=1 gr=1 — Which works shows that training large language mod…</option><option value="350">350: [inline_nonacl] sp=1 qual=2 gr=1 — ould you direct me to research that shows that the…</option><option value="351">351: [manual_acl] sp=1 qual=1 gr=1 — Are there any examples of using dense phrase retri…</option><option value="352">352: [manual_acl] sp=1 qual=1 gr=1 — Are there any large-scale and open-source text sim…</option><option value="354">354: [manual_acl] sp=1 qual=1 gr=1 — Could you recommend a dataset paper which presents…</option><option value="355">355: [manual_acl] sp=1 qual=1 gr=1 — Find the NLP paper that focuses on dialogue genera…</option><option value="356">356: [manual_acl] sp=0 qual=1 gr=1 — Give me a paper proposing to circumvent a single-t…</option><option value="357">357: [manual_acl] sp=1 qual=2 gr=1 — How to achieve zero-shot lip reading?…</option><option value="358">358: [manual_acl] sp=1 qual=2 gr=1 — How to better attract readers to news articles by …</option><option value="359">359: [manual_acl] sp=0 qual=1 gr=1 — How to faithfully and explicitly measure the helpf…</option><option value="361">361: [manual_acl] sp=1 qual=1 gr=1 — In multimodal (multilingual) abstractive summariza…</option><option value="362">362: [manual_acl] sp=1 qual=1 gr=1 — Is there a Chinese hate speech paper that construc…</option><option value="363">363: [manual_acl] sp=0 qual=1 gr=1 — Is there a dataset that allows to perform aspect-b…</option><option value="365">365: [manual_acl] sp=1 qual=1 gr=1 — Is there a dialogue dataset where a speaker's utte…</option><option value="367">367: [manual_acl] sp=1 qual=2 gr=1 — Is there a method that measures the information pr…</option><option value="368">368: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper comparing knowledge distillation …</option><option value="370">370: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that applies large language model…</option><option value="371">371: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that connects the basic elements …</option><option value="373">373: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that shows that language models' …</option><option value="374">374: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that supports the use of automate…</option><option value="375">375: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that uses Explainable AI techniqu…</option><option value="376">376: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses an app for a popular ta…</option><option value="377">377: [manual_acl] sp=0 qual=1 gr=1 — Is there a paper that uses evolutionary algorithms…</option><option value="379">379: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses the tree structure of m…</option><option value="380">380: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that utilizes the characteristics…</option><option value="381">381: [manual_acl] sp=1 qual=2 gr=1 — Is there a study that shows how to help the demons…</option><option value="383">383: [manual_acl] sp=1 qual=1 gr=1 — Is there an evaluation metric for natural language…</option><option value="384">384: [manual_acl] sp=1 qual=1 gr=1 — Is there any dataset that contains minimally-contr…</option><option value="385">385: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper about style transfer for storie…</option><option value="386">386: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper exploring real speakers and thu…</option><option value="387">387: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper leverages knowledge distillatio…</option><option value="388">388: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that address attacks on code mo…</option><option value="390">390: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that applies curriculum learnin…</option><option value="391">391: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that applies symbolic distillat…</option><option value="392">392: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that attempts to evaluate the s…</option><option value="393">393: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that automatically creates a da…</option><option value="394">394: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that combines causal inference …</option><option value="396">396: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that employs code LLMs to itera…</option><option value="397">397: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores and annotates the…</option><option value="398">398: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores using only an enc…</option><option value="399">399: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that investigates backdoor atta…</option><option value="400">400: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that leverages graph neural net…</option><option value="401">401: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that leverages syntactic rules …</option><option value="402">402: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that performs adversarial train…</option><option value="403">403: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that proposes a new multimodal …</option><option value="405">405: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that reveals annotation problem…</option><option value="406">406: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that studies a teacher AI infer…</option><option value="407">407: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that tries to investigate LLMs’…</option><option value="408">408: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that uses data collected from t…</option><option value="409">409: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses prompt tuning in mult…</option><option value="410">410: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses token-level loss to e…</option><option value="412">412: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that utilizes graph structure t…</option><option value="413">413: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that utilizes masked language m…</option><option value="415">415: [manual_acl] sp=1 qual=2 gr=1 — Is there any work that allows large numbers of mod…</option><option value="416">416: [manual_acl] sp=1 qual=1 gr=1 — Is there any work that attacks language models in …</option><option value="418">418: [manual_acl] sp=1 qual=2 gr=1 — Is there commonsense reasoning dataset which gener…</option><option value="419">419: [manual_acl] sp=1 qual=2 gr=1 — Is there such a factuality evaluation dataset that…</option><option value="420">420: [manual_acl] sp=1 qual=2 gr=1 — Is there such a reading comprehension dataset in u…</option><option value="421">421: [manual_acl] sp=1 qual=1 gr=1 — Provide an example of a paper which proposes a met…</option><option value="422">422: [manual_acl] sp=1 qual=2 gr=1 — What are some data-efficient ways to learn text em…</option><option value="424">424: [manual_acl] sp=0 qual=1 gr=1 — What is a large event-coverage general-domain even…</option><option value="425">425: [manual_acl] sp=1 qual=1 gr=1 — What is the first paper to address the problem of …</option><option value="427">427: [manual_acl] sp=1 qual=2 gr=1 — What limitations do large language models have in …</option><option value="428">428: [manual_acl] sp=1 qual=2 gr=1 — What paper compares humans' and language models' n…</option><option value="429">429: [manual_acl] sp=1 qual=2 gr=1 — What work attempts to explore multi-hop reasoning …</option><option value="430">430: [manual_acl] sp=1 qual=1 gr=1 — Which article first proposed shuffled-group-whiten…</option><option value="431">431: [manual_acl] sp=1 qual=1 gr=1 — Which dataset supports narration generation and te…</option><option value="432">432: [manual_acl] sp=0 qual=1 gr=1 — Which family of model generally perform the best f…</option><option value="434">434: [manual_acl] sp=1 qual=2 gr=1 — Which knowledge graph completion method focuses on…</option><option value="435">435: [manual_acl] sp=1 qual=1 gr=1 — Which language model distillation paper that first…</option><option value="436">436: [manual_acl] sp=1 qual=2 gr=1 — Which numerical reasoning paper first published a …</option><option value="437">437: [manual_acl] sp=1 qual=2 gr=1 — Which paper about parameter-efficient finetuning f…</option><option value="438">438: [manual_acl] sp=1 qual=1 gr=1 — Which paper combines the advantages of different f…</option><option value="439">439: [manual_acl] sp=0 qual=1 gr=1 — Which paper did a comprehensive survey of the code…</option><option value="440">440: [manual_acl] sp=1 qual=2 gr=1 — Which paper employs a two-stage approach in genera…</option><option value="441">441: [manual_acl] sp=1 qual=1 gr=1 — Which paper enables interactive semantic parsing b…</option><option value="442">442: [manual_acl] sp=1 qual=1 gr=1 — Which paper explored training a GPT-2 for automati…</option><option value="443">443: [manual_acl] sp=1 qual=1 gr=1 — Which paper first aggregates statements to represe…</option><option value="444">444: [manual_acl] sp=1 qual=2 gr=1 — Which paper first applied the chain-of-thought tec…</option><option value="445">445: [manual_acl] sp=1 qual=2 gr=1 — Which paper first apply mixture of experts idea to…</option><option value="446">446: [manual_acl] sp=1 qual=2 gr=1 — Which paper first attempts to take potential depen…</option><option value="447">447: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines different methods for u…</option><option value="448">448: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines rewriting and expansion…</option><option value="449">449: [manual_acl] sp=1 qual=1 gr=1 — Which paper first conducted the positioned error t…</option><option value="450">450: [manual_acl] sp=1 qual=1 gr=1 — Which paper first construct large-scale corpus to …</option><option value="452">452: [manual_acl] sp=1 qual=1 gr=1 — Which paper first explored In-context learning in …</option><option value="453">453: [manual_acl] sp=1 qual=1 gr=1 — Which paper first found that multilingual models c…</option><option value="454">454: [manual_acl] sp=1 qual=2 gr=1 — Which paper first introduced document content as a…</option><option value="455">455: [manual_acl] sp=1 qual=1 gr=1 — Which paper first propose to mask positions to pre…</option><option value="456">456: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed a cross-domain language…</option><option value="457">457: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed extracting the pair of …</option><option value="459">459: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed to combine pretrained m…</option><option value="460">460: [manual_acl] sp=0 qual=1 gr=1 — Which paper first proposed to only update some ori…</option><option value="461">461: [manual_acl] sp=1 qual=1 gr=1 — Which paper first published a real-world Chinese-E…</option><option value="462">462: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that it is possible to mai…</option><option value="463">463: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that large language models…</option><option value="464">464: [manual_acl] sp=1 qual=2 gr=1 — Which paper first studied the efficiency robustnes…</option><option value="465">465: [manual_acl] sp=1 qual=2 gr=1 — Which paper first use the attention weights to gui…</option><option value="466">466: [manual_acl] sp=0 qual=1 gr=1 — Which paper first used structural information for …</option><option value="468">468: [manual_acl] sp=0 qual=1 gr=1 — Which paper highlights the need for leveraging all…</option><option value="469">469: [manual_acl] sp=1 qual=2 gr=1 — Which paper introduce a DRO (distribution robust o…</option><option value="470">470: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduced the human-evaluated timelin…</option><option value="471">471: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduces the R-GCN technique into do…</option><option value="472">472: [manual_acl] sp=1 qual=1 gr=1 — Which paper investigates the influence of the dive…</option><option value="474">474: [manual_acl] sp=0 qual=1 gr=1 — Which paper is the first to comprehensively review…</option><option value="476">476: [manual_acl] sp=1 qual=2 gr=1 — Which paper measured how well the source-translati…</option><option value="477">477: [manual_acl] sp=1 qual=2 gr=1 — Which paper presents an easy to implement and high…</option><option value="478">478: [manual_acl] sp=1 qual=2 gr=1 — Which paper produces a dataset for text simplifica…</option><option value="479">479: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed a learning-based data augment…</option><option value="480">480: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed decomposing the logit update …</option><option value="481">481: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed dictionary-based Bayesian inf…</option><option value="482">482: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed the integration of human tran…</option><option value="483">483: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes a memory-efficient optimizer …</option><option value="484">484: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes the two-stage training method…</option><option value="485">485: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes to use rewriting based approa…</option><option value="486">486: [manual_acl] sp=1 qual=2 gr=1 — Which paper showed that social relationships were …</option><option value="487">487: [manual_acl] sp=1 qual=2 gr=1 — Which paper shows assessment of training instabili…</option><option value="488">488: [manual_acl] sp=1 qual=1 gr=1 — Which paper shows that in instruction tuning, the …</option><option value="489">489: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies how current retrieval systems …</option><option value="490">490: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies the concept of enhancing the c…</option><option value="491">491: [manual_acl] sp=1 qual=2 gr=1 — Which paper surveyed the datasets and tasks of ask…</option><option value="492">492: [manual_acl] sp=1 qual=2 gr=1 — Which paper used both automatically generated and …</option><option value="493">493: [manual_acl] sp=1 qual=2 gr=1 — Which paper utilizes language models to generate s…</option><option value="494">494: [manual_acl] sp=1 qual=1 gr=1 — Which paper was the first to propose combining hum…</option><option value="495">495: [manual_acl] sp=0 qual=2 gr=1 — Which papers develop methods to make in-context le…</option><option value="496">496: [manual_acl] sp=0 qual=1 gr=1 — Which papers were among the first to explore the t…</option><option value="497">497: [manual_acl] sp=1 qual=1 gr=1 — Which pre-trained model is specifically designed f…</option><option value="499">499: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model can demonstrate that v…</option><option value="500">500: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model paper in 2023 develope…</option><option value="501">501: [manual_acl] sp=1 qual=2 gr=1 — Which was the first paper to explore the online ad…</option><option value="503">503: [manual_acl] sp=1 qual=1 gr=1 — Which work proposes an approach to improve candida…</option><option value="504">504: [manual_acl] sp=1 qual=2 gr=1 — what's the first paper that manages to handle KBQA…</option><option value="505">505: [manual_acl] sp=1 qual=1 gr=1 — which paper first focuses on addressing the over-s…</option><option value="506">506: [manual_iclr] sp=1 qual=1 gr=1 — Can we reduce visual tokens in vision transformers…</option><option value="507">507: [manual_iclr] sp=1 qual=1 gr=1 — Can we learn to represent an image with arbitary n…</option><option value="508">508: [manual_iclr] sp=1 qual=2 gr=1 — Are there any papers that construct convolutional …</option><option value="509">509: [manual_iclr] sp=1 qual=1 gr=1 — Are there any papers that study whether you can id…</option><option value="511">511: [manual_iclr] sp=1 qual=1 gr=1 — Are there datasets and benchmarks available for me…</option><option value="512">512: [manual_iclr] sp=1 qual=2 gr=1 — Are there sequential learning guarantees for confi…</option><option value="513">513: [manual_iclr] sp=1 qual=2 gr=1 — Can we find the solution of the Bilevel optimizati…</option><option value="514">514: [manual_iclr] sp=0 qual=2 gr=1 — Can you find a dataset that shows LLM-based evalua…</option><option value="515">515: [manual_iclr] sp=1 qual=2 gr=1 — Can you find a research paper that discusses using…</option><option value="516">516: [manual_iclr] sp=1 qual=1 gr=1 — I'm using Local SGD with a decaying learning rate …</option><option value="517">517: [manual_iclr] sp=1 qual=1 gr=1 — In video diffusion models, is there any paper that…</option><option value="518">518: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper illustrating that pre-trained tra…</option><option value="519">519: [manual_iclr] sp=1 qual=2 gr=1 — Is there a paper that takes a mixed machine learni…</option><option value="520">520: [manual_iclr] sp=1 qual=1 gr=1 — Is there a paper which applies Bayesian optimizati…</option><option value="521">521: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper which proposes a general data sel…</option><option value="523">523: [manual_iclr] sp=0 qual=1 gr=1 — Is there a single GNN model that can inductively g…</option><option value="524">524: [manual_iclr] sp=1 qual=2 gr=1 — Is there a theory paper that explains why sometime…</option><option value="525">525: [manual_iclr] sp=1 qual=1 gr=1 — Is there an existing dataset of images with alt-te…</option><option value="526">526: [manual_iclr] sp=1 qual=1 gr=1 — Is there any generalizable NeRF paper that disenta…</option><option value="527">527: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper applies off-shelf GPT-2 model i…</option><option value="528">528: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper improves adversarial training b…</option><option value="529">529: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that explores ways to parameter…</option><option value="530">530: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that previously proposed to con…</option><option value="531">531: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that seamlessly integrates the …</option><option value="532">532: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that theoretically explains why…</option><option value="533">533: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that uses Lipschitz continuity …</option><option value="534">534: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper trying to improve MLE for auto-…</option><option value="535">535: [manual_iclr] sp=1 qual=1 gr=1 — Name a paper which proposes a probabilsitic formul…</option><option value="536">536: [manual_iclr] sp=1 qual=2 gr=1 — What are some evaluation benchmarks for LLM privac…</option><option value="537">537: [manual_iclr] sp=1 qual=1 gr=1 — What are the key advantages of coupling neural SDE…</option><option value="538">538: [manual_iclr] sp=1 qual=2 gr=1 — What is a paper studying data being collected in b…</option><option value="539">539: [manual_iclr] sp=1 qual=1 gr=1 — What is the first paper that theoretically studies…</option><option value="540">540: [manual_iclr] sp=1 qual=2 gr=1 — What is the first paper that uses the generalized …</option><option value="541">541: [manual_iclr] sp=1 qual=1 gr=1 — What molecular representation learning paper intro…</option><option value="542">542: [manual_iclr] sp=1 qual=2 gr=1 — What open-source dataset combined knowledge retrie…</option><option value="543">543: [manual_iclr] sp=0 qual=1 gr=1 — What paper considers sensitive data issue when pro…</option><option value="544">544: [manual_iclr] sp=0 qual=1 gr=1 — What paper evaluated the ability of visual few-sho…</option><option value="545">545: [manual_iclr] sp=1 qual=1 gr=1 — What paper first adapted ControlNet to generate co…</option><option value="546">546: [manual_iclr] sp=1 qual=1 gr=1 — What paper first associate the modeling frequency …</option><option value="547">547: [manual_iclr] sp=1 qual=2 gr=1 — What paper first extends rotary positional encodin…</option><option value="548">548: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposed a robust perceptual simi…</option><option value="549">549: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposes that simply reversing th…</option><option value="551">551: [manual_iclr] sp=1 qual=1 gr=1 — What paper first used the technique of prompt engi…</option><option value="552">552: [manual_iclr] sp=1 qual=1 gr=1 — What paper first uses decoupled workers in distrib…</option><option value="553">553: [manual_iclr] sp=1 qual=2 gr=1 — What paper investigated the effect of the relative…</option><option value="554">554: [manual_iclr] sp=1 qual=1 gr=1 — What paper is the first to prove finetuned LLM can…</option><option value="555">555: [manual_iclr] sp=1 qual=1 gr=1 — What paper mitigates language model sampling error…</option><option value="556">556: [manual_iclr] sp=1 qual=2 gr=1 — What paper mitigates the vocabulary size limitatio…</option><option value="558">558: [manual_iclr] sp=1 qual=1 gr=1 — What paper showed first that one can build a fully…</option><option value="559">559: [manual_iclr] sp=0 qual=1 gr=1 — What paper shows that RLAIF can fully replace RLHF…</option><option value="561">561: [manual_iclr] sp=1 qual=1 gr=1 — What work first uses LLM to code robotic simulatio…</option><option value="562">562: [manual_iclr] sp=1 qual=2 gr=1 — What work proposes a model to learn a latent regul…</option><option value="563">563: [manual_iclr] sp=1 qual=1 gr=1 — What work proposes to combine video foundation mod…</option><option value="565">565: [manual_iclr] sp=1 qual=1 gr=1 — Which foundation model paper first proposed a time…</option><option value="567">567: [manual_iclr] sp=1 qual=1 gr=1 — Which machine learning paper proposed certified ro…</option><option value="568">568: [manual_iclr] sp=0 qual=1 gr=1 — Which multimodal large language model represents v…</option><option value="569">569: [manual_iclr] sp=1 qual=1 gr=1 — Which neural theorem proving paper first attempted…</option><option value="570">570: [manual_iclr] sp=1 qual=2 gr=1 — Which paper considers both weights and activations…</option><option value="571">571: [manual_iclr] sp=1 qual=1 gr=1 — Which paper contains quantitative results demonstr…</option><option value="572">572: [manual_iclr] sp=1 qual=1 gr=1 — Which paper examined the scalability of instructio…</option><option value="573">573: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first applied the chain of thought con…</option><option value="574">574: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first derived online occupany estimati…</option><option value="575">575: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first found that REINFORCE works bette…</option><option value="576">576: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first found that when transformers are…</option><option value="577">577: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first investigates the knowledge prefe…</option><option value="578">578: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first proposes a unified framework for…</option><option value="579">579: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first proved that wide-enough transfor…</option><option value="580">580: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first showed that task-specific knowle…</option><option value="581">581: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first studied differential privacy for…</option><option value="582">582: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first study POMDP with enhanced feedba…</option><option value="583">583: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first tried to fine-tune LLMs with cha…</option><option value="584">584: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first used language models to emulate …</option><option value="585">585: [manual_iclr] sp=1 qual=1 gr=1 — Which paper formally defines the problem of model …</option><option value="586">586: [manual_iclr] sp=1 qual=2 gr=1 — Which paper found that using common character enco…</option><option value="587">587: [manual_iclr] sp=1 qual=1 gr=1 — Which paper in human motion generation can control…</option><option value="588">588: [manual_iclr] sp=1 qual=2 gr=1 — Which paper is the first to model the helpfulness …</option><option value="589">589: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes an alignment framework that s…</option><option value="590">590: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes to integrate black-box LLMs w…</option><option value="591">591: [manual_iclr] sp=1 qual=1 gr=1 — Which paper studies how difficult is a policy lear…</option><option value="592">592: [manual_iclr] sp=1 qual=2 gr=1 — Which paper trains on linear regression to hypothe…</option><option value="593">593: [manual_iclr] sp=1 qual=1 gr=1 — Which paper uses the latent diffusion model for th…</option><option value="594">594: [manual_iclr] sp=1 qual=2 gr=1 — Which paper utilized MMD flows with Riesz kernels …</option><option value="596">596: [manual_iclr] sp=1 qual=2 gr=1 — Which paper systematically examed the input mismat…</option></select>
    <p id="queryText" class="small has-abs" data-abs="">Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?</p>
    <p id="queryMeta" class="small">set:manual_acl | spec:1 | qual:2 | gr:0</p>

    <label>Relevant document (rank&nbsp;+&nbsp;title)</label>
    <select id="docSel"><option value="0" data-id="258546861" title="The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-ofthe-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.">0: rank=∞  Chain-of-Skills: A Configurable Model for Open-Domain Question Answering</option></select>
    <p id="docTitle" class="small has-abs" data-abs="The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-ofthe-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.">0: rank=∞  Chain-of-Skills: A Configurable Model for Open-Domain Question Answering</p>
  </div>

  <!-- PROMPT / LLM / ANNOTATION CARD -->
  <div class="card" style="flex:2 1 520px;">
    <h2>Prompt &amp; LLM setup</h2>

    <label>Prompt</label>
    <div style="display:flex;gap:.5rem;">
      <select id="promptSel" style="flex:1;"><option value="dummy">dummy</option><option value="full_text">full_text</option><option value="title_abstract">title_abstract</option></select>
      <button id="newPromptBtn">+&nbsp;New</button>
    </div>

    <label>Extractor</label>
    <select id="extSel"><option value="dummy">dummy</option><option value="json_list_extractor">json_list_extractor</option></select>

    <label>k (top‑k retrieval)</label>
    <input id="kInp" type="number" value="50" min="1">

    <label>Prompt text</label>
    <textarea id="promptBox"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="savePromptBtn">💾&nbsp;Save</button>
      <button id="reloadPromptBtn">⟳&nbsp;Reload</button>
    </div>

    <details style="margin-top:.75rem;">
      <summary><strong>LLM config</strong></summary>
      <label>API key</label><input id="apiKey" type="text" placeholder="sk‑…">
      <label>Model</label><input id="model" type="text" value="gpt-4o-mini">
      <label>Temperature</label><input id="temp" type="number" value="0" step=".1">
      <label>Max tokens</label><input id="maxTok" type="number" value="2048">
      <label style="display:flex;align-items:center;gap:.5rem;margin-top:.5rem;">
        <input id="wantJson" type="checkbox"> Expect JSON object response
      </label>
    </details>

    <label>Your annotation</label>
    <textarea id="noteBox" placeholder="Add notes about this run…"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="saveNoteBtn" style="background:#0b63ff;color:#fff;">Save&nbsp;annotation</button>
      <button id="runBtn" style="background:#14a44d;color:#fff;">Run</button>
    </div>
  </div>
</section>

<!-- BEFORE / AFTER TABLES ---------------------------------------------->
<section class="flex">
  <div class="card">
    <h2>Before&nbsp;(original)</h2>
    <table id="beforeTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody><tr class="has-abs" data-abs="Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study &quot;Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance?&quot; Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves 13× memory while maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individual sparse and dense retrievers. Nevertheless, our analysis shows that there is a large room to improve the robustness of retrievers, suggesting a new research direction."><td>1</td><td>A Study on the Efficiency and Generalization of Light Hybrid Retrieversd</td><td>0.526</td></tr><tr class="has-abs" data-abs="Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ∼ 50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget. 1 . wav2vec 2.0: A framework for self-supervised learning of speech representations. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. , et al. Learning transferable visual models from natural language supervision. . Scaling vision with sparse mixture of experts."><td>2</td><td>Published as a conference paper at ICLR 2023 SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS</td><td>0.530</td></tr><tr class="has-abs" data-abs="Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40× more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better. * Equal contribution. The order was decided by a coin toss. 1 arXiv:2308.00951v1 [cs.LG] 2 Aug 2023 1 def soft_m oe_lay er (X , Phi , experts ) : 2 # Compute the dispatch and combine weights .3 logits = jnp . einsum ( 'md , dnp -&gt; mnp ' , X , Phi ) 4 D = jax . nn . softmax ( logits , axis =(0 ,) ) 5 C = jax . nn . softmax ( logits , axis =(1 , 2) ) 6 # The input slots are a weighted average of all the input tokens , 7 # given by the dispatch weights .8 Xs = jnp . einsum ( 'md , mnp -&gt; npd ' , X , D ) 9 # Apply the corresponding expert function to each input slot .10 Ys = jnp . stack ([ 11 f_i ( Xs [i , : , :]) for i , f_i in enumerate ( experts ) ] , 12 axis =0) 13 # The output tokens are a weighted average of all the output slots , 14 # given by the combine weights . 15 Y = jnp . einsum ( 'npd , mnp -&gt; md ' , Ys , C ) 16 return Y Algorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at https://github.com/google-research/vmoe."><td>3</td><td>From Sparse to Soft Mixtures of Experts</td><td>0.533</td></tr><tr class="has-abs" data-abs="While dense retrieval has been shown to be effective and efficient across tasks and lan-"><td>4</td><td>Precise Zero-Shot Dense Retrieval without Relevance Labels</td><td>0.534</td></tr><tr class="has-abs" data-abs="Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dualencoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks. 1"><td>5</td><td>Dense Passage Retrieval for Open-Domain Question Answering</td><td>0.539</td></tr><tr class="has-abs" data-abs="Dense retrieval is a basic building block of information retrieval applications. One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words. A popular approach for handling misspelled queries is minimizing the representations discrepancy between misspelled queries and their pristine ones. Unlike the existing approaches, which only focus on the alignment between misspelled and pristine queries, our method also improves the contrast between each misspelled query and its surrounding queries. To assess the effectiveness of our proposed method, we compare it against the existing competitors using two benchmark datasets and two base encoders. Our method outperforms the competitors in all cases with misspelled queries. Our code and models are available at https://github. com/panuthept/DST-DenseRetrieval."><td>6</td><td>Typo-Robust Representation Learning for Dense Retrieval</td><td>0.541</td></tr><tr class="has-abs" data-abs="Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https: //github.com/microsoft/taser."><td>7</td><td>Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering</td><td>0.541</td></tr><tr class="has-abs" data-abs="We introduce an LSTM-based method for dynamically integrating several wordprediction experts to obtain a conditional language model which can be good simultaneously at several subtasks. We illustrate this general approach with an application to dialogue where we integrate a neural chat model, good at conversational aspects, with a neural question-answering model, good at retrieving precise information from a knowledge-base, and show how the integration combines the strengths of the independent components. We hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in NLP. 1"><td>8</td><td>LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues</td><td>0.541</td></tr><tr class="has-abs" data-abs="Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Promptbase Query Generation for Retriever (PROMPTAGATOR ), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, PROMPTAGATOR makes it possible to create task-specific end-to-end retrievers solely based on a few examples without using Natural Questions(Kwiatkowski et al., 2019)or MS MARCO (Nguyen et al., 2016)  to train dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2(Santhanam et al., 2022)by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given."><td>9</td><td>PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES</td><td>0.545</td></tr><tr class="has-abs" data-abs="Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark."><td>10</td><td>What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary</td><td>0.545</td></tr><tr class="has-abs" data-abs="We describe our two-stage system for the Multilingual Information Access (MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The first stage consists of multilingual passage retrieval with a hybrid dense and sparse retrieval strategy. The second stage consists of a reader which outputs the answer from the top passages returned by the first stage. We show the efficacy of using entity representations, sparse retrieval signals to help dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of 31.61. We improve over the official baseline by over 4 F1 points on both the development and test sets. 1"><td>11</td><td>MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering</td><td>0.551</td></tr><tr class="has-abs" data-abs="Recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models. To obtain high efficiency, the basic structure of these models is Bi-encoder in most cases. However, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. To address this problem, we design a method to mimic the queries on each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e., the cluster centroids). To boost the retrieval process using approximate nearest neighbor search library, we also optimize the matching function with a two-step score calculation procedure. Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results."><td>12</td><td>Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval</td><td>0.553</td></tr><tr class="has-abs" data-abs="In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple GPUs to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature."><td>13</td><td>Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering</td><td>0.556</td></tr><tr class="has-abs" data-abs="The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -less than 1% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints. Our code used in all the experiments is publicly available here: https://github.com/for-ai/parameter-efficient-moe."><td>14</td><td>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</td><td>0.556</td></tr><tr class="has-abs" data-abs="The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost. * Equally major contributors † Work done as a member of the Google Brain Residency program (g.co/brainresidency) Under review as a conference paper at ICLR 2017 Figure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In this case, the sparse gating function selects two experts to perform computations. Their outputs are modulated by the outputs of the gating network.While these ideas are promising in theory, no work to date has yet demonstrated massive improvements in model capacity, training time, or model quality. We blame this on a combination of the following challenges:• Modern computing devices, especially GPUs, are much faster at arithmetic than at branching. Most of the works above recognize this and propose turning on/off large chunks of the network with each gating decision.• Large batch sizes are critical for performance, as they amortize the costs of parameter transfers and updates. Conditional computation reduces the batch sizes for the conditionally active chunks of the network.• Network bandwidth can be a bottleneck. A cluster of GPUs may have computational power thousands of times greater than the aggregate inter-device network bandwidth. To be computationally efficient, the relative computational versus network demands of an algorithm must exceed this ratio. Embedding layers, which can be seen as a form of conditional computation, are handicapped by this very problem. Since the embeddings generally need to be sent across the network, the number of (example, parameter) interactions is limited by network bandwidth instead of computational capacity.• Depending on the scheme, loss terms may be necessary to achieve the desired level of sparsity per-chunk and/or per example. Bengio et al.(2015)use three such terms. These issues can affect both model quality and load-balancing. diction with lstm. Neural Computation, 2000. Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/ abs/1606.03401. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition."><td>15</td><td>OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</td><td>0.557</td></tr><tr class="has-abs" data-abs="Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not ''structurally ready'' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This ''lack of readiness'' results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pretrained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg . By concatenating vectors from the [CLS] token and agg , our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at https:// github.com/castorini/dhr.436"><td>16</td><td>Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval</td><td>0.558</td></tr><tr class="has-abs" data-abs="Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models 1 ."><td>17</td><td>Mixture of Attention Heads: Selecting Attention Heads Per Token</td><td>0.560</td></tr><tr class="has-abs" data-abs="Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a &quot;self-slimmable&quot; property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments across diverse transformer architectures on a variety of tasks demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively. Codes and models are available in https://github.com/VITA-Group/Random-MoE-as-Dropout."><td>18</td><td>SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS</td><td>0.560</td></tr><tr class="has-abs" data-abs="Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and achieve state-of-the-art performance on various retrieval tasks. These methods, however, are orders of magnitude slower and need more space to store their indexes compared to their single-vector counterparts. In this paper, we unify different multi-vector retrieval models from a token routing viewpoint and propose conditional token interaction via dynamic lexical routing, namely CITADEL, for efficient and effective multi-vector retrieval. CITADEL learns to route each token vector to the predicted lexical &quot;keys&quot; such that a query token vector only interacts with document token vectors routed to the same key. This design significantly reduces the computation cost while maintaining high accuracy. Notably, CITADEL achieves the same or slightly better performance than the previous state of the art, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR) evaluations, while being nearly 40 times faster. Source code and data are available at https: //github.com/facebookresearch/ dpr-scale/tree/citadel."><td>19</td><td>CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval</td><td>0.563</td></tr><tr class="has-abs" data-abs="Mixtures of Experts combine the outputs of several &quot;expert&quot; networks, each of which specializes in a different part of the input space. This is achieved by training a &quot;gating&quot; network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (&quot;where&quot;) experts at the first layer, and class-specific (&quot;what&quot;) experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations. * Marc'Aurelio Ranzato currently works at the Facebook AI Group."><td>20</td><td>Learning Factored Representations in a Deep Mixture of Experts</td><td>0.564</td></tr></tbody></table>
  </div>
  <div class="card">
    <h2>After&nbsp;(augmented)</h2>
    <table id="afterTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody></tbody></table>
  </div>
</section>

<!-- METRICS ------------------------------------------------------------->
<section id="metricsCard" class="card" style="">
  <h2>Metrics</h2>
  <div id="recallLine" class="small">Recall </div>

  <div class="tabs">
    <button id="tab1" class="">Δ recall</button>
    <button id="tab2" class="active">Rank maps</button>
  </div>

  <div id="body1" class="tab-body active">
    <p id="deltaLine" class="small"></p>
  </div>
  <div id="body2" class="tab-body active">
    <h3 class="small" style="margin-top:0;">ranks_before</h3>
    <pre id="rb" class="small">{
  "258546861": 9999999999
}</pre>
    <h3 class="small">ranks_after</h3>
    <pre id="ra" class="small"></pre>
  </div>
</section>




```
</body></html>