<!doctype html>
<html lang="en"><head></head><body>```html



<meta charset="utf-8">
<title>Prompt‑HyDE playground</title>
<style>
  body{font-family:system-ui,sans-serif;margin:0;padding:1rem 2rem;}
  h2{margin-top:2rem;}
  label{display:block;margin-top:.75rem;font-weight:600;}
  select,input[type=text],input[type=number],textarea{width:100%;padding:.4rem;}
  textarea{height:120px;font-family:monospace;}
  button{margin-top:1rem;padding:.5rem 1rem;cursor:pointer;}
  table{border-collapse:collapse;width:100%;margin-top:.5rem;}
  th,td{border:1px solid #ccc;padding:.25rem .5rem;text-align:left;vertical-align:top;}
  em.rev{font-style:italic;color:#9146ff;}
  strong.rel{color:#0b63ff;font-weight:700;}
  .flex{display:flex;gap:2rem;flex-wrap:wrap;}
  .card{flex:1 1 320px;border:1px solid #ddd;padding:1rem;}
  .small{font-size:.85rem;color:#555;margin:.25rem 0;}

  /* tooltip for abstracts */
  .has-abs{position:relative;}
  .has-abs:hover::after{
    content:attr(data-abs);
    position:absolute;left:0;top:100%;z-index:999;
    max-width:420px;white-space:pre-wrap;font-size:.8rem;
    background:#333;color:#fff;padding:.5rem;border-radius:.25rem;
    box-shadow:0 2px 6px rgba(0,0,0,.35);
  }

  /* tab styling */
  .tabs{display:flex;gap:.5rem;margin-bottom:.5rem;}
  .tabs button{padding:.25rem .75rem;border:1px solid #aaa;background:#eee;}
  .tabs button.active{background:#fff;border-bottom:none;font-weight:600;}
  .tab-body{display:none;}
  .tab-body.active{display:block;}
</style>


<h1>Prompt‑HyDE UI (vanilla&nbsp;JS)</h1>

<!-- CONFIG -------------------------------------------------------------->
<section class="card">
  <h2>Server configuration</h2>
  <pre id="cfg" class="small">{
  "embed_model": "grit",
  "index_path": "../0721_litsearch_example/faiss/litsearch.index",
  "backend": "faiss",
  "query_dataset": "../0721_litsearch_example/query_with_score.parquet",
  "corpus_dataset": "../0721_litsearch_example/corpus_clean_dedup.parquet",
  "prompt_dir": "prompts",
  "extractor_dir": "extractors",
  "annotation_dir": "annotations",
  "ui_config": "configs/ui_config.json",
  "host": "0.0.0.0",
  "port": 8200,
  "id_field": "corpusid",
  "relevant_documents_field": "corpusids"
}</pre>
</section>

<section class="flex">
  <!-- DATA CARD -->
  <div class="card" style="max-width:440px;">
    <h2>Data</h2>

    <label>Query</label>
    <select id="querySel"><option value="0">0: [inline_acl] sp=0 qual=2 gr=0 — Are there any research papers on methods to compre…</option><option value="3">3: [inline_acl] sp=1 qual=2 gr=0 — Are there any tools or studies that have focused o…</option><option value="4">4: [inline_acl] sp=1 qual=2 gr=0 — Are there papers that propose contextualized calib…</option><option value="10">10: [inline_acl] sp=1 qual=2 gr=0 — Can you point me to a work that uses diagnostic to…</option><option value="11">11: [inline_acl] sp=0 qual=1 gr=0 — Can you point me to studies discussing methods for…</option><option value="17">17: [inline_acl] sp=1 qual=2 gr=0 — Can you recommend a paper that uses an NLI model f…</option><option value="24">24: [inline_acl] sp=0 qual=1 gr=0 — Can you suggest recent studies that have integrate…</option><option value="30">30: [inline_acl] sp=0 qual=1 gr=0 — Could you point me to studies that have investigat…</option><option value="35">35: [inline_acl] sp=1 qual=2 gr=0 — Could you suggest a paper that introduces an appro…</option><option value="39">39: [inline_acl] sp=1 qual=2 gr=0 — I am looking for research that has explored topic …</option><option value="44">44: [inline_acl] sp=1 qual=2 gr=0 — I'm exploring research that utilizes large dataset…</option><option value="49">49: [inline_acl] sp=1 qual=2 gr=0 — I'm looking for innovative approaches to data anno…</option><option value="55">55: [inline_acl] sp=1 qual=2 gr=0 — In discourse parsing literature, which works have …</option><option value="56">56: [inline_acl] sp=1 qual=2 gr=0 — In researching metrics for human-interaction with …</option><option value="58">58: [inline_acl] sp=1 qual=2 gr=0 — In the context of Named Entity Recognition tasks a…</option><option value="59">59: [inline_acl] sp=0 qual=1 gr=0 — In the context of machine translation, can you poi…</option><option value="61">61: [inline_acl] sp=1 qual=2 gr=0 — In the context of simultaneous machine translation…</option><option value="62">62: [inline_acl] sp=1 qual=2 gr=0 — In the field of reinforcement learning models for …</option><option value="63">63: [inline_acl] sp=1 qual=2 gr=0 — What approaches have been used to address the limi…</option><option value="71">71: [inline_acl] sp=1 qual=2 gr=0 — What are the recent developments in evaluating the…</option><option value="75">75: [inline_acl] sp=0 qual=1 gr=0 — What research could I reference to understand the …</option><option value="84">84: [inline_acl] sp=1 qual=2 gr=0 — Where can I find a multilingual corpus that includ…</option><option value="109">109: [inline_nonacl] sp=1 qual=2 gr=0 — Can you recommend research that uses an LLM to gen…</option><option value="110">110: [inline_nonacl] sp=0 qual=1 gr=0 — Can you show me a paper that built a large structu…</option><option value="111">111: [inline_nonacl] sp=1 qual=2 gr=0 — Can you suggest research that deals with the multi…</option><option value="119">119: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that examines how inco…</option><option value="123">123: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that explores how lang…</option><option value="124">124: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores mitigati…</option><option value="126">126: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores the diff…</option><option value="128">128: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that initializes embed…</option><option value="132">132: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that investigates enha…</option><option value="135">135: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="136">136: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates how …</option><option value="137">137: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="142">142: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates the …</option><option value="143">143: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates the …</option><option value="145">145: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that uses feedback-dri…</option><option value="149">149: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research papers that investiga…</option><option value="155">155: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that evaluates the pe…</option><option value="157">157: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that examines how dec…</option><option value="163">163: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that explores how the…</option><option value="168">168: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates app…</option><option value="169">169: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates gen…</option><option value="176">176: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates the…</option><option value="179">179: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research that proposed enhanci…</option><option value="194">194: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest a study that explores data annot…</option><option value="195">195: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that explores employing …</option><option value="197">197: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest a study that explores the idea o…</option><option value="199">199: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that investigates the in…</option><option value="205">205: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest papers that tackle conversationa…</option><option value="208">208: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that concentrates on pi…</option><option value="209">209: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines a system …</option><option value="213">213: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that examines how stran…</option><option value="221">221: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines the effec…</option><option value="224">224: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores generatin…</option><option value="226">226: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores the idea …</option><option value="228">228: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that includes an online…</option><option value="231">231: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that investigates apply…</option><option value="238">238: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that investigates how c…</option><option value="244">244: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest research that investigates the u…</option><option value="251">251: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest studies focused on emotion-class…</option><option value="252">252: [inline_nonacl] sp=1 qual=2 gr=0 — Has any research explored using other off-the-shel…</option><option value="260">260: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research efforts been made to gather dial…</option><option value="261">261: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers been published on models …</option><option value="265">265: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers examined whether using la…</option><option value="269">269: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research papers investigated the creation…</option><option value="272">272: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers tried to create conversat…</option><option value="278">278: [inline_nonacl] sp=1 qual=2 gr=0 — I know about prompt tuning, but have any works tri…</option><option value="297">297: [inline_nonacl] sp=1 qual=2 gr=0 — What are some scholarly articles that explore the …</option><option value="306">306: [inline_nonacl] sp=0 qual=1 gr=0 — What papers discuss the effect of false negatives …</option><option value="307">307: [inline_nonacl] sp=1 qual=2 gr=0 — What papers explore replacing schema linking with …</option><option value="311">311: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists comparing adapter-based tunin…</option><option value="312">312: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists on employing generative model…</option><option value="325">325: [inline_nonacl] sp=0 qual=1 gr=0 — What research is available on acquiring sentence e…</option><option value="326">326: [inline_nonacl] sp=0 qual=2 gr=0 — What research is available on hybrid approaches th…</option><option value="331">331: [inline_nonacl] sp=0 qual=1 gr=0 — What techniques and frameworks have been suggested…</option><option value="346">346: [inline_nonacl] sp=1 qual=1 gr=0 — Which work shows that only emplying instance-level…</option><option value="347">347: [inline_nonacl] sp=1 qual=2 gr=0 — Which work shows that reducing the number of train…</option><option value="353">353: [manual_acl] sp=1 qual=2 gr=0 — Are there any papers that build dense retrievers w…</option><option value="360">360: [manual_acl] sp=1 qual=2 gr=0 — If one would like to train (or evaluate) a helpful…</option><option value="364">364: [manual_acl] sp=1 qual=2 gr=0 — Is there a decoder-only language model that does n…</option><option value="366">366: [manual_acl] sp=1 qual=2 gr=0 — Is there a method for measuring the critical error…</option><option value="369">369: [manual_acl] sp=0 qual=1 gr=0 — Is there a paper exploring the curse of multilingu…</option><option value="372">372: [manual_acl] sp=0 qual=2 gr=0 — Is there a paper that links exposure bias to disti…</option><option value="378">378: [manual_acl] sp=1 qual=2 gr=0 — Is there a paper that uses similarity scores to ch…</option><option value="382">382: [manual_acl] sp=1 qual=2 gr=0 — Is there a tool that can automatically segment spe…</option><option value="389">389: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that aligns speech and text emb…</option><option value="395">395: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that constructs augmented train…</option><option value="404">404: [manual_acl] sp=0 qual=2 gr=0 — Is there any paper that proposes a set of criteria…</option><option value="411">411: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that utilizes Gaussian processe…</option><option value="414">414: [manual_acl] sp=1 qual=1 gr=0 — Is there any research paper that can extract attri…</option><option value="417">417: [manual_acl] sp=1 qual=2 gr=0 — Is there any works that explores how to achieve ba…</option><option value="423">423: [manual_acl] sp=0 qual=2 gr=0 — What are some methods for solving the class-increm…</option><option value="426">426: [manual_acl] sp=0 qual=1 gr=0 — What is the performance of large language models i…</option><option value="433">433: [manual_acl] sp=1 qual=2 gr=0 — Which is the first multimodal model combining text…</option><option value="451">451: [manual_acl] sp=1 qual=2 gr=0 — Which paper first constructed a structured knowled…</option><option value="458">458: [manual_acl] sp=1 qual=1 gr=0 — Which paper first proposed shared adapter module a…</option><option value="467">467: [manual_acl] sp=0 qual=1 gr=0 — Which paper found that mutual learning benefits mu…</option><option value="473">473: [manual_acl] sp=1 qual=1 gr=0 — Which paper is among the earliest to train on exte…</option><option value="475">475: [manual_acl] sp=0 qual=1 gr=0 — Which paper makes sure that the questions used in …</option><option value="498">498: [manual_acl] sp=1 qual=2 gr=0 — Which research paper leverages event structure inf…</option><option value="502">502: [manual_acl] sp=1 qual=2 gr=0 — Which work discusses an analysis of source and tar…</option><option value="510">510: [manual_iclr] sp=1 qual=2 gr=0 — Are there any papers that use a world model for pl…</option><option value="522">522: [manual_iclr] sp=1 qual=2 gr=0 — Is there a parameter-efficient fine-tuning method …</option><option value="550">550: [manual_iclr] sp=1 qual=1 gr=0 — What paper first showed that you can score the cod…</option><option value="557">557: [manual_iclr] sp=0 qual=1 gr=0 — What paper proposes breaking down programming prob…</option><option value="560">560: [manual_iclr] sp=1 qual=1 gr=0 — What research first proposed a new kind of cascade…</option><option value="564">564: [manual_iclr] sp=1 qual=2 gr=0 — Which backdoor paper first used the CLIP to suppre…</option><option value="566">566: [manual_iclr] sp=1 qual=1 gr=0 — Which is one of the first papers to highlight and …</option><option value="595">595: [manual_iclr] sp=0 qual=2 gr=0 — What paper provides generalization bounds for self…</option><option value="18">18: [inline_acl] sp=0 qual=2 gr=0.25 — Can you recommend some literature that focuses on …</option><option value="2">2: [inline_acl] sp=0 qual=2 gr=0.5 — Are there any studies that explore post-hoc techni…</option><option value="12">12: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me to studies that explore the impac…</option><option value="13">13: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me towards research on contrastive l…</option><option value="53">53: [inline_acl] sp=1 qual=2 gr=0.5 — I'm researching on the efficacy of recurrent netwo…</option><option value="74">74: [inline_acl] sp=1 qual=2 gr=0.5 — What prior works suggested that exposure bias coul…</option><option value="92">92: [inline_acl] sp=0 qual=1 gr=0.5 — Where might I find research on the evaluation of c…</option><option value="102">102: [inline_nonacl] sp=1 qual=2 gr=0.5 — Are there any studies investigating example-based …</option><option value="113">113: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you direct me to studies investigating the e…</option><option value="150">150: [inline_nonacl] sp=0 qual=1 gr=0.5 — Could you recommend research that analyses prompt …</option><option value="156">156: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you recommend research that examines how an …</option><option value="235">235: [inline_nonacl] sp=1 qual=1 gr=0.5 — Could you suggest research that investigates enhan…</option><option value="257">257: [inline_nonacl] sp=0 qual=2 gr=0.5 — Have any new metrics been developed to assess the …</option><option value="328">328: [inline_nonacl] sp=0 qual=2 gr=0.5 — What research should I consult regarding the appli…</option><option value="334">334: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques exist to enhance the few-shot fine…</option><option value="335">335: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques have been investigated to enhance …</option><option value="96">96: [inline_acl] sp=0 qual=2 gr=0.6 — Which studies should I look into that have explore…</option><option value="8">8: [inline_acl] sp=0 qual=1 gr=0.6666666666666666 — Can you list some publications that discuss the ev…</option><option value="23">23: [inline_acl] sp=0 qual=2 gr=0.6666666666666666 — Can you suggest literature on enhanced semantic pa…</option><option value="1">1: [inline_acl] sp=1 qual=2 gr=1 — Are there any resources available for translating …</option><option value="5">5: [inline_acl] sp=1 qual=2 gr=1 — Are there studies that combine convolutional and r…</option><option value="6">6: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to research that explores method…</option><option value="7">7: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to studies that explore techniqu…</option><option value="9">9: [inline_acl] sp=0 qual=2 gr=1 — Can you point me to a paper that discussed transfo…</option><option value="14">14: [inline_acl] sp=0 qual=1 gr=1 — Can you point to studies or tasks focused on detec…</option><option value="15">15: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a conversational QA dataset wher…</option><option value="16">16: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a foundational paper that provid…</option><option value="19">19: [inline_acl] sp=1 qual=2 gr=1 — Can you refer me to research that adapts the conce…</option><option value="20">20: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest a corpus that contains French ency…</option><option value="21">21: [inline_acl] sp=0 qual=2 gr=1 — Can you suggest any literature that explores the i…</option><option value="22">22: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest literature on a dataset that categ…</option><option value="25">25: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some literature that evaluates the…</option><option value="26">26: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some recent datasets that have bee…</option><option value="27">27: [inline_acl] sp=1 qual=2 gr=1 — Could you direct me towards a study that explores …</option><option value="28">28: [inline_acl] sp=1 qual=2 gr=1 — Could you point me to research on binary classific…</option><option value="29">29: [inline_acl] sp=0 qual=2 gr=1 — Could you point me to studies that discuss the dev…</option><option value="31">31: [inline_acl] sp=1 qual=1 gr=1 — Could you point me toward some large-scale multili…</option><option value="32">32: [inline_acl] sp=1 qual=2 gr=1 — Could you provide me with a reference that discuss…</option><option value="33">33: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend datasets that include SQL anno…</option><option value="34">34: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend studies that provide a baselin…</option><option value="36">36: [inline_acl] sp=0 qual=1 gr=1 — Could you suggest studies that employ novel method…</option><option value="37">37: [inline_acl] sp=1 qual=2 gr=1 — Has there been any recent work or competitions foc…</option><option value="38">38: [inline_acl] sp=0 qual=1 gr=1 — I am exploring state-of-the-art techniques in lang…</option><option value="40">40: [inline_acl] sp=0 qual=2 gr=1 — I am looking to understand more about sequence-to-…</option><option value="41">41: [inline_acl] sp=1 qual=2 gr=1 — I would like to understand the theoretical basis f…</option><option value="42">42: [inline_acl] sp=1 qual=2 gr=1 — I'm conducting research on computational humor and…</option><option value="43">43: [inline_acl] sp=1 qual=1 gr=1 — I'm exploring efficient transformer architectures …</option><option value="45">45: [inline_acl] sp=0 qual=2 gr=1 — I'm exploring ways to enhance question answering s…</option><option value="46">46: [inline_acl] sp=1 qual=2 gr=1 — I'm interested in understanding how perplexity is …</option><option value="47">47: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a comprehensive dataset that has b…</option><option value="48">48: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a paper that discusses improvement…</option><option value="50">50: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into morphological embedding algorithm…</option><option value="51">51: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into the distillation process of langu…</option><option value="52">52: [inline_acl] sp=0 qual=1 gr=1 — I'm researching insertion-based decoding methods f…</option><option value="54">54: [inline_acl] sp=1 qual=1 gr=1 — I'm searching for studies that explore advancement…</option><option value="57">57: [inline_acl] sp=1 qual=2 gr=1 — In the area of argument mining, could you point to…</option><option value="60">60: [inline_acl] sp=1 qual=2 gr=1 — In the context of natural language processing, I a…</option><option value="64">64: [inline_acl] sp=1 qual=1 gr=1 — What are some approaches to generating sports news…</option><option value="65">65: [inline_acl] sp=1 qual=2 gr=1 — What are some good datasets for conversational que…</option><option value="66">66: [inline_acl] sp=1 qual=2 gr=1 — What are some of the key papers to look at for und…</option><option value="67">67: [inline_acl] sp=0 qual=2 gr=1 — What are some recent advancements in training syst…</option><option value="68">68: [inline_acl] sp=1 qual=2 gr=1 — What are some soft-constrained methods proposed in…</option><option value="69">69: [inline_acl] sp=1 qual=2 gr=1 — What are some studies that leverage statistical ma…</option><option value="70">70: [inline_acl] sp=0 qual=2 gr=1 — What are some techniques or tools used in machine …</option><option value="72">72: [inline_acl] sp=1 qual=2 gr=1 — What paper should I look at if I am interested in …</option><option value="73">73: [inline_acl] sp=1 qual=2 gr=1 — What papers should I refer to if I want to explore…</option><option value="76">76: [inline_acl] sp=1 qual=2 gr=1 — What research has been done on annotating user com…</option><option value="77">77: [inline_acl] sp=0 qual=1 gr=1 — What research has been done on improving named ent…</option><option value="78">78: [inline_acl] sp=1 qual=1 gr=1 — What research should I explore to understand metho…</option><option value="79">79: [inline_acl] sp=1 qual=2 gr=1 — When using pretrained transformer models for gener…</option><option value="80">80: [inline_acl] sp=0 qual=1 gr=1 — Where can I find a corpus of CCG annotations for n…</option><option value="81">81: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a detailed discussion on automati…</option><option value="82">82: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a discourse treebank tailored to …</option><option value="83">83: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a large corpus of annotated socia…</option><option value="85">85: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a paper that discusses annotating…</option><option value="86">86: [inline_acl] sp=0 qual=1 gr=1 — Where can I find guidelines on standard practices …</option><option value="87">87: [inline_acl] sp=0 qual=1 gr=1 — Where can I find information on self-attentive par…</option><option value="88">88: [inline_acl] sp=0 qual=1 gr=1 — Where can I find interdisciplinary research that i…</option><option value="89">89: [inline_acl] sp=1 qual=2 gr=1 — Where can I find multilingual datasets used for th…</option><option value="90">90: [inline_acl] sp=1 qual=2 gr=1 — Where can I find research about automatic evaluati…</option><option value="91">91: [inline_acl] sp=0 qual=1 gr=1 — Where might I find a dataset annotated specificall…</option><option value="93">93: [inline_acl] sp=1 qual=2 gr=1 — Which corpora are frequently used in research to b…</option><option value="94">94: [inline_acl] sp=1 qual=2 gr=1 — Which paper specifies the typical configurations u…</option><option value="95">95: [inline_acl] sp=0 qual=1 gr=1 — Which papers should I refer to for learning about …</option><option value="97">97: [inline_acl] sp=1 qual=2 gr=1 — Which work should I explore to understand the tech…</option><option value="98">98: [inline_nonacl] sp=1 qual=1 gr=1 — *Could you suggest a dataset with legally or ethic…</option><option value="99">99: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any papers on training video-language mo…</option><option value="100">100: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any recent papers investigating the use …</option><option value="101">101: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any research papers investigating the im…</option><option value="103">103: [inline_nonacl] sp=1 qual=2 gr=1 — Are there any studies investigating sentiment anal…</option><option value="104">104: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any studies on incorporating external co…</option><option value="105">105: [inline_nonacl] sp=1 qual=2 gr=1 — Are there studies examining how well question answ…</option><option value="106">106: [inline_nonacl] sp=1 qual=1 gr=1 — Are there studies that investigate debiasing langu…</option><option value="107">107: [inline_nonacl] sp=0 qual=1 gr=1 — Can you give me a paper that does self-supervised …</option><option value="108">108: [inline_nonacl] sp=0 qual=1 gr=1 — Can you recommend a dialogue summarization dataset…</option><option value="112">112: [inline_nonacl] sp=1 qual=2 gr=1 — Could you direct me to research that evaluates few…</option><option value="114">114: [inline_nonacl] sp=1 qual=1 gr=1 — Could you point me to research that tackles the is…</option><option value="115">115: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a contemporary research paper …</option><option value="116">116: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a paper that builds a writing …</option><option value="117">117: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that does data-augment…</option><option value="118">118: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that examines how cros…</option><option value="120">120: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that examines the intr…</option><option value="121">121: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores a pre-tr…</option><option value="122">122: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that explores employin…</option><option value="125">125: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that explores strategi…</option><option value="127">127: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores the impr…</option><option value="129">129: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates empl…</option><option value="130">130: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates empl…</option><option value="131">131: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates enha…</option><option value="133">133: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates grap…</option><option value="134">134: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates guid…</option><option value="138">138: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates inco…</option><option value="139">139: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates know…</option><option value="140">140: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates repr…</option><option value="141">141: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates text…</option><option value="144">144: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates the …</option><option value="146">146: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend articles that explore the role…</option><option value="147">147: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research articles that explore…</option><option value="148">148: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research papers that explore a…</option><option value="151">151: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that assesses how wel…</option><option value="152">152: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that assesses how wel…</option><option value="153">153: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that assesses techniq…</option><option value="154">154: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that employs a relaxe…</option><option value="158">158: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how mul…</option><option value="159">159: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how opt…</option><option value="160">160: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that examines how syn…</option><option value="161">161: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the cha…</option><option value="162">162: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the eff…</option><option value="164">164: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that explores identif…</option><option value="165">165: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that has introduced a…</option><option value="166">166: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that improves knowled…</option><option value="167">167: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend research that introduces a met…</option><option value="170">170: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="171">171: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="172">172: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates mer…</option><option value="173">173: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates met…</option><option value="174">174: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates tec…</option><option value="175">175: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="177">177: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="178">178: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates usi…</option><option value="180">180: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend scholarly articles that invest…</option><option value="181">181: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend studies on hierarchical modeli…</option><option value="182">182: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that concentrate on an…</option><option value="183">183: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that investigate fine-…</option><option value="184">184: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend studies that tackle the issue …</option><option value="185">185: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies which explore how to o…</option><option value="186">186: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a dataset containing diverse, in…</option><option value="187">187: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a dataset for question-answering…</option><option value="188">188: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a research article that explores…</option><option value="189">189: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study examining how transforme…</option><option value="190">190: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that evaluates cross-enc…</option><option value="191">191: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that examines how well c…</option><option value="192">192: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a cohesive…</option><option value="193">193: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a compress…</option><option value="196">196: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores improved t…</option><option value="198">198: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores the use of…</option><option value="200">200: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a study that proposes high-param…</option><option value="201">201: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a thorough comparative analysis …</option><option value="202">202: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a triplet-formatted structured d…</option><option value="203">203: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest an article that leverages the sp…</option><option value="204">204: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest datasets that can benchmark LLM …</option><option value="206">206: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research on detecting common err…</option><option value="207">207: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that assesses if langua…</option><option value="210">210: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how coref…</option><option value="211">211: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="212">212: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="214">214: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that examines how the o…</option><option value="215">215: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="216">216: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines how well …</option><option value="217">217: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="218">218: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines the appli…</option><option value="219">219: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines the chall…</option><option value="220">220: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines the diffi…</option><option value="222">222: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores a pre-tra…</option><option value="223">223: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores employing…</option><option value="225">225: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores the drawb…</option><option value="227">227: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores the impro…</option><option value="229">229: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates a clu…</option><option value="230">230: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates acros…</option><option value="232">232: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates aspec…</option><option value="233">233: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates effic…</option><option value="234">234: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates emplo…</option><option value="236">236: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates expan…</option><option value="237">237: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates graph…</option><option value="239">239: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how h…</option><option value="240">240: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how m…</option><option value="241">241: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that investigates how n…</option><option value="242">242: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates how u…</option><option value="243">243: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates impro…</option><option value="245">245: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates train…</option><option value="246">246: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that offers an in-depth…</option><option value="247">247: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that shows multilingual…</option><option value="248">248: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that trains language mo…</option><option value="249">249: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that tries to interpret…</option><option value="250">250: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest some work that develops multimod…</option><option value="253">253: [inline_nonacl] sp=1 qual=2 gr=1 — Has any research tried to mitigate overfitting in …</option><option value="254">254: [inline_nonacl] sp=1 qual=1 gr=1 — Has any study explored the zero-shot extraction of…</option><option value="255">255: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any research that uses multiple mod…</option><option value="256">256: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any work that improves the work on …</option><option value="258">258: [inline_nonacl] sp=1 qual=2 gr=1 — Have any papers tried to address the background-sh…</option><option value="259">259: [inline_nonacl] sp=1 qual=2 gr=1 — Have any recent publications explored the use of n…</option><option value="262">262: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers collected feedback from r…</option><option value="263">263: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers critically analyzed the p…</option><option value="264">264: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers examined the efficacy of …</option><option value="266">266: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers explored methods to impro…</option><option value="267">267: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers introduced a dedicated pr…</option><option value="268">268: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers investigated human capaci…</option><option value="270">270: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers suggested methods for sum…</option><option value="271">271: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers suggested techniques for …</option><option value="273">273: [inline_nonacl] sp=1 qual=1 gr=1 — Have any studies explored the creation of memory m…</option><option value="274">274: [inline_nonacl] sp=1 qual=2 gr=1 — Have there been any advancements in language model…</option><option value="275">275: [inline_nonacl] sp=1 qual=2 gr=1 — How can I locate a dataset containing toxic senten…</option><option value="276">276: [inline_nonacl] sp=1 qual=2 gr=1 — How can SQL-to-text be utilized to improve text-to…</option><option value="277">277: [inline_nonacl] sp=0 qual=1 gr=1 — How can dense retrieval models for open-domain que…</option><option value="279">279: [inline_nonacl] sp=1 qual=2 gr=1 — In multi-hop question answering, is there a paper …</option><option value="280">280: [inline_nonacl] sp=1 qual=2 gr=1 — Is it possible to adatp named entity recognition s…</option><option value="281">281: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a benchmark designed to assess language m…</option><option value="282">282: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a comprehensive dataset available for sum…</option><option value="283">283: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a dataset available for open-domain targe…</option><option value="284">284: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a dataset containing question-answer pair…</option><option value="285">285: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a research paper that has developed a cus…</option><option value="286">286: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a specialized question answering dataset …</option><option value="287">287: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a study that investigates if large langua…</option><option value="288">288: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any paper that tried fine-tuning mBERT to…</option><option value="289">289: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any research that investigates how to use…</option><option value="290">290: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research examining if multilingual pre-tr…</option><option value="291">291: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research on a specialized language model …</option><option value="292">292: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research that argues for transparency and…</option><option value="293">293: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research that investigates embedding mult…</option><option value="294">294: [inline_nonacl] sp=1 qual=2 gr=1 — Is there work on text classification that explores…</option><option value="295">295: [inline_nonacl] sp=1 qual=2 gr=1 — What approaches have been suggested to lower the c…</option><option value="296">296: [inline_nonacl] sp=1 qual=1 gr=1 — What are some scholarly articles that explore scal…</option><option value="298">298: [inline_nonacl] sp=0 qual=2 gr=1 — What are some studies that explore data-poisoning …</option><option value="299">299: [inline_nonacl] sp=1 qual=1 gr=1 — What are the latest advancements in predicting sui…</option><option value="300">300: [inline_nonacl] sp=0 qual=1 gr=1 — What are the latest developments in conversational…</option><option value="301">301: [inline_nonacl] sp=0 qual=1 gr=1 — What benchmarks have prior research utilized to as…</option><option value="302">302: [inline_nonacl] sp=1 qual=2 gr=1 — What concerns or key points have been highlighted …</option><option value="303">303: [inline_nonacl] sp=0 qual=2 gr=1 — What difficulties do neural conversational models …</option><option value="304">304: [inline_nonacl] sp=1 qual=2 gr=1 — What literature is available on training semantic …</option><option value="305">305: [inline_nonacl] sp=1 qual=2 gr=1 — What methods exist for tailoring news suggestions …</option><option value="308">308: [inline_nonacl] sp=1 qual=1 gr=1 — What recent developments in transformer architectu…</option><option value="309">309: [inline_nonacl] sp=1 qual=1 gr=1 — What recent research has been conducted on improvi…</option><option value="310">310: [inline_nonacl] sp=1 qual=1 gr=1 — What research articles should I consult to underst…</option><option value="313">313: [inline_nonacl] sp=0 qual=1 gr=1 — What research exists on incorporating knowledge gr…</option><option value="314">314: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on leveraging syntactic roles…</option><option value="315">315: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on the impact of scaling on p…</option><option value="316">316: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on using reinforcement learni…</option><option value="317">317: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on applying contr…</option><option value="318">318: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on creating neura…</option><option value="319">319: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on determining th…</option><option value="320">320: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on enhancing conv…</option><option value="321">321: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on incorporating …</option><option value="322">322: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on news recommend…</option><option value="323">323: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on scaling within…</option><option value="324">324: [inline_nonacl] sp=0 qual=2 gr=1 — What research has been conducted on the impact of …</option><option value="327">327: [inline_nonacl] sp=1 qual=1 gr=1 — What research is available on the concept of using…</option><option value="329">329: [inline_nonacl] sp=1 qual=2 gr=1 — What resources or toolkits are available to facili…</option><option value="330">330: [inline_nonacl] sp=1 qual=1 gr=1 — What sources offer research on maintaining factual…</option><option value="332">332: [inline_nonacl] sp=0 qual=1 gr=1 — What techniques exist for efficiently fine-tuning …</option><option value="333">333: [inline_nonacl] sp=0 qual=2 gr=1 — What techniques exist for incorporating context in…</option><option value="336">336: [inline_nonacl] sp=1 qual=1 gr=1 — Where can I find a database of good prompts to use…</option><option value="337">337: [inline_nonacl] sp=0 qual=1 gr=1 — Where can I read about the using soft embeddings t…</option><option value="338">338: [inline_nonacl] sp=1 qual=2 gr=1 — Which method involves training additional prompt t…</option><option value="339">339: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper has conducted a thorough analysis of h…</option><option value="340">340: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper introduced the task of creating extend…</option><option value="341">341: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper presents a platform that emphasizes ev…</option><option value="342">342: [inline_nonacl] sp=0 qual=1 gr=1 — Which paper shows that generated captions of model…</option><option value="343">343: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper shows that human experts and non-exper…</option><option value="344">344: [inline_nonacl] sp=0 qual=1 gr=1 — Which work introduces sparse attention modules and…</option><option value="345">345: [inline_nonacl] sp=1 qual=1 gr=1 — Which work pushes the limit of model quantization …</option><option value="348">348: [inline_nonacl] sp=1 qual=1 gr=1 — Which work suggests that machine translation model…</option><option value="349">349: [inline_nonacl] sp=1 qual=1 gr=1 — Which works shows that training large language mod…</option><option value="350">350: [inline_nonacl] sp=1 qual=2 gr=1 — ould you direct me to research that shows that the…</option><option value="351">351: [manual_acl] sp=1 qual=1 gr=1 — Are there any examples of using dense phrase retri…</option><option value="352">352: [manual_acl] sp=1 qual=1 gr=1 — Are there any large-scale and open-source text sim…</option><option value="354">354: [manual_acl] sp=1 qual=1 gr=1 — Could you recommend a dataset paper which presents…</option><option value="355">355: [manual_acl] sp=1 qual=1 gr=1 — Find the NLP paper that focuses on dialogue genera…</option><option value="356">356: [manual_acl] sp=0 qual=1 gr=1 — Give me a paper proposing to circumvent a single-t…</option><option value="357">357: [manual_acl] sp=1 qual=2 gr=1 — How to achieve zero-shot lip reading?…</option><option value="358">358: [manual_acl] sp=1 qual=2 gr=1 — How to better attract readers to news articles by …</option><option value="359">359: [manual_acl] sp=0 qual=1 gr=1 — How to faithfully and explicitly measure the helpf…</option><option value="361">361: [manual_acl] sp=1 qual=1 gr=1 — In multimodal (multilingual) abstractive summariza…</option><option value="362">362: [manual_acl] sp=1 qual=1 gr=1 — Is there a Chinese hate speech paper that construc…</option><option value="363">363: [manual_acl] sp=0 qual=1 gr=1 — Is there a dataset that allows to perform aspect-b…</option><option value="365">365: [manual_acl] sp=1 qual=1 gr=1 — Is there a dialogue dataset where a speaker's utte…</option><option value="367">367: [manual_acl] sp=1 qual=2 gr=1 — Is there a method that measures the information pr…</option><option value="368">368: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper comparing knowledge distillation …</option><option value="370">370: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that applies large language model…</option><option value="371">371: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that connects the basic elements …</option><option value="373">373: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that shows that language models' …</option><option value="374">374: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that supports the use of automate…</option><option value="375">375: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that uses Explainable AI techniqu…</option><option value="376">376: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses an app for a popular ta…</option><option value="377">377: [manual_acl] sp=0 qual=1 gr=1 — Is there a paper that uses evolutionary algorithms…</option><option value="379">379: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses the tree structure of m…</option><option value="380">380: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that utilizes the characteristics…</option><option value="381">381: [manual_acl] sp=1 qual=2 gr=1 — Is there a study that shows how to help the demons…</option><option value="383">383: [manual_acl] sp=1 qual=1 gr=1 — Is there an evaluation metric for natural language…</option><option value="384">384: [manual_acl] sp=1 qual=1 gr=1 — Is there any dataset that contains minimally-contr…</option><option value="385">385: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper about style transfer for storie…</option><option value="386">386: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper exploring real speakers and thu…</option><option value="387">387: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper leverages knowledge distillatio…</option><option value="388">388: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that address attacks on code mo…</option><option value="390">390: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that applies curriculum learnin…</option><option value="391">391: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that applies symbolic distillat…</option><option value="392">392: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that attempts to evaluate the s…</option><option value="393">393: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that automatically creates a da…</option><option value="394">394: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that combines causal inference …</option><option value="396">396: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that employs code LLMs to itera…</option><option value="397">397: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores and annotates the…</option><option value="398">398: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores using only an enc…</option><option value="399">399: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that investigates backdoor atta…</option><option value="400">400: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that leverages graph neural net…</option><option value="401">401: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that leverages syntactic rules …</option><option value="402">402: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that performs adversarial train…</option><option value="403">403: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that proposes a new multimodal …</option><option value="405">405: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that reveals annotation problem…</option><option value="406">406: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that studies a teacher AI infer…</option><option value="407">407: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that tries to investigate LLMs’…</option><option value="408">408: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that uses data collected from t…</option><option value="409">409: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses prompt tuning in mult…</option><option value="410">410: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses token-level loss to e…</option><option value="412">412: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that utilizes graph structure t…</option><option value="413">413: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that utilizes masked language m…</option><option value="415">415: [manual_acl] sp=1 qual=2 gr=1 — Is there any work that allows large numbers of mod…</option><option value="416">416: [manual_acl] sp=1 qual=1 gr=1 — Is there any work that attacks language models in …</option><option value="418">418: [manual_acl] sp=1 qual=2 gr=1 — Is there commonsense reasoning dataset which gener…</option><option value="419">419: [manual_acl] sp=1 qual=2 gr=1 — Is there such a factuality evaluation dataset that…</option><option value="420">420: [manual_acl] sp=1 qual=2 gr=1 — Is there such a reading comprehension dataset in u…</option><option value="421">421: [manual_acl] sp=1 qual=1 gr=1 — Provide an example of a paper which proposes a met…</option><option value="422">422: [manual_acl] sp=1 qual=2 gr=1 — What are some data-efficient ways to learn text em…</option><option value="424">424: [manual_acl] sp=0 qual=1 gr=1 — What is a large event-coverage general-domain even…</option><option value="425">425: [manual_acl] sp=1 qual=1 gr=1 — What is the first paper to address the problem of …</option><option value="427">427: [manual_acl] sp=1 qual=2 gr=1 — What limitations do large language models have in …</option><option value="428">428: [manual_acl] sp=1 qual=2 gr=1 — What paper compares humans' and language models' n…</option><option value="429">429: [manual_acl] sp=1 qual=2 gr=1 — What work attempts to explore multi-hop reasoning …</option><option value="430">430: [manual_acl] sp=1 qual=1 gr=1 — Which article first proposed shuffled-group-whiten…</option><option value="431">431: [manual_acl] sp=1 qual=1 gr=1 — Which dataset supports narration generation and te…</option><option value="432">432: [manual_acl] sp=0 qual=1 gr=1 — Which family of model generally perform the best f…</option><option value="434">434: [manual_acl] sp=1 qual=2 gr=1 — Which knowledge graph completion method focuses on…</option><option value="435">435: [manual_acl] sp=1 qual=1 gr=1 — Which language model distillation paper that first…</option><option value="436">436: [manual_acl] sp=1 qual=2 gr=1 — Which numerical reasoning paper first published a …</option><option value="437">437: [manual_acl] sp=1 qual=2 gr=1 — Which paper about parameter-efficient finetuning f…</option><option value="438">438: [manual_acl] sp=1 qual=1 gr=1 — Which paper combines the advantages of different f…</option><option value="439">439: [manual_acl] sp=0 qual=1 gr=1 — Which paper did a comprehensive survey of the code…</option><option value="440">440: [manual_acl] sp=1 qual=2 gr=1 — Which paper employs a two-stage approach in genera…</option><option value="441">441: [manual_acl] sp=1 qual=1 gr=1 — Which paper enables interactive semantic parsing b…</option><option value="442">442: [manual_acl] sp=1 qual=1 gr=1 — Which paper explored training a GPT-2 for automati…</option><option value="443">443: [manual_acl] sp=1 qual=1 gr=1 — Which paper first aggregates statements to represe…</option><option value="444">444: [manual_acl] sp=1 qual=2 gr=1 — Which paper first applied the chain-of-thought tec…</option><option value="445">445: [manual_acl] sp=1 qual=2 gr=1 — Which paper first apply mixture of experts idea to…</option><option value="446">446: [manual_acl] sp=1 qual=2 gr=1 — Which paper first attempts to take potential depen…</option><option value="447">447: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines different methods for u…</option><option value="448">448: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines rewriting and expansion…</option><option value="449">449: [manual_acl] sp=1 qual=1 gr=1 — Which paper first conducted the positioned error t…</option><option value="450">450: [manual_acl] sp=1 qual=1 gr=1 — Which paper first construct large-scale corpus to …</option><option value="452">452: [manual_acl] sp=1 qual=1 gr=1 — Which paper first explored In-context learning in …</option><option value="453">453: [manual_acl] sp=1 qual=1 gr=1 — Which paper first found that multilingual models c…</option><option value="454">454: [manual_acl] sp=1 qual=2 gr=1 — Which paper first introduced document content as a…</option><option value="455">455: [manual_acl] sp=1 qual=1 gr=1 — Which paper first propose to mask positions to pre…</option><option value="456">456: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed a cross-domain language…</option><option value="457">457: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed extracting the pair of …</option><option value="459">459: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed to combine pretrained m…</option><option value="460">460: [manual_acl] sp=0 qual=1 gr=1 — Which paper first proposed to only update some ori…</option><option value="461">461: [manual_acl] sp=1 qual=1 gr=1 — Which paper first published a real-world Chinese-E…</option><option value="462">462: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that it is possible to mai…</option><option value="463">463: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that large language models…</option><option value="464">464: [manual_acl] sp=1 qual=2 gr=1 — Which paper first studied the efficiency robustnes…</option><option value="465">465: [manual_acl] sp=1 qual=2 gr=1 — Which paper first use the attention weights to gui…</option><option value="466">466: [manual_acl] sp=0 qual=1 gr=1 — Which paper first used structural information for …</option><option value="468">468: [manual_acl] sp=0 qual=1 gr=1 — Which paper highlights the need for leveraging all…</option><option value="469">469: [manual_acl] sp=1 qual=2 gr=1 — Which paper introduce a DRO (distribution robust o…</option><option value="470">470: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduced the human-evaluated timelin…</option><option value="471">471: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduces the R-GCN technique into do…</option><option value="472">472: [manual_acl] sp=1 qual=1 gr=1 — Which paper investigates the influence of the dive…</option><option value="474">474: [manual_acl] sp=0 qual=1 gr=1 — Which paper is the first to comprehensively review…</option><option value="476">476: [manual_acl] sp=1 qual=2 gr=1 — Which paper measured how well the source-translati…</option><option value="477">477: [manual_acl] sp=1 qual=2 gr=1 — Which paper presents an easy to implement and high…</option><option value="478">478: [manual_acl] sp=1 qual=2 gr=1 — Which paper produces a dataset for text simplifica…</option><option value="479">479: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed a learning-based data augment…</option><option value="480">480: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed decomposing the logit update …</option><option value="481">481: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed dictionary-based Bayesian inf…</option><option value="482">482: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed the integration of human tran…</option><option value="483">483: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes a memory-efficient optimizer …</option><option value="484">484: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes the two-stage training method…</option><option value="485">485: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes to use rewriting based approa…</option><option value="486">486: [manual_acl] sp=1 qual=2 gr=1 — Which paper showed that social relationships were …</option><option value="487">487: [manual_acl] sp=1 qual=2 gr=1 — Which paper shows assessment of training instabili…</option><option value="488">488: [manual_acl] sp=1 qual=1 gr=1 — Which paper shows that in instruction tuning, the …</option><option value="489">489: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies how current retrieval systems …</option><option value="490">490: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies the concept of enhancing the c…</option><option value="491">491: [manual_acl] sp=1 qual=2 gr=1 — Which paper surveyed the datasets and tasks of ask…</option><option value="492">492: [manual_acl] sp=1 qual=2 gr=1 — Which paper used both automatically generated and …</option><option value="493">493: [manual_acl] sp=1 qual=2 gr=1 — Which paper utilizes language models to generate s…</option><option value="494">494: [manual_acl] sp=1 qual=1 gr=1 — Which paper was the first to propose combining hum…</option><option value="495">495: [manual_acl] sp=0 qual=2 gr=1 — Which papers develop methods to make in-context le…</option><option value="496">496: [manual_acl] sp=0 qual=1 gr=1 — Which papers were among the first to explore the t…</option><option value="497">497: [manual_acl] sp=1 qual=1 gr=1 — Which pre-trained model is specifically designed f…</option><option value="499">499: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model can demonstrate that v…</option><option value="500">500: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model paper in 2023 develope…</option><option value="501">501: [manual_acl] sp=1 qual=2 gr=1 — Which was the first paper to explore the online ad…</option><option value="503">503: [manual_acl] sp=1 qual=1 gr=1 — Which work proposes an approach to improve candida…</option><option value="504">504: [manual_acl] sp=1 qual=2 gr=1 — what's the first paper that manages to handle KBQA…</option><option value="505">505: [manual_acl] sp=1 qual=1 gr=1 — which paper first focuses on addressing the over-s…</option><option value="506">506: [manual_iclr] sp=1 qual=1 gr=1 — Can we reduce visual tokens in vision transformers…</option><option value="507">507: [manual_iclr] sp=1 qual=1 gr=1 — Can we learn to represent an image with arbitary n…</option><option value="508">508: [manual_iclr] sp=1 qual=2 gr=1 — Are there any papers that construct convolutional …</option><option value="509">509: [manual_iclr] sp=1 qual=1 gr=1 — Are there any papers that study whether you can id…</option><option value="511">511: [manual_iclr] sp=1 qual=1 gr=1 — Are there datasets and benchmarks available for me…</option><option value="512">512: [manual_iclr] sp=1 qual=2 gr=1 — Are there sequential learning guarantees for confi…</option><option value="513">513: [manual_iclr] sp=1 qual=2 gr=1 — Can we find the solution of the Bilevel optimizati…</option><option value="514">514: [manual_iclr] sp=0 qual=2 gr=1 — Can you find a dataset that shows LLM-based evalua…</option><option value="515">515: [manual_iclr] sp=1 qual=2 gr=1 — Can you find a research paper that discusses using…</option><option value="516">516: [manual_iclr] sp=1 qual=1 gr=1 — I'm using Local SGD with a decaying learning rate …</option><option value="517">517: [manual_iclr] sp=1 qual=1 gr=1 — In video diffusion models, is there any paper that…</option><option value="518">518: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper illustrating that pre-trained tra…</option><option value="519">519: [manual_iclr] sp=1 qual=2 gr=1 — Is there a paper that takes a mixed machine learni…</option><option value="520">520: [manual_iclr] sp=1 qual=1 gr=1 — Is there a paper which applies Bayesian optimizati…</option><option value="521">521: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper which proposes a general data sel…</option><option value="523">523: [manual_iclr] sp=0 qual=1 gr=1 — Is there a single GNN model that can inductively g…</option><option value="524">524: [manual_iclr] sp=1 qual=2 gr=1 — Is there a theory paper that explains why sometime…</option><option value="525">525: [manual_iclr] sp=1 qual=1 gr=1 — Is there an existing dataset of images with alt-te…</option><option value="526">526: [manual_iclr] sp=1 qual=1 gr=1 — Is there any generalizable NeRF paper that disenta…</option><option value="527">527: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper applies off-shelf GPT-2 model i…</option><option value="528">528: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper improves adversarial training b…</option><option value="529">529: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that explores ways to parameter…</option><option value="530">530: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that previously proposed to con…</option><option value="531">531: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that seamlessly integrates the …</option><option value="532">532: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that theoretically explains why…</option><option value="533">533: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that uses Lipschitz continuity …</option><option value="534">534: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper trying to improve MLE for auto-…</option><option value="535">535: [manual_iclr] sp=1 qual=1 gr=1 — Name a paper which proposes a probabilsitic formul…</option><option value="536">536: [manual_iclr] sp=1 qual=2 gr=1 — What are some evaluation benchmarks for LLM privac…</option><option value="537">537: [manual_iclr] sp=1 qual=1 gr=1 — What are the key advantages of coupling neural SDE…</option><option value="538">538: [manual_iclr] sp=1 qual=2 gr=1 — What is a paper studying data being collected in b…</option><option value="539">539: [manual_iclr] sp=1 qual=1 gr=1 — What is the first paper that theoretically studies…</option><option value="540">540: [manual_iclr] sp=1 qual=2 gr=1 — What is the first paper that uses the generalized …</option><option value="541">541: [manual_iclr] sp=1 qual=1 gr=1 — What molecular representation learning paper intro…</option><option value="542">542: [manual_iclr] sp=1 qual=2 gr=1 — What open-source dataset combined knowledge retrie…</option><option value="543">543: [manual_iclr] sp=0 qual=1 gr=1 — What paper considers sensitive data issue when pro…</option><option value="544">544: [manual_iclr] sp=0 qual=1 gr=1 — What paper evaluated the ability of visual few-sho…</option><option value="545">545: [manual_iclr] sp=1 qual=1 gr=1 — What paper first adapted ControlNet to generate co…</option><option value="546">546: [manual_iclr] sp=1 qual=1 gr=1 — What paper first associate the modeling frequency …</option><option value="547">547: [manual_iclr] sp=1 qual=2 gr=1 — What paper first extends rotary positional encodin…</option><option value="548">548: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposed a robust perceptual simi…</option><option value="549">549: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposes that simply reversing th…</option><option value="551">551: [manual_iclr] sp=1 qual=1 gr=1 — What paper first used the technique of prompt engi…</option><option value="552">552: [manual_iclr] sp=1 qual=1 gr=1 — What paper first uses decoupled workers in distrib…</option><option value="553">553: [manual_iclr] sp=1 qual=2 gr=1 — What paper investigated the effect of the relative…</option><option value="554">554: [manual_iclr] sp=1 qual=1 gr=1 — What paper is the first to prove finetuned LLM can…</option><option value="555">555: [manual_iclr] sp=1 qual=1 gr=1 — What paper mitigates language model sampling error…</option><option value="556">556: [manual_iclr] sp=1 qual=2 gr=1 — What paper mitigates the vocabulary size limitatio…</option><option value="558">558: [manual_iclr] sp=1 qual=1 gr=1 — What paper showed first that one can build a fully…</option><option value="559">559: [manual_iclr] sp=0 qual=1 gr=1 — What paper shows that RLAIF can fully replace RLHF…</option><option value="561">561: [manual_iclr] sp=1 qual=1 gr=1 — What work first uses LLM to code robotic simulatio…</option><option value="562">562: [manual_iclr] sp=1 qual=2 gr=1 — What work proposes a model to learn a latent regul…</option><option value="563">563: [manual_iclr] sp=1 qual=1 gr=1 — What work proposes to combine video foundation mod…</option><option value="565">565: [manual_iclr] sp=1 qual=1 gr=1 — Which foundation model paper first proposed a time…</option><option value="567">567: [manual_iclr] sp=1 qual=1 gr=1 — Which machine learning paper proposed certified ro…</option><option value="568">568: [manual_iclr] sp=0 qual=1 gr=1 — Which multimodal large language model represents v…</option><option value="569">569: [manual_iclr] sp=1 qual=1 gr=1 — Which neural theorem proving paper first attempted…</option><option value="570">570: [manual_iclr] sp=1 qual=2 gr=1 — Which paper considers both weights and activations…</option><option value="571">571: [manual_iclr] sp=1 qual=1 gr=1 — Which paper contains quantitative results demonstr…</option><option value="572">572: [manual_iclr] sp=1 qual=1 gr=1 — Which paper examined the scalability of instructio…</option><option value="573">573: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first applied the chain of thought con…</option><option value="574">574: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first derived online occupany estimati…</option><option value="575">575: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first found that REINFORCE works bette…</option><option value="576">576: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first found that when transformers are…</option><option value="577">577: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first investigates the knowledge prefe…</option><option value="578">578: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first proposes a unified framework for…</option><option value="579">579: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first proved that wide-enough transfor…</option><option value="580">580: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first showed that task-specific knowle…</option><option value="581">581: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first studied differential privacy for…</option><option value="582">582: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first study POMDP with enhanced feedba…</option><option value="583">583: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first tried to fine-tune LLMs with cha…</option><option value="584">584: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first used language models to emulate …</option><option value="585">585: [manual_iclr] sp=1 qual=1 gr=1 — Which paper formally defines the problem of model …</option><option value="586">586: [manual_iclr] sp=1 qual=2 gr=1 — Which paper found that using common character enco…</option><option value="587">587: [manual_iclr] sp=1 qual=1 gr=1 — Which paper in human motion generation can control…</option><option value="588">588: [manual_iclr] sp=1 qual=2 gr=1 — Which paper is the first to model the helpfulness …</option><option value="589">589: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes an alignment framework that s…</option><option value="590">590: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes to integrate black-box LLMs w…</option><option value="591">591: [manual_iclr] sp=1 qual=1 gr=1 — Which paper studies how difficult is a policy lear…</option><option value="592">592: [manual_iclr] sp=1 qual=2 gr=1 — Which paper trains on linear regression to hypothe…</option><option value="593">593: [manual_iclr] sp=1 qual=1 gr=1 — Which paper uses the latent diffusion model for th…</option><option value="594">594: [manual_iclr] sp=1 qual=2 gr=1 — Which paper utilized MMD flows with Riesz kernels …</option><option value="596">596: [manual_iclr] sp=1 qual=2 gr=1 — Which paper systematically examed the input mismat…</option></select>
    <p id="queryText" class="small has-abs" data-abs="">Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?</p>
    <p id="queryMeta" class="small">set:manual_acl | spec:1 | qual:2 | gr:0</p>

    <label>Relevant document (rank&nbsp;+&nbsp;title)</label>
    <select id="docSel"><option value="0" data-id="254877406" title="State-of-the-art poetry generation systems are often complex. They either consist of taskspecific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.1">0: rank=35  ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models</option></select>
    <p id="docTitle" class="small has-abs" data-abs="State-of-the-art poetry generation systems are often complex. They either consist of taskspecific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.1">0: rank=35  ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models</p>
  </div>

  <!-- PROMPT / LLM / ANNOTATION CARD -->
  <div class="card" style="flex:2 1 520px;">
    <h2>Prompt &amp; LLM setup</h2>

    <label>Prompt</label>
    <div style="display:flex;gap:.5rem;">
      <select id="promptSel" style="flex:1;"><option value="dummy">dummy</option><option value="full_text">full_text</option><option value="title_abstract">title_abstract</option></select>
      <button id="newPromptBtn">+&nbsp;New</button>
    </div>

    <label>Extractor</label>
    <select id="extSel"><option value="dummy">dummy</option><option value="json_list_extractor">json_list_extractor</option></select>

    <label>k (top‑k retrieval)</label>
    <input id="kInp" type="number" value="50" min="1">

    <label>Prompt text</label>
    <textarea id="promptBox"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="savePromptBtn">💾&nbsp;Save</button>
      <button id="reloadPromptBtn">⟳&nbsp;Reload</button>
    </div>

    <details style="margin-top:.75rem;">
      <summary><strong>LLM config</strong></summary>
      <label>API key</label><input id="apiKey" type="text" placeholder="sk‑…">
      <label>Model</label><input id="model" type="text" value="gpt-4o-mini">
      <label>Temperature</label><input id="temp" type="number" value="0" step=".1">
      <label>Max tokens</label><input id="maxTok" type="number" value="2048">
      <label style="display:flex;align-items:center;gap:.5rem;margin-top:.5rem;">
        <input id="wantJson" type="checkbox"> Expect JSON object response
      </label>
    </details>

    <label>Your annotation</label>
    <textarea id="noteBox" placeholder="Add notes about this run…"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="saveNoteBtn" style="background:#0b63ff;color:#fff;">Save&nbsp;annotation</button>
      <button id="runBtn" style="background:#14a44d;color:#fff;">Run</button>
    </div>
  </div>
</section>

<!-- BEFORE / AFTER TABLES ---------------------------------------------->
<section class="flex">
  <div class="card">
    <h2>Before&nbsp;(original)</h2>
    <table id="beforeTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody><tr class="has-abs" data-abs="Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular. 1"><td>1</td><td>Neural Machine Translation without Embeddings</td><td>0.470</td></tr><tr class="has-abs" data-abs="State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce CHARFORMER, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that CHARFORMER outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, CHARFORMER is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end."><td>2</td><td>CHARFORMER: FAST CHARACTER TRANSFORMERS VIA GRADIENT-BASED SUBWORD TOKENIZATION</td><td>0.512</td></tr><tr class="has-abs" data-abs="We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label]  where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than languagespecific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-ofthe-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning &quot;from scratch&quot; in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text."><td>3</td><td>Multilingual Language Processing From Bytes</td><td>0.520</td></tr><tr class="has-abs" data-abs="Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences-without explicit tokenization or vocabulary-and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 5.7 F1 on TYDI QA, a challenging multilingual benchmark, despite having fewer model parameters.73"><td>4</td><td>CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</td><td>0.545</td></tr><tr class="has-abs" data-abs="Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus may not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes oversegment low-resource languages, leading to a drop in translation performance. An alternative to subword tokenizers is byte-based tokenization, i.e., tokenization into byte sequences using the UTF-8 encoding scheme. Byte tokens often represent inputs at a sub-character granularity, i.e., one character can be represented by a span of byte tokens. This results in much longer byte sequences that are hard to interpret without aggregating local information from multiple byte tokens. In this paper, we propose a Local Byte Fusion (LOBEF) method for byte-based machine translation-utilizing byte n-gram and word boundaries-to aggregate local semantic information. Extensive experiments on multilingual translation, zero-shot cross-lingual transfer, and domain adaptation reveal a consistent improvement over vanilla byte-based models. Further analysis also indicates that our byte-based models are parameter-efficient and perform competitive to subword models."><td>5</td><td>Local Byte Fusion for Neural Machine Translation</td><td>0.550</td></tr><tr class="has-abs" data-abs="The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE)(Sennrich et al., 2016;Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE."><td>6</td><td>Byte Pair Encoding is Suboptimal for Language Model Pretraining</td><td>0.558</td></tr><tr class="has-abs" data-abs="Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intraword module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift."><td>7</td><td>From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding</td><td>0.560</td></tr><tr class="has-abs" data-abs="Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. 1  We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels. arXiv:2207.06991v2 [cs.CL] 26 Apr 2023 Published as a conference paper at ICLR 2023 softmax age ion Embedding ⊕ Encoder its in a beautiful box full of black beans. CLS MLP ue: 0.999 / False: 0.001 ᓚᘏᗢ sits in a beautiful box beans. Render Text as Image 1 My cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner. My cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner. Projection + Position Embedding 2 ⊕ CLS Embedding &amp; Span Mask m patches 3 CLS Encoder My cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner. CLS CLS Decoder (a) PIXEL pretraining softmax Render Text as Image 1 Projection + Position Embedding 2 ⊕ Encoder My cool cat ᓚᘏᗢ sits in a beautiful box full of black beans. CLS Embedding 3 CLS CLS CLS MLP True: 0.999 / False: 0.001 My cool cat ᓚᘏᗢ sits in a beautiful box full of black beans."><td>8</td><td>LANGUAGE MODELLING WITH PIXELS</td><td>0.562</td></tr><tr class="has-abs" data-abs="The zero-shot cross-lingual ability of models pretrained on multilingual and even monolingual corpora has spurred many hypotheses to explain this intriguing empirical result. However, due to the costs of pretraining, most research uses public models whose pretraining methodology, such as the choice of tokenization, corpus size, and computational budget, might differ drastically. When researchers pretrain their own models, they often do so under a constrained budget, and the resulting models might underperform significantly compared to SOTA models. These experimental differences led to various inconsistent conclusions about the nature of the cross-lingual ability of these models. To help further research on the topic, we released 10 monolingual byte-level models 1 rigorously pretrained under the same configuration with a large compute budget (equivalent to 420 days on a V100) and corpora that are 4 times larger than the original BERT's. Because they are tokenizer-free, the problem of unseen token embeddings is eliminated, thus allowing researchers to try a wider range of cross-lingual experiments in languages with different scripts. Additionally, we release two models pretrained on non-natural language texts that can be used in sanity-check experiments. Experiments on QA and NLI tasks show that our monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen our understanding of cross-lingual transferability in language models."><td>9</td><td>MonoByte: A Pool of Monolingual Byte-level Language Models</td><td>0.566</td></tr><tr class="has-abs" data-abs="The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text whi te space correctio nwithTransf or mers, produce whitespace correction with Transformers. We compare two Transformer-based models, a characterlevel encoder-decoder model and a byte-level encoder-only model. We find that the encoderonly model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F 1 -score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces."><td>10</td><td>Fast Whitespace Correction with Encoder-Only Transformers</td><td>0.568</td></tr><tr class="has-abs" data-abs="Transformers in their common form are inherently limited to operate on whole token sequences rather than on one token at a time. Consequently, their use during online inference on time-series data entails considerable redundancy due to the overlap in successive token sequences. In this work, we propose novel formulations of the Scaled Dot-Product Attention, which enable Transformers to perform efficient online token-by-token inference on a continual input stream. Importantly, our modifications are purely to the order of computations, while the outputs and learned weights are identical to those of the original Transformer Encoder. We validate our Continual Transformer Encoder with experiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable results: Our Continual one-and two-block architectures reduce the floating point operations per prediction by up to 63× and 2.6×, respectively, while retaining predictive performance."><td>11</td><td>Published as a conference paper at ICLR 2023 CONTINUAL TRANSFORMERS: REDUNDANCY-FREE ATTENTION FOR ONLINE INFERENCE</td><td>0.591</td></tr><tr class="has-abs" data-abs="We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the representations as well as to using them in an application. We demonstrate better performance than prior work on entity typing and text denoising."><td>12</td><td>Nonsymbolic Text Representation</td><td>0.592</td></tr><tr class="has-abs" data-abs="For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods."><td>13</td><td>Stochastic Tokenization with a Language Model for Neural Text Classification</td><td>0.593</td></tr><tr class="has-abs" data-abs="In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.Autoregressive (AR) models have shown excellent results in generating text (e.g., GPT-3, Brown  et al., 2020). However, while their training scales very well, sampling is prohibitively slow for many practical applications. Moreover, there are limitations to the kinds of conditioning AR models can seamlessly handle: the left-to-right restriction makes it hard to &quot;fill in the gaps&quot; in a partially written text draft. Even more importantly, this prohibits iterative refinement of complete text drafts to make them more self-consistent, which is a common task for human writers. Finally, AR models require network architectures to be causal, severely limiting the kinds of neural network architectures that can be used for text-modeling. All of these motivated the machine learning community to make extensive efforts to propose alternatives to AR models.Machine translation (MT) was perhaps one of the first tasks where non-AR approaches were shown to seriously rival the AR-based state of the art: methods like CMLM (Ghazvininejad et al., 2019)  and DisCo (Kasai et al., 2020)  show promising results and their decoding speed is excellent compared to AR. However, while their performance is competitive, they are still behind the AR benchmark and actually require distillation of a larger AR model -without which, performance drops considerably.Non-AR methods have proven hard to apply to the general unconditional language modeling (LM) task. When there is no conditioning, the multi-modality problem becomes paramount, as shown by Gu et al. (2017), which likely makes it problematic to use methods like CMLM and DisCo because their decoding mechanism is deterministic and does not model uncertainty. Yet, recently the community has seen promising results from non-AR models like Multinomial Diffusion (Hoogeboom et al., 2021)  and D3PM (Austin et al., 2021). These methods optimize a lower bound (ELBO) on likelihoods and have shown negative log-likelihood (NLL) results approaching AR models on several benchmarks like text8 (Mahoney, 2011) and LM1B (Chelba et al., 2013). However, a major gap in NLL persists, and samples from those models lack coherence.In this paper we propose a novel non-autoregressive method which shows state-of-the-art results in machine translation on WMT'14 EN→DE raw data (without distillation from AR) amongst non-AR methods and good qualitative results on unconditional language modeling on the Colossal Clean Common Crawl (C4) dataset(Raffel et al., 2019)and a dataset of Python code from GitHub. Our model operates as a time-homogeneous Markov Chain similar to that of Lee et al. (2018): conditioned on the corrupted data, it tries to approximate the original uncorrupted samples by a per-token * Shared first authorship. , et al. Language models are few-shot learners. arXiv preprint arXiv:One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020a.Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Pre-training transformers as energy-based cloze models. arXiv preprint arXiv:2012.08561, 2020b.Cyprien de Masson d'Autume, Mihaela Rosca, Jack Rae, and Shakir Mohamed. Training language gans from scratch. arXiv preprint arXiv:1905.09922, 2019."><td>14</td><td>STEP-UNROLLED DENOISING AUTOENCODERS FOR TEXT GENERATION</td><td>0.596</td></tr><tr class="has-abs" data-abs="We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations. * Joint first-authors. Ordered randomly. † Work done as a member of the Google Brain Residency (g.co/brainresidency)"><td>15</td><td>Published as a conference paper at ICLR 2018 GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES</td><td>0.598</td></tr><tr class="has-abs" data-abs="The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoderdecoder with a subword-level encoder and a character-level decoder on four language pairs-En-Cs, En-De, En-Ru and En-Fiusing the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru."><td>16</td><td>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</td><td>0.601</td></tr><tr class="has-abs" data-abs="Byte-pair encodings is a method for splitting a word into sub-word tokens, a language model then assigns contextual representations separately to each of these tokens. In this paper, we evaluate four different methods of composing such sub-word representations into word representations. We evaluate the methods on morphological sequence classification, the task of predicting grammatical features of a word. Our experiments reveal that using an RNN to compute word representations is consistently more effective than the other methods tested across a sample of eight languages with different typology and varying numbers of byte-pair tokens per word."><td>17</td><td>Composing Byte-Pair Encodings for Morphological Sequence Classification</td><td>0.602</td></tr><tr class="has-abs" data-abs="Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards highperformance and easy to train character-based models that are not extremely large."><td>18</td><td>Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems</td><td>0.602</td></tr><tr class="has-abs" data-abs="Grammatical error correction (GEC) is the task of correcting typos, spelling, punctuation and grammatical issues in text. Approaching the problem as a sequence-to-sequence task, we compare the use of a common subword unit vocabulary and byte-level encoding. Initial synthetic training data is created using an error-generating pipeline, and used for finetuning two subword-level models and one bytelevel model. Models are then finetuned further on hand-corrected error corpora, including texts written by children, university students, dyslexic and second-language writers, and evaluated over different error types and origins. We show that a byte-level model enables higher correction quality than a subword approach, not only for simple spelling errors, but also for more complex semantic, stylistic and grammatical issues. In particular, initial training on synthetic corpora followed by finetuning on a relatively small parallel corpus of real-world errors helps the byte-level model correct a wide range of commonly occurring errors. Our experiments are run for the Icelandic language but should hold for other similar languages, particularly morphologically rich ones.https://github.com/mideind/ byte-gec"><td>19</td><td>Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora</td><td>0.603</td></tr><tr class="has-abs" data-abs="Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network. MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit."><td>20</td><td>MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning WARNING: This paper contains model outputs which are offensive in nature</td><td>0.604</td></tr><tr class="has-abs" data-abs="The predominant approach for language modeling is to process sequences from left to right, but this eliminates a source of information: the order by which the sequence was generated. One strategy to recover this information is to decode both the content and ordering of tokens. Existing approaches supervise content and ordering by designing problem-specific loss functions and pre-training with an ordering pre-selected. Other recent works use iterative search to discover problemspecific orderings for training, but suffer from high time complexity and cannot be efficiently parallelized. We address these limitations with an unsupervised parallelizable learner that discovers high-quality generation orders purely from training data-no domain knowledge required. The learner contains an encoder network and decoder language model that perform variational inference with autoregressive orders (represented as permutation matrices) as latent variables. The corresponding ELBO is not differentiable, so we develop a practical algorithm for end-to-end optimization using policy gradients. We implement the encoder as a Transformer with non-causal attention that outputs permutations in one forward pass. Permutations then serve as target generation orders for training an insertionbased Transformer language model. Empirical results in language modeling tasks demonstrate that our method is context-aware and discovers orderings that are competitive with or even better than fixed orders. * Authors contributed equally."><td>21</td><td>DISCOVERING NON-MONOTONIC AUTOREGRESSIVE ORDERINGS WITH VARIATIONAL INFERENCE</td><td>0.604</td></tr><tr class="has-abs" data-abs="Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute."><td>22</td><td>ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS</td><td>0.607</td></tr><tr class="has-abs" data-abs="The semantics of a text is manifested not only by what is read, but also by what is not read. In this article, we will study how the implicit &quot;not read&quot; information such as end-of-paragraph (EOP) and end-of-sequence (EOS) affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the EOP in the fine-tuning stage. Experimental results on English story generation show that EOP can lead to higher BLEU score and lower EOS perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without EOP or EOS during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with EOP. Our code is available on GitHub. 1 . 2019. How decoding strategies affect the verifiability of generated text. CoRR, abs/1911.03587.Alec Radford. 2018. Improving language understanding by generative pre-training."><td>23</td><td>Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2</td><td>0.607</td></tr><tr class="has-abs" data-abs="We propose a novel gradient-based attack against transformer-based language models that searches for an adversarial example in a continuous space of token probabilities. Our algorithm mitigates the gap between adversarial loss for continuous and discrete text representations by performing multi-step quantization in a quantization-compensation loop. Experiments show that our method significantly outperforms other approaches on various natural language processing (NLP) tasks."><td>24</td><td>Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks</td><td>0.607</td></tr><tr class="has-abs" data-abs="Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters-typically a multiple of their output dimension-and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations."><td>25</td><td>Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks</td><td>0.607</td></tr><tr class="has-abs" data-abs="Cet article propose une architecture neuronale pour un modèle de langue à vocabulaire ouvert. Les représentations continues des mots sont calculées à la volée à partir des caractères les composant, gràce à une couche convolutionnelle suivie d'une couche de regroupement (pooling). Cela permet au modèle de représenter n'importe quel mot, qu'il fasse partie du contexte ou soit évalué pour la prédiction. La fonction objectif est dérivée de l'estimation contrastive bruitée (Noise Contrastive Estimation, ou NCE), calculable dans notre cas sans vocabulaire. Nous évaluons la capacité de notre modèle à construire des représentations continues de mots inconnus sur la tâche de traduction automatique IWSLT-2016, de l'Anglais vers le Tchèque, en ré-évaluant les N meilleures hypothèses (N-best reranking). Les résultats expérimentaux permettent des gains jusqu'à 0,7 point BLEU. Ils montrent aussi la difficulté d'utiliser des représentations dérivées des caractères pour la prédiction.ABSTRACTOpening the vocabulary of neural language models with character-level word representations This paper introduces an architecture for an open-vocabulary neural language model. Word representations are computed on-the-fly by a convolution network followed by pooling layer. This allows the model to consider any word, in the context or for the prediction. The training objective is derived from the Noise-Contrastive Estimation to adapt it the open vocabulary case. We test the ability of our model to build representations of unknown words on the MT task of IWSLT-2016 from English to Czech, in a reranking setting. Experimental results show a gain up to 0.7 BLEU point. They also emphasize the difficulty and instability when training such models with character-based representations for the predicted words.MOTS-CLÉS : Modèle de langue neuronal, Représentations continues dérivées des caractères,Traduction automatique par approche statistique.Description du modèleHabituellement, les représentations continues des mots apprises par le modèle sont des vecteurs de paramètres (embeddings), stockés dans une matrice L. La représentation r word w d'un mot w est simplement la colonne de L correspondant à son indice dans le vocabulaire :Cette première méthode est illustrée figure 1, dans l'encadré rouge."><td>26</td><td>Représentations continues dérivées des caractères pour un modèle de langue neuronal à vocabulaire ouvert</td><td>0.608</td></tr><tr class="has-abs" data-abs="Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token's string representation. We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens. Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to accurately spell up to a third of the vocabulary and reach high character ngram overlap across all token types. We further test whether enriching subword models with character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment. Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks. 1"><td>27</td><td>Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens</td><td>0.608</td></tr><tr class="has-abs" data-abs="Context plays an important role in human language understanding, thus it may also be useful for machines learning vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. We carefully designed experiments to show that neither an autoregressive decoder nor an RNN decoder is required. After that, we designed a model which still keeps an RNN as the encoder, while using a non-autoregressive convolutional decoder. We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabelled corpora, and in both cases the transferability is evaluated on a set of downstream NLP tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks."><td>28</td><td>Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding</td><td>0.608</td></tr><tr class="has-abs" data-abs="Much recent effort has been invested in non-autoregressive neural machine translation, which appears to be an efficient alternative to state-of-the-art autoregressive machine translation on modern GPUs. In contrast to the latter, where generation is sequential, the former allows generation to be parallelized across target token positions. Some of the latest non-autoregressive models have achieved impressive translation quality-speed tradeoffs compared to autoregressive baselines. In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a single-layer autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. We show that the speed disadvantage for autoregressive baselines compared to non-autoregressive methods has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. Our results establish a new protocol for future research toward fast, accurate machine translation. Our code is available at https://github.com/jungokasai/deep-shallow.Recent work proposed methods to mitigate this multimodality issue, including iterative refinement (e.g.,Lee et al., 2018;Ghazvininejad et al., 2019), and modeling with latent variables (e.g.,Ma et al., 2019;Shu et al., 2020). These approaches modify the decoder transformer to find a balance between decoding parallelism and translation quality. In this work, however, we adopt a different speed-quality tradeoff. Recent work byKim et al. (2019)in autoregressive machine translation (AR) suggests that better speed-quality tradeoffs can be achieved by having different depths in the encoder and the decoder. Here, we make a formal argument in favor of deep encoder, shallow decoder configurations and empirically demonstrate better speed-quality tradeoffs for the AR baselines. * Work partially done at Facebook AI."><td>29</td><td>Published as a conference paper at ICLR 2021 DEEP ENCODER, SHALLOW DECODER: REEVALUATING NON-AUTOREGRESSIVE MACHINE TRANSLATION</td><td>0.610</td></tr><tr class="has-abs" data-abs="This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task. guistics: EMNLP 2020, pages 4617-4624, Online. Association for Computational Linguistics. . 2015. A large annotated corpus for learning natural language inference. In . 2015. Microsoft coco captions: Data collection and evaluation server."><td>30</td><td>How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese</td><td>0.610</td></tr><tr class="has-abs" data-abs="Encoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks."><td>31</td><td>Is Encoder-Decoder Redundant for Neural Machine Translation?</td><td>0.610</td></tr><tr class="has-abs" data-abs="We report on our system for the shared task on discrimination of similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies."><td>32</td><td>Byte-based Language Identification with Deep Convolutional Networks</td><td>0.610</td></tr><tr class="has-abs" data-abs="Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence. However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses. We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism 1 . * This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1  Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM"><td>33</td><td>Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models</td><td>0.611</td></tr><tr class="has-abs" data-abs="This paper proposes a message-passing mechanism to address language modelling. A new layer type is introduced that aims to substitute self-attention for unidirectional sequence generation tasks. The system is shown to be competitive with existing methods: Given N tokens, the computational complexity is O(N logN ) and the memory complexity is O(N ) under reasonable assumptions. In the end, the Dispatcher layer is seen to achieve comparable perplexity to prior results while being more efficient 1 . Amodei. 2020. Language models are few-shot learners.Ciprian Chelba, Tomás Mikolov, Mike Schuster, Qi Ge,  Thorsten Brants, and Phillipp Koehn. 2013. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005. . 2020. Rethinking attention with performers."><td>34</td><td>Dispatcher: A Message-Passing Approach To Language Modelling</td><td>0.612</td></tr><tr class="has-abs" data-abs="State-of-the-art poetry generation systems are often complex. They either consist of taskspecific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.1"><td>35</td><td><strong class="rel">ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models</strong></td><td>0.612</td></tr><tr class="has-abs" data-abs="Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems.This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction.Autoencoders are attractive because of their latent space structure and generative properties.We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model.We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder.We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models. 1"><td>36</td><td>Sentence Bottleneck Autoencoders from Transformer Language Models</td><td>0.613</td></tr><tr class="has-abs" data-abs="In this study, we compare token representations constructed from visual features (i.e., pixels) with standard lookup-based embeddings. Our goal is to gain insight about the challenges of encoding a text representation from low-level features, e.g. from characters or pixels. We focus on Chinese, which-as a logographic language-has properties that make a representation via visual features challenging and interesting. To train and evaluate different models for the token representation, we chose the task of character-based neural machine translation (NMT) from Chinese to English. We found that a token representation computed only from visual features can achieve competitive results to lookup embeddings. However, we also show different strengths and weaknesses in the models' performance in a part-ofspeech tagging task and also a semantic similarity task. In summary, we show that it is possible to achieve a text representation only from pixels. We hope that this is a useful stepping stone for future studies that exclusively rely on visual input, or aim at exploiting visual features of written language."><td>37</td><td>Learning Distributional Token Representations from Visual Features</td><td>0.614</td></tr><tr class="has-abs" data-abs="Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our past decode regularization (PDR) method achieves state-of-the-art word level perplexity on the Penn Treebank (55.6) and WikiText-2 (63.5) datasets and bits-per-character on the Penn Treebank Character (1.169) dataset for character level language modeling. Using dynamic evaluation, we also achieve the first sub 50 perplexity of 49.3 on the Penn Treebank test set."><td>38</td><td>IMPROVED LANGUAGE MODELING BY DECODING THE PAST</td><td>0.615</td></tr><tr class="has-abs" data-abs="We propose MASKER, an unsupervised textediting method for style transfer. To tackle cases when no parallel source-target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that MASKER performs competitively in a fully unsupervised setting. Moreover, in lowresource settings, it improves supervised methods' accuracy by over 10 percentage points when pre-training them on silver training data generated by MASKER."><td>39</td><td>Unsupervised Text Style Transfer with Padded Masked Language Models</td><td>0.615</td></tr><tr class="has-abs" data-abs="Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art nonautoregressive NMT models and almost constant decoding time w.r.t the sequence length. 1"><td>40</td><td>FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</td><td>0.615</td></tr><tr class="has-abs" data-abs="Since traditional tokenizers are isolated from a downstream task and model, they cannot output an appropriate tokenization depending on the task and model, although recent studies imply that the appropriate tokenization improves the performance. In this paper, we propose a novel method to find an appropriate tokenization to a given downstream model by jointly optimizing a tokenizer and the model. The proposed method has no restriction except for using loss values computed by the downstream model to train the tokenizer, and thus, we can apply the proposed method to any NLP task. Moreover, the proposed method can be used to explore the appropriate tokenization for an already trained model as post-processing. Therefore, the proposed method is applicable to various situations. We evaluated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations. 9 https"><td>41</td><td>Joint Optimization of Tokenization and Downstream Model</td><td>0.616</td></tr><tr class="has-abs" data-abs="State-of-the-art neural models typically encode document-query pairs using crossattention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant computational cost. This paper proposes a new training and inference paradigm for re-ranking. We propose to finetune a pretrained encoder-decoder model using in the form of document to query generation.Subsequently, we show that this encoder-decoder architecture can be decomposed into a decoder-only language model during inference.This results in significant inference time speedups since the decoder-only architecture only needs to learn to interpret static encoder embeddings during inference. Our experiments show that this new paradigm achieves results that are comparable to the more expensive cross-attention ranking approaches while being up to 6.8X faster. We believe this work paves the way for more efficient neural rankers that leverage large pretrained models."><td>42</td><td>ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference</td><td>0.617</td></tr><tr class="has-abs" data-abs="This paper presents a simple method that extends a standard Transformer-based autoregressive decoder, to speed up decoding. The proposed method generates a token from the head and tail of a sentence (two tokens in total) in each step. By simultaneously generating multiple tokens that rarely depend on each other, the decoding speed is increased while the degradation in translation quality is minimized. In our experiments, the proposed method increased the translation speed by around 113%-155% in comparison with a standard autoregressive decoder, while degrading the BLEU scores by no more than 1.03. It was faster than an iterative nonautoregressive decoder in many conditions."><td>43</td><td>Transformer-based Double-token Bidirectional Autoregressive Decoding in Neural Machine Translation</td><td>0.617</td></tr><tr class="has-abs" data-abs="Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for nonautoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster."><td>44</td><td>Mask-Predict: Parallel Decoding of Conditional Masked Language Models</td><td>0.617</td></tr><tr class="has-abs" data-abs="Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment."><td>45</td><td>Fully Character-Level Neural Machine Translation without Explicit Segmentation</td><td>0.617</td></tr><tr class="has-abs" data-abs="We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new openvocabulary language modelling benchmark derived from books, PG-19.Sukhbaatar et al., 2019;Lample et al., 2019). However sparse attention does not solve the storage problem, and often requires custom sparse kernels for efficient implementation. Instead we look back to the notion of compactly representing the past. We show this can be built with simple dense linear-algebra components, such as convolutions, and can reduce both the space and compute cost of our models.We propose the Compressive Transformer, a simple extension to the Transformer which maps past hidden activations (memories) to a smaller set of compressed representations (compressed memories). The Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, learning to query both its short-term granular memory and longer-term coarse memory. We observe this improves the modelling of text, achieving state-of-the-art results in character-based language modelling -0.97 bpc on Enwik8 from the Hutter Prize (Hutter, 2012) -and word-level language modelling -17.1 perplexity on WikiText-103 (Merity et al., 2016). Specifically, we see the Compressive Transformer improves the modelling of rare words.We show the Compressive Transformer works not only for language, but can also model the waveform of high-frequency speech with a trend of lower likelihood than the TransformerXL and Wavenet  when trained over 400,000 steps. We also show the Compressive Transformer can be used as a memory component within an RL agent,IMPALA (Espeholt et al., 2018), and can successfully compress and make use of past observations."><td>46</td><td>COMPRESSIVE TRANSFORMERS FOR LONG-RANGE SEQUENCE MODELLING</td><td>0.617</td></tr><tr class="has-abs" data-abs="We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use."><td>47</td><td>THE NEURAL NOISY CHANNEL</td><td>0.617</td></tr><tr class="has-abs" data-abs="The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding."><td>48</td><td>Transformer-Based Direct Hidden Markov Model for Machine Translation</td><td>0.617</td></tr><tr class="has-abs" data-abs="This paper investigates two latent alignment models for non-autoregressive machine translation, namely CTC and Imputer. CTC generates outputs in a single step, makes strong conditional independence assumptions about output variables, and marginalizes out latent alignments using dynamic programming. Imputer generates outputs in a constant number of steps, and approximately marginalizes out possible generation orders and latent alignments for training. These models are simpler than existing non-autoregressive methods, since they do not require output length prediction as a pre-process. In addition, our architecture is simpler than typical encoder-decoder architectures, since input-output cross attention is not used. On the competitive WMT'14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the baseline autoregressive Transformer with 27.8 BLEU."><td>49</td><td>Non-Autoregressive Machine Translation with Latent Alignments</td><td>0.618</td></tr><tr class="has-abs" data-abs="We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu."><td>50</td><td>Decoding with Large-Scale Neural Language Models Improves Translation</td><td>0.618</td></tr><tr class="has-abs" data-abs="The recent demonstration of the power of huge language models such as GPT-2 to memorise the answers to factoid questions raises questions about the extent to which knowledge is being embedded directly within these large models. This short paper describes an architecture through which much smaller models can also answer such questions -by making use of 'raw' external knowledge. The contribution of this work is that the methods presented here rely on unsupervised learning techniques, complementing the unsupervised training of the Language Model. The goal of this line of research is to be able to add knowledge explicitly, without extensive training."><td>51</td><td>Unsupervised Natural Question Answering with a Small Model</td><td>0.618</td></tr><tr class="has-abs" data-abs="We present RACAI's Entry for the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection. The system is based on an attention-free encoder-decoder neural architecture with a bidirectional LSTM for encoding the input sequence and a unidirectional LSTM for decoding and producing the output. Instead of directly applying a sequence-to-sequence model at character-level we use a dynamic algorithm to align the input and output sequences. Based on these alignments we produce a series of special symbols which are similar to those of a finite-statetransducer (FST)."><td>52</td><td>Attention-free encoder decoder for morphological processing</td><td>0.619</td></tr><tr class="has-abs" data-abs="We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that &quot;mix&quot; input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the &quot;efficient Transformers&quot; on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts. 1 . 2020. Masked language modeling for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555."><td>53</td><td>FNet: Mixing Tokens with Fourier Transforms</td><td>0.619</td></tr><tr class="has-abs" data-abs="The Softmax function is used in the final layer of nearly all existing sequence-tosequence models for language generation. However, it is usually the slowest layer to compute which limits the vocabulary size to a subset of most frequent types; and it has a large memory footprint. We propose a general technique for replacing the softmax layer with a continuous embedding layer. Our primary innovations are a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word embeddings, instead of a multinomial distribution over the vocabulary obtained via softmax. We evaluate this new class of sequence-to-sequence models with continuous outputs on the task of neural machine translation. We show that our models train up to 2.5x faster than the state-of-the-art models while achieving comparable translation quality. These models are capable of handling very large vocabularies without compromising on translation quality or speed. They also produce more meaningful errors than the softmax-based models, as these errors typically lie in a subspace of the vector space of the reference translations 1 ."><td>54</td><td>VON MISES-FISHER LOSS FOR TRAINING SEQUENCE TO SEQUENCE MODELS WITH CONTINUOUS OUTPUTS</td><td>0.619</td></tr><tr class="has-abs" data-abs="Autoregressive Transformers are strong language models but incur O(T ) complexity during per-token generation due to the selfattention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative -decaying fast weights -that runs fast on GPU, outperforms prior methods, and retains 99% of attention's performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes."><td>55</td><td>Fine-Tuning Pre-trained Transformers into Decaying Fast Weights</td><td>0.619</td></tr><tr class="has-abs" data-abs="Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget."><td>56</td><td>Efficient Transformers with Dynamic Token Pooling</td><td>0.619</td></tr><tr class="has-abs" data-abs="Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks by up to 1.88 BLEU gain, while maintaining the inference latency comparable to other fully NAT models. 1 * Corresponding Author 1 We released our code at:https://github.com/ zhanjiaao/NAT_DePA."><td>57</td><td>DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder</td><td>0.619</td></tr><tr class="has-abs" data-abs="Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase."><td>58</td><td>CODEBPE: INVESTIGATING SUBTOKENIZATION OPTIONS FOR LARGE LANGUAGE MODEL PRETRAINING ON SOURCE CODE</td><td>0.620</td></tr><tr class="has-abs" data-abs="Subword tokenization is a commonly used input pre-processing step in most recent NLP models. However, it limits the models' ability to leverage end-to-end task learning. Its frequency-based vocabulary creation compromises tokenization in low-resource languages, leading models to produce suboptimal representations. Additionally, the dependency on a fixed vocabulary limits the subword models' adaptability across languages and domains. In this work, we propose a vocabulary-free neural tokenizer by distilling segmentation information from heuristicbased subword tokenization. We pre-train our character-based tokenizer by processing unique words from multilingual corpus, thereby extensively increasing word diversity across languages. Unlike the predefined and fixed vocabularies in subword methods, our tokenizer allows end-to-end task learning, resulting in optimal task-specific tokenization. The experimental results show that replacing the subword tokenizer with our neural tokenizer consistently improves performance on multilingual (NLI) and code-switching (sentiment analysis) tasks, with larger gains in low-resource languages. Additionally, our neural tokenizer exhibits a robust performance on downstream tasks when adversarial noise is present (typos and misspelling), further increasing the initial improvements over statistical subword tokenizers."><td>59</td><td>A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning</td><td>0.620</td></tr><tr class="has-abs" data-abs="Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.Published as a conference paper at ICLR 2020 perturbations of the ground truth would be efficient but hardly useful for generation purposes, when at test time the model needs to generate from scratch."><td>60</td><td>RESIDUAL ENERGY-BASED MODELS FOR TEXT GENERATION</td><td>0.620</td></tr><tr class="has-abs" data-abs="Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning. method for stochastic optimization. arXiv preprint arXiv:1412.6980. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank."><td>61</td><td>HyperMixer: An MLP-based Low Cost Alternative to Transformers</td><td>0.621</td></tr><tr class="has-abs" data-abs="We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unknown or rare words and to also capture morphological information. We show their influence in the task of machine translation using continuous space language models based on restricted Boltzmann machines. We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German. Using our new approach a gain in BLEU score by up to 0.4 points can be achieved."><td>62</td><td>Letter N-Gram-based Input Encoding for Continuous Space Language Models</td><td>0.622</td></tr><tr class="has-abs" data-abs="It has long been established that predictive models can be transformed into lossless compressors and vice versa.Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models.Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors.In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models.We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning.For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model."><td>63</td><td>Language Modeling Is Compression</td><td>0.622</td></tr><tr class="has-abs" data-abs="We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage. * equal contribution † Work done as a member of the Google AI Residency Program. 1 Output embedding is sometimes referred to as &quot;output weights&quot;, i.e., the weight matrix in the output projection in a language model. 2  We focus on encoder-only models, and do not consider encoder-decoder models like T5 (Raffel et al.,  2020)  where none of the embedding matrices are discarded after pre-training. Output embeddings may also be"><td>64</td><td>RETHINKING EMBEDDING COUPLING IN PRE-TRAINED LANGUAGE MODELS</td><td>0.622</td></tr><tr class="has-abs" data-abs="Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks by up to 1.88 BLEU gain, while maintaining the inference latency comparable to other fully NAT models."><td>65</td><td>DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder</td><td>0.623</td></tr><tr class="has-abs" data-abs="Contextual embedding-based language models trained on large data sets, such as BERT and RoBERTa, provide strong performance across a wide range of tasks and are ubiquitous in modern NLP. It has been observed that fine-tuning these models on tasks involving data from domains different from that on which they were pretrained can lead to suboptimal performance. Recent work has explored approaches to adapt pretrained language models to new domains by incorporating additional pretraining using domain-specific corpora and task data. We propose an alternative approach for transferring pretrained language models to new domains by adapting their tokenizers. We show that domain-specific subword sequences can be efficiently determined directly from divergences in the conditional token distributions of the base and domain-specific corpora. In datasets from four disparate domains, we find adaptive tokenization on a pretrained RoBERTa model provides &gt;97% of the performance benefits of domain specific pretraining. Our approach produces smaller models and less training and inference time than other approaches using tokenizer augmentation. While adaptive tokenization incurs a 6% increase in model parameters in our experimentation, due to the introduction of 10k new domain-specific tokens, our approach, using 64 vCPUs, is 72x faster than further pretraining the language model on domain-specific corpora on 8 TPUs."><td>66</td><td>Efficient Domain Adaptation of Language Models via Adaptive Tokenization</td><td>0.623</td></tr><tr class="has-abs" data-abs="The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary.In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection.We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits.The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary.Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations.Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps.We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training.Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training. 1"><td>67</td><td>(no title)</td><td>0.623</td></tr><tr class="has-abs" data-abs="TE VISUAL TOKENIZATION
29 Sep 20234C8D833F4622C6A583127C3A667E25A5arXiv:2309.04669v2[cs.CV]
Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data.However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM.Such an inequitable treatment of vision and language heavily constrains the model's potential.In this paper, we break through this limitation by representing both vision and language in a unified form.Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read.The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image.Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm.This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks.Our code and models will be available at https:"><td>68</td><td>UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION</td><td>0.623</td></tr><tr class="has-abs" data-abs="Sequence-to-sequence models rely on a fixed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6%. * Work done at Google Brain."><td>69</td><td>LATENT SEQUENCE DECOMPOSITIONS</td><td>0.624</td></tr><tr class="has-abs" data-abs="Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for downstream metrics can better optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches."><td>70</td><td>EEL: Efficiently Encoding Lattices for Reranking</td><td>0.624</td></tr><tr class="has-abs" data-abs="(no abstract)"><td>71</td><td>Non-Autoregressive Models for Fast Sequence Generation</td><td>0.624</td></tr><tr class="has-abs" data-abs="Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficient. Our binary model, while less accurate, achieves a highly nontrivial score of 35.6. For machine translation, we achieved BLEU scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full precision mBART model score of 26.8. We also compare our approach in the 8-bit activation setting, where our ternary and even binary weight models can match or outperform the best existing 8-bit weight models in the literature. Our code and models are available at: https://github.com/facebookresearch/ Ternary_Binary_Transformer."><td>72</td><td>Binary and Ternary Natural Language Generation</td><td>0.624</td></tr><tr class="has-abs" data-abs="Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. We provide a simple yet rigorous explanation for this behaviour by introducing the concept of an optimal representation space, in which semantically close symbols are mapped to representations that are close under a similarity measure induced by the model's objective function. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks. Workshop track -ICLR 2018   encoder and decoders, and FastSent (Hill et al., 2016), which replaces the RNNs with simpler bagof-words (BOW) versions.Models trained in an unsupervised manner on large text corpora are usually applied to supervised transfer tasks, where the representation for a sentence forms the input to a supervised classification problem, or to unsupervised similarity tasks, where the similarity (typically taken to be the cosine similarity) of two inputs is compared with corresponding human judgements of semantic similarity in order to inform some downstream process, such as information retrieval."><td>73</td><td>Workshop track -ICLR 2018 DECODING DECODERS: FINDING OPTIMAL REPRESENTATION SPACES FOR UNSUPERVISED SIMILARITY TASKS</td><td>0.625</td></tr><tr class="has-abs" data-abs="We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders."><td>74</td><td>Online Segment to Segment Neural Transduction</td><td>0.625</td></tr><tr class="has-abs" data-abs="In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model sacrifices its transfer performance due to the lack of conditional dependence between output tokens. To this end, we investigate three techniques, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding."><td>75</td><td>Exploring Non-Autoregressive Text Style Transfer</td><td>0.625</td></tr><tr class="has-abs" data-abs="Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting-forcing models to generate to the correct sequence length at test time-to compare the lengthextrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.Gary Marcus. 2018. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631.William Merrill. 2020. On the linguistic capacity of real-time counter automata. arXiv preprint arXiv:2004.06866."><td>76</td><td>The EOS Decision and Length Extrapolation</td><td>0.625</td></tr><tr class="has-abs" data-abs="We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points."><td>77</td><td>Large Scale Decipherment for Out-of-Domain Machine Translation</td><td>0.626</td></tr><tr class="has-abs" data-abs="Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies.In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT).We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts.Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency.We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets."><td>78</td><td>Headless Language Models: Learning without Predicting with Contrastive Weight Tying</td><td>0.626</td></tr><tr class="has-abs" data-abs="Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale."><td>79</td><td>Better Language Model with Hypernym Class Prediction</td><td>0.626</td></tr><tr class="has-abs" data-abs="This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language.Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens.Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs.We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval).Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks.We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect.This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.Code is available at https://github.com/ziqipang/LM4VisualEncoding."><td>80</td><td>FROZEN TRANSFORMERS IN LANGUAGE MODELS ARE EFFECTIVE VISUAL ENCODER LAYERS</td><td>0.627</td></tr><tr class="has-abs" data-abs="Since the introduction of the transformer model byVaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark. 1 1 Code &amp; models: https://github.com/ofirpress/attention_with_linear_biases 2Figure 7in the appendix plots training speed, in words per second, against L."><td>81</td><td>Published as a conference paper at ICLR 2022 TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</td><td>0.627</td></tr><tr class="has-abs" data-abs="Language models (LM) have played crucial roles in automatic speech recognition (ASR) to enhance end-to-end (E2E) ASR systems' performance. There are two categories of approaches: finding better ways to integrate LMs into ASR systems and adapting on LMs to the task domain. This article will start with a reflection of interpolationbased integration methods of E2E ASR's scores and LM's scores. Then we will focus on LM augmentation approaches based on the noisy channel model, which is intrigued by insights obtained from the above reflection. The experiments show that we can enhance an ASR E2E model based on encoder-decoder architecture by pre-training the decoder with text data. This implies the decoder of an E2E model can be treated as an LM and reveals the possibility of enhancing the E2E model without an external LM. Based on those ideas, we proposed the implicit language model canceling method and then did more discussion about the decoder part of an E2E ASR model. The experimental results on the TED-LIUM2 dataset show that our approach achieves a 3.4% relative WER reduction compared with the baseline system, and more analytic experiments provide concrete experimental supports for our assumption."><td>82</td><td>Can We Train a Language Model Inside an End-to-End ASR Model? - Investigating Effective Implicit Language Modeling</td><td>0.627</td></tr><tr class="has-abs" data-abs="This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE(Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations."><td>83</td><td>Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</td><td>0.627</td></tr><tr class="has-abs" data-abs="(no abstract)"><td>84</td><td>Masked language models directly encode linguistic uncertainty</td><td>0.627</td></tr><tr class="has-abs" data-abs="In Neural Machine Translation, using wordlevel tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies.We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information.Our experiments show that this approach maintains the translation quality with respect to the normal Transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies."><td>85</td><td>Combining Subword Representations into Word-level Representations in the Transformer Architecture</td><td>0.628</td></tr><tr class="has-abs" data-abs="We introduce a language modeling architecture which operates over sequences of images, or over multimodal sequences of images with associated labels. We use this architecture alongside other embedding models to investigate a category of signs called complex graphemes (CGs) in the undeciphered proto-Elamite script. We argue that CGs have meanings which are at least partly compositional, and we discover novel rules governing the construction of CGs. We find that a language model over sign images produces more interpretable results than a model over text or over sign images and text, which suggests that the names given to signs may be obscuring signals in the corpus. Our results reveal previously unknown regularities in proto-Elamite sign use that can inform future decipherment efforts, and our image-aware language model provides a novel way to abstract away from biases introduced by human annotators."><td>86</td><td>Compositionality of Complex Graphemes in the Undeciphered Proto-Elamite Script using Image and Text Embedding Models</td><td>0.628</td></tr><tr class="has-abs" data-abs="Text Style Transfer (TST) involves transforming a source sentence with a given style label to an output with another target style meanwhile preserving content and fluency.We look at a fill-in-the-blanks approach (also referred to as prototype editing), where the source sentence is stripped off all style-containing words and filled in with suitable words.This closely resembles a Masked Language Model (MLM) objective, with the added initial step of masking only relevant style words rather than BERT's random masking.We show this simple MLM, trained to reconstruct style-masked sentences back into their original style, can even transfer style by making this MLM &quot;Style-Aware&quot;.This simply involves appending the source sentence with a target style special token.The Style-Aware MLM (SA-MLM) now also accounts for the direction of style transfer and enables style transfer by simply manipulating these special tokens.To learn this n-word to n-word style reconstruction task, we use a single transformer encoder block with 8 heads, 2 layers and no auto-regressive decoder, making it non-generational.We empirically show that this lightweight encoder trained on a simple reconstruction task compares with elaborately engineered state-of-the-art TST models for even complex styles like Discourse or flow of logic, i.e.Contradiction to Entailment and vice-versa.Additionally, we introduce a more accurate attention-based style-masking step and a novel &quot;attention-surplus&quot; method to determine the position of masks from any arbitrary attribution model in O(1) time.Finally, we show that the SA-MLM arises naturally by considering a probabilistic framework for style transfer.*"><td>87</td><td>On Text Style Transfer via Style-Aware Masked Language Models</td><td>0.628</td></tr><tr class="has-abs" data-abs="This paper investigates data-driven segmentation using Re-Pair or Byte Pair Encoding-techniques. In contrast to previous work which has primarily been focused on subword units for machine translation, we are interested in the general properties of such segments above the word level. We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure."><td>88</td><td>R-grams: Unsupervised Learning of Semantic Units</td><td>0.628</td></tr><tr class="has-abs" data-abs="This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations (Clark et al., 2022; Jaegle et al., 2022;Tay et al., 2021), and a variety of different pretraining objectives on a suite of evaluation tasks in order to find the optimal way to build and train character-level BERT-like models. We find that the best recipe combines the Charformer and CANINE model architectures, and follows the CANINE training procedure. This model exceeds the performance of a tokenbased model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a learnt tokeniser during pretraining, and final model performance is highly dependent on tokeniser quality. We believe our results demonstrate the readiness of character-level models for multilingual language representation, and encourage NLP practitioners to try them for their needs."><td>89</td><td>What is the best recipe for character-level encoder-only modelling?</td><td>0.628</td></tr><tr class="has-abs" data-abs="Learning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison. In this paper, we present a neural encoder-decoder model for learning canonical morphological segmentation. Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments. We obtain up to 4% improvement over a strong character-level encoderdecoder baseline for three languages. Our model outperforms the previous state-ofthe-art for two languages, while eliminating the need for external resources such as large dictionaries. Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.With the spread of natural language processing to a wider range of languages, learning internal word structure becomes increasingly important for developing practical applications. Analysis of internal word structure, usually termed morphological segmentation, has been shown helpful in tasks such as machine translation(Dyer et al., 2008;Narasimhan et al., 2014), speech processing (Creutz et al., 2007 and parsing(Seeker and Ç etinoglu, 2015). Additionally, there is a growing interest in automatic learning of morphological segmentation for the purpose of theoretical lan-2 Cf. Leipzig Glossing Rules at https://www.eva. mpg.de/lingua/resources/glossing-rules. php 3  The examples are adapted from(Eifring and Theil, 2005)"><td>90</td><td>Neural Sequence-to-sequence Learning of Internal Word Structure</td><td>0.629</td></tr><tr class="has-abs" data-abs="For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance."><td>91</td><td>DEFINE: DEEP FACTORIZED INPUT TOKEN EMBEDDINGS FOR NEURAL SEQUENCE MODELING</td><td>0.629</td></tr><tr class="has-abs" data-abs="We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers.In the attention of Transformers, keys handle information selection while values specify what information is conveyed. Our model, dubbed QKVAE, uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values.We run experiments on latent representations and experiments on syntax/semantics transfer which show that QKVAE displays clear signs of disentangled syntax and semantics. We also show that our model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer. The code for our experiments is publicly available 1 ."><td>92</td><td>Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs</td><td>0.629</td></tr><tr class="has-abs" data-abs="Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4."><td>93</td><td>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</td><td>0.630</td></tr><tr class="has-abs" data-abs="Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smartprompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (&gt; 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space. 1"><td>94</td><td>Extracting Latent Steering Vectors from Pretrained Language Models</td><td>0.630</td></tr><tr class="has-abs" data-abs="Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance."><td>95</td><td>What Works and Doesn't Work, A Deep Decoder for Neural Machine Translation</td><td>0.630</td></tr><tr class="has-abs" data-abs="Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose iterative realignment, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14× decoding speedup; on LibriSpeech, we reach an LM-free testother WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/ amazon-research/align-refine."><td>96</td><td>Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment</td><td>0.631</td></tr><tr class="has-abs" data-abs="Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable monotonic interpolation scheme to predict the duration of each input token. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision. 1 * Equal contribution. First author determined by coin toss. 1 Listen to our model reading this abstract at: https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech"><td>97</td><td>End-to-End Adversarial Text-to-Speech</td><td>0.631</td></tr><tr class="has-abs" data-abs="This work introduces the Compressive Performer, a hybrid Transformer variant based on two existing model architectures: the Performer, which reduces the memory requirement and processing time of the Transformer to linear complexity, and the Compressive Transformer, which retains contextual dependencies over a long range by compressing old activations instead of discarding them. Experiments in language modelling at the character level, the word level, and the sub-word level demonstrate that the Compressive Performer shows improved perplexity scores on the enwik-8 dataset, compared to its base models. This work also compares convolutional compression with autoencoder compression, determining that both show similar perplexity scores."><td>98</td><td>Compressive Performers in Language Modelling</td><td>0.631</td></tr><tr class="has-abs" data-abs="Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs' internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks. 1"><td>99</td><td>Condenser: a Pre-training Architecture for Dense Retrieval</td><td>0.632</td></tr><tr class="has-abs" data-abs="Sequence-to-sequence models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypothesize that these encoder outputs can be compressed to shorten the sequence delivered for decoding. We take Transformer as the testbed and introduce a layer of stochastic gates in-between the encoder and the decoder. The gates are regularized using the expected value of the sparsityinducing L 0 penalty, resulting in completely masking-out a subset of encoder outputs. In other words, via joint training, the L 0 DROP layer forces Transformer to route information through a subset of its encoder states. We investigate the effects of this sparsification on two machine translation and two summarization tasks. Experiments show that, depending on the task, around 40-70% of source encodings can be pruned without significantly compromising quality. The decrease of the output length endows L 0 DROP with the potential of improving decoding efficiency, where it yields a speedup of up to 1.65× on document summarization tasks against the standard Transformer. We analyze the L 0 DROP behaviour and observe that it exhibits systematic preferences for pruning certain word types, e.g., function words and punctuation get pruned most. Inspired by these observations, we explore the feasibility of specifying rule-based patterns that mask out encoder outputs based on information such as part-of-speech tags, word frequency and word position. 1"><td>100</td><td>On Sparsifying Encoder Outputs in Sequence-to-Sequence Models</td><td>0.632</td></tr></tbody></table>
  </div>
  <div class="card">
    <h2>After&nbsp;(augmented)</h2>
    <table id="afterTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody></tbody></table>
  </div>
</section>

<!-- METRICS ------------------------------------------------------------->
<section id="metricsCard" class="card" style="">
  <h2>Metrics</h2>
  <div id="recallLine" class="small">Recall </div>

  <div class="tabs">
    <button id="tab1" class="">Δ recall</button>
    <button id="tab2" class="active">Rank maps</button>
  </div>

  <div id="body1" class="tab-body active">
    <p id="deltaLine" class="small"></p>
  </div>
  <div id="body2" class="tab-body active">
    <h3 class="small" style="margin-top:0;">ranks_before</h3>
    <pre id="rb" class="small">{
  "254877406": 35
}</pre>
    <h3 class="small">ranks_after</h3>
    <pre id="ra" class="small"></pre>
  </div>
</section>




```
</body></html>