<!doctype html>
<html lang="en"><head></head><body>```html



<meta charset="utf-8">
<title>Prompt‑HyDE playground</title>
<style>
  body{font-family:system-ui,sans-serif;margin:0;padding:1rem 2rem;}
  h2{margin-top:2rem;}
  label{display:block;margin-top:.75rem;font-weight:600;}
  select,input[type=text],input[type=number],textarea{width:100%;padding:.4rem;}
  textarea{height:120px;font-family:monospace;}
  button{margin-top:1rem;padding:.5rem 1rem;cursor:pointer;}
  table{border-collapse:collapse;width:100%;margin-top:.5rem;}
  th,td{border:1px solid #ccc;padding:.25rem .5rem;text-align:left;vertical-align:top;}
  em.rev{font-style:italic;color:#9146ff;}
  strong.rel{color:#0b63ff;font-weight:700;}
  .flex{display:flex;gap:2rem;flex-wrap:wrap;}
  .card{flex:1 1 320px;border:1px solid #ddd;padding:1rem;}
  .small{font-size:.85rem;color:#555;margin:.25rem 0;}

  /* tooltip for abstracts */
  .has-abs{position:relative;}
  .has-abs:hover::after{
    content:attr(data-abs);
    position:absolute;left:0;top:100%;z-index:999;
    max-width:420px;white-space:pre-wrap;font-size:.8rem;
    background:#333;color:#fff;padding:.5rem;border-radius:.25rem;
    box-shadow:0 2px 6px rgba(0,0,0,.35);
  }

  /* tab styling */
  .tabs{display:flex;gap:.5rem;margin-bottom:.5rem;}
  .tabs button{padding:.25rem .75rem;border:1px solid #aaa;background:#eee;}
  .tabs button.active{background:#fff;border-bottom:none;font-weight:600;}
  .tab-body{display:none;}
  .tab-body.active{display:block;}
</style>


<h1>Prompt‑HyDE UI (vanilla&nbsp;JS)</h1>

<!-- CONFIG -------------------------------------------------------------->
<section class="card">
  <h2>Server configuration</h2>
  <pre id="cfg" class="small">{
  "embed_model": "grit",
  "index_path": "../0721_litsearch_example/faiss/litsearch.index",
  "backend": "faiss",
  "query_dataset": "../0721_litsearch_example/query_with_score.parquet",
  "corpus_dataset": "../0721_litsearch_example/corpus_clean_dedup.parquet",
  "prompt_dir": "prompts",
  "extractor_dir": "extractors",
  "annotation_dir": "annotations",
  "ui_config": "configs/ui_config.json",
  "host": "0.0.0.0",
  "port": 8200,
  "id_field": "corpusid",
  "relevant_documents_field": "corpusids"
}</pre>
</section>

<section class="flex">
  <!-- DATA CARD -->
  <div class="card" style="max-width:440px;">
    <h2>Data</h2>

    <label>Query</label>
    <select id="querySel"><option value="0">0: [inline_acl] sp=0 qual=2 gr=0 — Are there any research papers on methods to compre…</option><option value="3">3: [inline_acl] sp=1 qual=2 gr=0 — Are there any tools or studies that have focused o…</option><option value="4">4: [inline_acl] sp=1 qual=2 gr=0 — Are there papers that propose contextualized calib…</option><option value="10">10: [inline_acl] sp=1 qual=2 gr=0 — Can you point me to a work that uses diagnostic to…</option><option value="11">11: [inline_acl] sp=0 qual=1 gr=0 — Can you point me to studies discussing methods for…</option><option value="17">17: [inline_acl] sp=1 qual=2 gr=0 — Can you recommend a paper that uses an NLI model f…</option><option value="24">24: [inline_acl] sp=0 qual=1 gr=0 — Can you suggest recent studies that have integrate…</option><option value="30">30: [inline_acl] sp=0 qual=1 gr=0 — Could you point me to studies that have investigat…</option><option value="35">35: [inline_acl] sp=1 qual=2 gr=0 — Could you suggest a paper that introduces an appro…</option><option value="39">39: [inline_acl] sp=1 qual=2 gr=0 — I am looking for research that has explored topic …</option><option value="44">44: [inline_acl] sp=1 qual=2 gr=0 — I'm exploring research that utilizes large dataset…</option><option value="49">49: [inline_acl] sp=1 qual=2 gr=0 — I'm looking for innovative approaches to data anno…</option><option value="55">55: [inline_acl] sp=1 qual=2 gr=0 — In discourse parsing literature, which works have …</option><option value="56">56: [inline_acl] sp=1 qual=2 gr=0 — In researching metrics for human-interaction with …</option><option value="58">58: [inline_acl] sp=1 qual=2 gr=0 — In the context of Named Entity Recognition tasks a…</option><option value="59">59: [inline_acl] sp=0 qual=1 gr=0 — In the context of machine translation, can you poi…</option><option value="61">61: [inline_acl] sp=1 qual=2 gr=0 — In the context of simultaneous machine translation…</option><option value="62">62: [inline_acl] sp=1 qual=2 gr=0 — In the field of reinforcement learning models for …</option><option value="63">63: [inline_acl] sp=1 qual=2 gr=0 — What approaches have been used to address the limi…</option><option value="71">71: [inline_acl] sp=1 qual=2 gr=0 — What are the recent developments in evaluating the…</option><option value="75">75: [inline_acl] sp=0 qual=1 gr=0 — What research could I reference to understand the …</option><option value="84">84: [inline_acl] sp=1 qual=2 gr=0 — Where can I find a multilingual corpus that includ…</option><option value="109">109: [inline_nonacl] sp=1 qual=2 gr=0 — Can you recommend research that uses an LLM to gen…</option><option value="110">110: [inline_nonacl] sp=0 qual=1 gr=0 — Can you show me a paper that built a large structu…</option><option value="111">111: [inline_nonacl] sp=1 qual=2 gr=0 — Can you suggest research that deals with the multi…</option><option value="119">119: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that examines how inco…</option><option value="123">123: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that explores how lang…</option><option value="124">124: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores mitigati…</option><option value="126">126: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores the diff…</option><option value="128">128: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that initializes embed…</option><option value="132">132: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that investigates enha…</option><option value="135">135: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="136">136: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates how …</option><option value="137">137: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="142">142: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates the …</option><option value="143">143: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates the …</option><option value="145">145: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that uses feedback-dri…</option><option value="149">149: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research papers that investiga…</option><option value="155">155: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that evaluates the pe…</option><option value="157">157: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that examines how dec…</option><option value="163">163: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that explores how the…</option><option value="168">168: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates app…</option><option value="169">169: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates gen…</option><option value="176">176: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates the…</option><option value="179">179: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research that proposed enhanci…</option><option value="194">194: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest a study that explores data annot…</option><option value="195">195: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that explores employing …</option><option value="197">197: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest a study that explores the idea o…</option><option value="199">199: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that investigates the in…</option><option value="205">205: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest papers that tackle conversationa…</option><option value="208">208: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that concentrates on pi…</option><option value="209">209: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines a system …</option><option value="213">213: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that examines how stran…</option><option value="221">221: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines the effec…</option><option value="224">224: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores generatin…</option><option value="226">226: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores the idea …</option><option value="228">228: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that includes an online…</option><option value="231">231: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that investigates apply…</option><option value="238">238: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that investigates how c…</option><option value="244">244: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest research that investigates the u…</option><option value="251">251: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest studies focused on emotion-class…</option><option value="252">252: [inline_nonacl] sp=1 qual=2 gr=0 — Has any research explored using other off-the-shel…</option><option value="260">260: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research efforts been made to gather dial…</option><option value="261">261: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers been published on models …</option><option value="265">265: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers examined whether using la…</option><option value="269">269: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research papers investigated the creation…</option><option value="272">272: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers tried to create conversat…</option><option value="278">278: [inline_nonacl] sp=1 qual=2 gr=0 — I know about prompt tuning, but have any works tri…</option><option value="297">297: [inline_nonacl] sp=1 qual=2 gr=0 — What are some scholarly articles that explore the …</option><option value="306">306: [inline_nonacl] sp=0 qual=1 gr=0 — What papers discuss the effect of false negatives …</option><option value="307">307: [inline_nonacl] sp=1 qual=2 gr=0 — What papers explore replacing schema linking with …</option><option value="311">311: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists comparing adapter-based tunin…</option><option value="312">312: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists on employing generative model…</option><option value="325">325: [inline_nonacl] sp=0 qual=1 gr=0 — What research is available on acquiring sentence e…</option><option value="326">326: [inline_nonacl] sp=0 qual=2 gr=0 — What research is available on hybrid approaches th…</option><option value="331">331: [inline_nonacl] sp=0 qual=1 gr=0 — What techniques and frameworks have been suggested…</option><option value="346">346: [inline_nonacl] sp=1 qual=1 gr=0 — Which work shows that only emplying instance-level…</option><option value="347">347: [inline_nonacl] sp=1 qual=2 gr=0 — Which work shows that reducing the number of train…</option><option value="353">353: [manual_acl] sp=1 qual=2 gr=0 — Are there any papers that build dense retrievers w…</option><option value="360">360: [manual_acl] sp=1 qual=2 gr=0 — If one would like to train (or evaluate) a helpful…</option><option value="364">364: [manual_acl] sp=1 qual=2 gr=0 — Is there a decoder-only language model that does n…</option><option value="366">366: [manual_acl] sp=1 qual=2 gr=0 — Is there a method for measuring the critical error…</option><option value="369">369: [manual_acl] sp=0 qual=1 gr=0 — Is there a paper exploring the curse of multilingu…</option><option value="372">372: [manual_acl] sp=0 qual=2 gr=0 — Is there a paper that links exposure bias to disti…</option><option value="378">378: [manual_acl] sp=1 qual=2 gr=0 — Is there a paper that uses similarity scores to ch…</option><option value="382">382: [manual_acl] sp=1 qual=2 gr=0 — Is there a tool that can automatically segment spe…</option><option value="389">389: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that aligns speech and text emb…</option><option value="395">395: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that constructs augmented train…</option><option value="404">404: [manual_acl] sp=0 qual=2 gr=0 — Is there any paper that proposes a set of criteria…</option><option value="411">411: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that utilizes Gaussian processe…</option><option value="414">414: [manual_acl] sp=1 qual=1 gr=0 — Is there any research paper that can extract attri…</option><option value="417">417: [manual_acl] sp=1 qual=2 gr=0 — Is there any works that explores how to achieve ba…</option><option value="423">423: [manual_acl] sp=0 qual=2 gr=0 — What are some methods for solving the class-increm…</option><option value="426">426: [manual_acl] sp=0 qual=1 gr=0 — What is the performance of large language models i…</option><option value="433">433: [manual_acl] sp=1 qual=2 gr=0 — Which is the first multimodal model combining text…</option><option value="451">451: [manual_acl] sp=1 qual=2 gr=0 — Which paper first constructed a structured knowled…</option><option value="458">458: [manual_acl] sp=1 qual=1 gr=0 — Which paper first proposed shared adapter module a…</option><option value="467">467: [manual_acl] sp=0 qual=1 gr=0 — Which paper found that mutual learning benefits mu…</option><option value="473">473: [manual_acl] sp=1 qual=1 gr=0 — Which paper is among the earliest to train on exte…</option><option value="475">475: [manual_acl] sp=0 qual=1 gr=0 — Which paper makes sure that the questions used in …</option><option value="498">498: [manual_acl] sp=1 qual=2 gr=0 — Which research paper leverages event structure inf…</option><option value="502">502: [manual_acl] sp=1 qual=2 gr=0 — Which work discusses an analysis of source and tar…</option><option value="510">510: [manual_iclr] sp=1 qual=2 gr=0 — Are there any papers that use a world model for pl…</option><option value="522">522: [manual_iclr] sp=1 qual=2 gr=0 — Is there a parameter-efficient fine-tuning method …</option><option value="550">550: [manual_iclr] sp=1 qual=1 gr=0 — What paper first showed that you can score the cod…</option><option value="557">557: [manual_iclr] sp=0 qual=1 gr=0 — What paper proposes breaking down programming prob…</option><option value="560">560: [manual_iclr] sp=1 qual=1 gr=0 — What research first proposed a new kind of cascade…</option><option value="564">564: [manual_iclr] sp=1 qual=2 gr=0 — Which backdoor paper first used the CLIP to suppre…</option><option value="566">566: [manual_iclr] sp=1 qual=1 gr=0 — Which is one of the first papers to highlight and …</option><option value="595">595: [manual_iclr] sp=0 qual=2 gr=0 — What paper provides generalization bounds for self…</option><option value="18">18: [inline_acl] sp=0 qual=2 gr=0.25 — Can you recommend some literature that focuses on …</option><option value="2">2: [inline_acl] sp=0 qual=2 gr=0.5 — Are there any studies that explore post-hoc techni…</option><option value="12">12: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me to studies that explore the impac…</option><option value="13">13: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me towards research on contrastive l…</option><option value="53">53: [inline_acl] sp=1 qual=2 gr=0.5 — I'm researching on the efficacy of recurrent netwo…</option><option value="74">74: [inline_acl] sp=1 qual=2 gr=0.5 — What prior works suggested that exposure bias coul…</option><option value="92">92: [inline_acl] sp=0 qual=1 gr=0.5 — Where might I find research on the evaluation of c…</option><option value="102">102: [inline_nonacl] sp=1 qual=2 gr=0.5 — Are there any studies investigating example-based …</option><option value="113">113: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you direct me to studies investigating the e…</option><option value="150">150: [inline_nonacl] sp=0 qual=1 gr=0.5 — Could you recommend research that analyses prompt …</option><option value="156">156: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you recommend research that examines how an …</option><option value="235">235: [inline_nonacl] sp=1 qual=1 gr=0.5 — Could you suggest research that investigates enhan…</option><option value="257">257: [inline_nonacl] sp=0 qual=2 gr=0.5 — Have any new metrics been developed to assess the …</option><option value="328">328: [inline_nonacl] sp=0 qual=2 gr=0.5 — What research should I consult regarding the appli…</option><option value="334">334: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques exist to enhance the few-shot fine…</option><option value="335">335: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques have been investigated to enhance …</option><option value="96">96: [inline_acl] sp=0 qual=2 gr=0.6 — Which studies should I look into that have explore…</option><option value="8">8: [inline_acl] sp=0 qual=1 gr=0.6666666666666666 — Can you list some publications that discuss the ev…</option><option value="23">23: [inline_acl] sp=0 qual=2 gr=0.6666666666666666 — Can you suggest literature on enhanced semantic pa…</option><option value="1">1: [inline_acl] sp=1 qual=2 gr=1 — Are there any resources available for translating …</option><option value="5">5: [inline_acl] sp=1 qual=2 gr=1 — Are there studies that combine convolutional and r…</option><option value="6">6: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to research that explores method…</option><option value="7">7: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to studies that explore techniqu…</option><option value="9">9: [inline_acl] sp=0 qual=2 gr=1 — Can you point me to a paper that discussed transfo…</option><option value="14">14: [inline_acl] sp=0 qual=1 gr=1 — Can you point to studies or tasks focused on detec…</option><option value="15">15: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a conversational QA dataset wher…</option><option value="16">16: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a foundational paper that provid…</option><option value="19">19: [inline_acl] sp=1 qual=2 gr=1 — Can you refer me to research that adapts the conce…</option><option value="20">20: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest a corpus that contains French ency…</option><option value="21">21: [inline_acl] sp=0 qual=2 gr=1 — Can you suggest any literature that explores the i…</option><option value="22">22: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest literature on a dataset that categ…</option><option value="25">25: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some literature that evaluates the…</option><option value="26">26: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some recent datasets that have bee…</option><option value="27">27: [inline_acl] sp=1 qual=2 gr=1 — Could you direct me towards a study that explores …</option><option value="28">28: [inline_acl] sp=1 qual=2 gr=1 — Could you point me to research on binary classific…</option><option value="29">29: [inline_acl] sp=0 qual=2 gr=1 — Could you point me to studies that discuss the dev…</option><option value="31">31: [inline_acl] sp=1 qual=1 gr=1 — Could you point me toward some large-scale multili…</option><option value="32">32: [inline_acl] sp=1 qual=2 gr=1 — Could you provide me with a reference that discuss…</option><option value="33">33: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend datasets that include SQL anno…</option><option value="34">34: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend studies that provide a baselin…</option><option value="36">36: [inline_acl] sp=0 qual=1 gr=1 — Could you suggest studies that employ novel method…</option><option value="37">37: [inline_acl] sp=1 qual=2 gr=1 — Has there been any recent work or competitions foc…</option><option value="38">38: [inline_acl] sp=0 qual=1 gr=1 — I am exploring state-of-the-art techniques in lang…</option><option value="40">40: [inline_acl] sp=0 qual=2 gr=1 — I am looking to understand more about sequence-to-…</option><option value="41">41: [inline_acl] sp=1 qual=2 gr=1 — I would like to understand the theoretical basis f…</option><option value="42">42: [inline_acl] sp=1 qual=2 gr=1 — I'm conducting research on computational humor and…</option><option value="43">43: [inline_acl] sp=1 qual=1 gr=1 — I'm exploring efficient transformer architectures …</option><option value="45">45: [inline_acl] sp=0 qual=2 gr=1 — I'm exploring ways to enhance question answering s…</option><option value="46">46: [inline_acl] sp=1 qual=2 gr=1 — I'm interested in understanding how perplexity is …</option><option value="47">47: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a comprehensive dataset that has b…</option><option value="48">48: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a paper that discusses improvement…</option><option value="50">50: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into morphological embedding algorithm…</option><option value="51">51: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into the distillation process of langu…</option><option value="52">52: [inline_acl] sp=0 qual=1 gr=1 — I'm researching insertion-based decoding methods f…</option><option value="54">54: [inline_acl] sp=1 qual=1 gr=1 — I'm searching for studies that explore advancement…</option><option value="57">57: [inline_acl] sp=1 qual=2 gr=1 — In the area of argument mining, could you point to…</option><option value="60">60: [inline_acl] sp=1 qual=2 gr=1 — In the context of natural language processing, I a…</option><option value="64">64: [inline_acl] sp=1 qual=1 gr=1 — What are some approaches to generating sports news…</option><option value="65">65: [inline_acl] sp=1 qual=2 gr=1 — What are some good datasets for conversational que…</option><option value="66">66: [inline_acl] sp=1 qual=2 gr=1 — What are some of the key papers to look at for und…</option><option value="67">67: [inline_acl] sp=0 qual=2 gr=1 — What are some recent advancements in training syst…</option><option value="68">68: [inline_acl] sp=1 qual=2 gr=1 — What are some soft-constrained methods proposed in…</option><option value="69">69: [inline_acl] sp=1 qual=2 gr=1 — What are some studies that leverage statistical ma…</option><option value="70">70: [inline_acl] sp=0 qual=2 gr=1 — What are some techniques or tools used in machine …</option><option value="72">72: [inline_acl] sp=1 qual=2 gr=1 — What paper should I look at if I am interested in …</option><option value="73">73: [inline_acl] sp=1 qual=2 gr=1 — What papers should I refer to if I want to explore…</option><option value="76">76: [inline_acl] sp=1 qual=2 gr=1 — What research has been done on annotating user com…</option><option value="77">77: [inline_acl] sp=0 qual=1 gr=1 — What research has been done on improving named ent…</option><option value="78">78: [inline_acl] sp=1 qual=1 gr=1 — What research should I explore to understand metho…</option><option value="79">79: [inline_acl] sp=1 qual=2 gr=1 — When using pretrained transformer models for gener…</option><option value="80">80: [inline_acl] sp=0 qual=1 gr=1 — Where can I find a corpus of CCG annotations for n…</option><option value="81">81: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a detailed discussion on automati…</option><option value="82">82: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a discourse treebank tailored to …</option><option value="83">83: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a large corpus of annotated socia…</option><option value="85">85: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a paper that discusses annotating…</option><option value="86">86: [inline_acl] sp=0 qual=1 gr=1 — Where can I find guidelines on standard practices …</option><option value="87">87: [inline_acl] sp=0 qual=1 gr=1 — Where can I find information on self-attentive par…</option><option value="88">88: [inline_acl] sp=0 qual=1 gr=1 — Where can I find interdisciplinary research that i…</option><option value="89">89: [inline_acl] sp=1 qual=2 gr=1 — Where can I find multilingual datasets used for th…</option><option value="90">90: [inline_acl] sp=1 qual=2 gr=1 — Where can I find research about automatic evaluati…</option><option value="91">91: [inline_acl] sp=0 qual=1 gr=1 — Where might I find a dataset annotated specificall…</option><option value="93">93: [inline_acl] sp=1 qual=2 gr=1 — Which corpora are frequently used in research to b…</option><option value="94">94: [inline_acl] sp=1 qual=2 gr=1 — Which paper specifies the typical configurations u…</option><option value="95">95: [inline_acl] sp=0 qual=1 gr=1 — Which papers should I refer to for learning about …</option><option value="97">97: [inline_acl] sp=1 qual=2 gr=1 — Which work should I explore to understand the tech…</option><option value="98">98: [inline_nonacl] sp=1 qual=1 gr=1 — *Could you suggest a dataset with legally or ethic…</option><option value="99">99: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any papers on training video-language mo…</option><option value="100">100: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any recent papers investigating the use …</option><option value="101">101: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any research papers investigating the im…</option><option value="103">103: [inline_nonacl] sp=1 qual=2 gr=1 — Are there any studies investigating sentiment anal…</option><option value="104">104: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any studies on incorporating external co…</option><option value="105">105: [inline_nonacl] sp=1 qual=2 gr=1 — Are there studies examining how well question answ…</option><option value="106">106: [inline_nonacl] sp=1 qual=1 gr=1 — Are there studies that investigate debiasing langu…</option><option value="107">107: [inline_nonacl] sp=0 qual=1 gr=1 — Can you give me a paper that does self-supervised …</option><option value="108">108: [inline_nonacl] sp=0 qual=1 gr=1 — Can you recommend a dialogue summarization dataset…</option><option value="112">112: [inline_nonacl] sp=1 qual=2 gr=1 — Could you direct me to research that evaluates few…</option><option value="114">114: [inline_nonacl] sp=1 qual=1 gr=1 — Could you point me to research that tackles the is…</option><option value="115">115: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a contemporary research paper …</option><option value="116">116: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a paper that builds a writing …</option><option value="117">117: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that does data-augment…</option><option value="118">118: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that examines how cros…</option><option value="120">120: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that examines the intr…</option><option value="121">121: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores a pre-tr…</option><option value="122">122: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that explores employin…</option><option value="125">125: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that explores strategi…</option><option value="127">127: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores the impr…</option><option value="129">129: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates empl…</option><option value="130">130: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates empl…</option><option value="131">131: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates enha…</option><option value="133">133: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates grap…</option><option value="134">134: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates guid…</option><option value="138">138: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates inco…</option><option value="139">139: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates know…</option><option value="140">140: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates repr…</option><option value="141">141: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates text…</option><option value="144">144: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates the …</option><option value="146">146: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend articles that explore the role…</option><option value="147">147: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research articles that explore…</option><option value="148">148: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research papers that explore a…</option><option value="151">151: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that assesses how wel…</option><option value="152">152: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that assesses how wel…</option><option value="153">153: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that assesses techniq…</option><option value="154">154: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that employs a relaxe…</option><option value="158">158: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how mul…</option><option value="159">159: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how opt…</option><option value="160">160: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that examines how syn…</option><option value="161">161: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the cha…</option><option value="162">162: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the eff…</option><option value="164">164: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that explores identif…</option><option value="165">165: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that has introduced a…</option><option value="166">166: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that improves knowled…</option><option value="167">167: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend research that introduces a met…</option><option value="170">170: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="171">171: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="172">172: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates mer…</option><option value="173">173: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates met…</option><option value="174">174: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates tec…</option><option value="175">175: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="177">177: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="178">178: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates usi…</option><option value="180">180: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend scholarly articles that invest…</option><option value="181">181: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend studies on hierarchical modeli…</option><option value="182">182: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that concentrate on an…</option><option value="183">183: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that investigate fine-…</option><option value="184">184: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend studies that tackle the issue …</option><option value="185">185: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies which explore how to o…</option><option value="186">186: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a dataset containing diverse, in…</option><option value="187">187: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a dataset for question-answering…</option><option value="188">188: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a research article that explores…</option><option value="189">189: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study examining how transforme…</option><option value="190">190: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that evaluates cross-enc…</option><option value="191">191: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that examines how well c…</option><option value="192">192: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a cohesive…</option><option value="193">193: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a compress…</option><option value="196">196: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores improved t…</option><option value="198">198: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores the use of…</option><option value="200">200: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a study that proposes high-param…</option><option value="201">201: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a thorough comparative analysis …</option><option value="202">202: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a triplet-formatted structured d…</option><option value="203">203: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest an article that leverages the sp…</option><option value="204">204: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest datasets that can benchmark LLM …</option><option value="206">206: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research on detecting common err…</option><option value="207">207: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that assesses if langua…</option><option value="210">210: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how coref…</option><option value="211">211: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="212">212: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="214">214: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that examines how the o…</option><option value="215">215: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="216">216: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines how well …</option><option value="217">217: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="218">218: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines the appli…</option><option value="219">219: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines the chall…</option><option value="220">220: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines the diffi…</option><option value="222">222: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores a pre-tra…</option><option value="223">223: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores employing…</option><option value="225">225: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores the drawb…</option><option value="227">227: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores the impro…</option><option value="229">229: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates a clu…</option><option value="230">230: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates acros…</option><option value="232">232: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates aspec…</option><option value="233">233: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates effic…</option><option value="234">234: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates emplo…</option><option value="236">236: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates expan…</option><option value="237">237: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates graph…</option><option value="239">239: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how h…</option><option value="240">240: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how m…</option><option value="241">241: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that investigates how n…</option><option value="242">242: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates how u…</option><option value="243">243: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates impro…</option><option value="245">245: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates train…</option><option value="246">246: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that offers an in-depth…</option><option value="247">247: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that shows multilingual…</option><option value="248">248: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that trains language mo…</option><option value="249">249: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that tries to interpret…</option><option value="250">250: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest some work that develops multimod…</option><option value="253">253: [inline_nonacl] sp=1 qual=2 gr=1 — Has any research tried to mitigate overfitting in …</option><option value="254">254: [inline_nonacl] sp=1 qual=1 gr=1 — Has any study explored the zero-shot extraction of…</option><option value="255">255: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any research that uses multiple mod…</option><option value="256">256: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any work that improves the work on …</option><option value="258">258: [inline_nonacl] sp=1 qual=2 gr=1 — Have any papers tried to address the background-sh…</option><option value="259">259: [inline_nonacl] sp=1 qual=2 gr=1 — Have any recent publications explored the use of n…</option><option value="262">262: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers collected feedback from r…</option><option value="263">263: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers critically analyzed the p…</option><option value="264">264: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers examined the efficacy of …</option><option value="266">266: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers explored methods to impro…</option><option value="267">267: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers introduced a dedicated pr…</option><option value="268">268: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers investigated human capaci…</option><option value="270">270: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers suggested methods for sum…</option><option value="271">271: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers suggested techniques for …</option><option value="273">273: [inline_nonacl] sp=1 qual=1 gr=1 — Have any studies explored the creation of memory m…</option><option value="274">274: [inline_nonacl] sp=1 qual=2 gr=1 — Have there been any advancements in language model…</option><option value="275">275: [inline_nonacl] sp=1 qual=2 gr=1 — How can I locate a dataset containing toxic senten…</option><option value="276">276: [inline_nonacl] sp=1 qual=2 gr=1 — How can SQL-to-text be utilized to improve text-to…</option><option value="277">277: [inline_nonacl] sp=0 qual=1 gr=1 — How can dense retrieval models for open-domain que…</option><option value="279">279: [inline_nonacl] sp=1 qual=2 gr=1 — In multi-hop question answering, is there a paper …</option><option value="280">280: [inline_nonacl] sp=1 qual=2 gr=1 — Is it possible to adatp named entity recognition s…</option><option value="281">281: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a benchmark designed to assess language m…</option><option value="282">282: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a comprehensive dataset available for sum…</option><option value="283">283: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a dataset available for open-domain targe…</option><option value="284">284: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a dataset containing question-answer pair…</option><option value="285">285: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a research paper that has developed a cus…</option><option value="286">286: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a specialized question answering dataset …</option><option value="287">287: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a study that investigates if large langua…</option><option value="288">288: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any paper that tried fine-tuning mBERT to…</option><option value="289">289: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any research that investigates how to use…</option><option value="290">290: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research examining if multilingual pre-tr…</option><option value="291">291: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research on a specialized language model …</option><option value="292">292: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research that argues for transparency and…</option><option value="293">293: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research that investigates embedding mult…</option><option value="294">294: [inline_nonacl] sp=1 qual=2 gr=1 — Is there work on text classification that explores…</option><option value="295">295: [inline_nonacl] sp=1 qual=2 gr=1 — What approaches have been suggested to lower the c…</option><option value="296">296: [inline_nonacl] sp=1 qual=1 gr=1 — What are some scholarly articles that explore scal…</option><option value="298">298: [inline_nonacl] sp=0 qual=2 gr=1 — What are some studies that explore data-poisoning …</option><option value="299">299: [inline_nonacl] sp=1 qual=1 gr=1 — What are the latest advancements in predicting sui…</option><option value="300">300: [inline_nonacl] sp=0 qual=1 gr=1 — What are the latest developments in conversational…</option><option value="301">301: [inline_nonacl] sp=0 qual=1 gr=1 — What benchmarks have prior research utilized to as…</option><option value="302">302: [inline_nonacl] sp=1 qual=2 gr=1 — What concerns or key points have been highlighted …</option><option value="303">303: [inline_nonacl] sp=0 qual=2 gr=1 — What difficulties do neural conversational models …</option><option value="304">304: [inline_nonacl] sp=1 qual=2 gr=1 — What literature is available on training semantic …</option><option value="305">305: [inline_nonacl] sp=1 qual=2 gr=1 — What methods exist for tailoring news suggestions …</option><option value="308">308: [inline_nonacl] sp=1 qual=1 gr=1 — What recent developments in transformer architectu…</option><option value="309">309: [inline_nonacl] sp=1 qual=1 gr=1 — What recent research has been conducted on improvi…</option><option value="310">310: [inline_nonacl] sp=1 qual=1 gr=1 — What research articles should I consult to underst…</option><option value="313">313: [inline_nonacl] sp=0 qual=1 gr=1 — What research exists on incorporating knowledge gr…</option><option value="314">314: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on leveraging syntactic roles…</option><option value="315">315: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on the impact of scaling on p…</option><option value="316">316: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on using reinforcement learni…</option><option value="317">317: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on applying contr…</option><option value="318">318: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on creating neura…</option><option value="319">319: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on determining th…</option><option value="320">320: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on enhancing conv…</option><option value="321">321: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on incorporating …</option><option value="322">322: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on news recommend…</option><option value="323">323: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on scaling within…</option><option value="324">324: [inline_nonacl] sp=0 qual=2 gr=1 — What research has been conducted on the impact of …</option><option value="327">327: [inline_nonacl] sp=1 qual=1 gr=1 — What research is available on the concept of using…</option><option value="329">329: [inline_nonacl] sp=1 qual=2 gr=1 — What resources or toolkits are available to facili…</option><option value="330">330: [inline_nonacl] sp=1 qual=1 gr=1 — What sources offer research on maintaining factual…</option><option value="332">332: [inline_nonacl] sp=0 qual=1 gr=1 — What techniques exist for efficiently fine-tuning …</option><option value="333">333: [inline_nonacl] sp=0 qual=2 gr=1 — What techniques exist for incorporating context in…</option><option value="336">336: [inline_nonacl] sp=1 qual=1 gr=1 — Where can I find a database of good prompts to use…</option><option value="337">337: [inline_nonacl] sp=0 qual=1 gr=1 — Where can I read about the using soft embeddings t…</option><option value="338">338: [inline_nonacl] sp=1 qual=2 gr=1 — Which method involves training additional prompt t…</option><option value="339">339: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper has conducted a thorough analysis of h…</option><option value="340">340: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper introduced the task of creating extend…</option><option value="341">341: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper presents a platform that emphasizes ev…</option><option value="342">342: [inline_nonacl] sp=0 qual=1 gr=1 — Which paper shows that generated captions of model…</option><option value="343">343: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper shows that human experts and non-exper…</option><option value="344">344: [inline_nonacl] sp=0 qual=1 gr=1 — Which work introduces sparse attention modules and…</option><option value="345">345: [inline_nonacl] sp=1 qual=1 gr=1 — Which work pushes the limit of model quantization …</option><option value="348">348: [inline_nonacl] sp=1 qual=1 gr=1 — Which work suggests that machine translation model…</option><option value="349">349: [inline_nonacl] sp=1 qual=1 gr=1 — Which works shows that training large language mod…</option><option value="350">350: [inline_nonacl] sp=1 qual=2 gr=1 — ould you direct me to research that shows that the…</option><option value="351">351: [manual_acl] sp=1 qual=1 gr=1 — Are there any examples of using dense phrase retri…</option><option value="352">352: [manual_acl] sp=1 qual=1 gr=1 — Are there any large-scale and open-source text sim…</option><option value="354">354: [manual_acl] sp=1 qual=1 gr=1 — Could you recommend a dataset paper which presents…</option><option value="355">355: [manual_acl] sp=1 qual=1 gr=1 — Find the NLP paper that focuses on dialogue genera…</option><option value="356">356: [manual_acl] sp=0 qual=1 gr=1 — Give me a paper proposing to circumvent a single-t…</option><option value="357">357: [manual_acl] sp=1 qual=2 gr=1 — How to achieve zero-shot lip reading?…</option><option value="358">358: [manual_acl] sp=1 qual=2 gr=1 — How to better attract readers to news articles by …</option><option value="359">359: [manual_acl] sp=0 qual=1 gr=1 — How to faithfully and explicitly measure the helpf…</option><option value="361">361: [manual_acl] sp=1 qual=1 gr=1 — In multimodal (multilingual) abstractive summariza…</option><option value="362">362: [manual_acl] sp=1 qual=1 gr=1 — Is there a Chinese hate speech paper that construc…</option><option value="363">363: [manual_acl] sp=0 qual=1 gr=1 — Is there a dataset that allows to perform aspect-b…</option><option value="365">365: [manual_acl] sp=1 qual=1 gr=1 — Is there a dialogue dataset where a speaker's utte…</option><option value="367">367: [manual_acl] sp=1 qual=2 gr=1 — Is there a method that measures the information pr…</option><option value="368">368: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper comparing knowledge distillation …</option><option value="370">370: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that applies large language model…</option><option value="371">371: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that connects the basic elements …</option><option value="373">373: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that shows that language models' …</option><option value="374">374: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that supports the use of automate…</option><option value="375">375: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that uses Explainable AI techniqu…</option><option value="376">376: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses an app for a popular ta…</option><option value="377">377: [manual_acl] sp=0 qual=1 gr=1 — Is there a paper that uses evolutionary algorithms…</option><option value="379">379: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses the tree structure of m…</option><option value="380">380: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that utilizes the characteristics…</option><option value="381">381: [manual_acl] sp=1 qual=2 gr=1 — Is there a study that shows how to help the demons…</option><option value="383">383: [manual_acl] sp=1 qual=1 gr=1 — Is there an evaluation metric for natural language…</option><option value="384">384: [manual_acl] sp=1 qual=1 gr=1 — Is there any dataset that contains minimally-contr…</option><option value="385">385: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper about style transfer for storie…</option><option value="386">386: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper exploring real speakers and thu…</option><option value="387">387: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper leverages knowledge distillatio…</option><option value="388">388: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that address attacks on code mo…</option><option value="390">390: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that applies curriculum learnin…</option><option value="391">391: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that applies symbolic distillat…</option><option value="392">392: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that attempts to evaluate the s…</option><option value="393">393: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that automatically creates a da…</option><option value="394">394: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that combines causal inference …</option><option value="396">396: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that employs code LLMs to itera…</option><option value="397">397: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores and annotates the…</option><option value="398">398: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores using only an enc…</option><option value="399">399: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that investigates backdoor atta…</option><option value="400">400: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that leverages graph neural net…</option><option value="401">401: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that leverages syntactic rules …</option><option value="402">402: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that performs adversarial train…</option><option value="403">403: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that proposes a new multimodal …</option><option value="405">405: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that reveals annotation problem…</option><option value="406">406: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that studies a teacher AI infer…</option><option value="407">407: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that tries to investigate LLMs’…</option><option value="408">408: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that uses data collected from t…</option><option value="409">409: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses prompt tuning in mult…</option><option value="410">410: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses token-level loss to e…</option><option value="412">412: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that utilizes graph structure t…</option><option value="413">413: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that utilizes masked language m…</option><option value="415">415: [manual_acl] sp=1 qual=2 gr=1 — Is there any work that allows large numbers of mod…</option><option value="416">416: [manual_acl] sp=1 qual=1 gr=1 — Is there any work that attacks language models in …</option><option value="418">418: [manual_acl] sp=1 qual=2 gr=1 — Is there commonsense reasoning dataset which gener…</option><option value="419">419: [manual_acl] sp=1 qual=2 gr=1 — Is there such a factuality evaluation dataset that…</option><option value="420">420: [manual_acl] sp=1 qual=2 gr=1 — Is there such a reading comprehension dataset in u…</option><option value="421">421: [manual_acl] sp=1 qual=1 gr=1 — Provide an example of a paper which proposes a met…</option><option value="422">422: [manual_acl] sp=1 qual=2 gr=1 — What are some data-efficient ways to learn text em…</option><option value="424">424: [manual_acl] sp=0 qual=1 gr=1 — What is a large event-coverage general-domain even…</option><option value="425">425: [manual_acl] sp=1 qual=1 gr=1 — What is the first paper to address the problem of …</option><option value="427">427: [manual_acl] sp=1 qual=2 gr=1 — What limitations do large language models have in …</option><option value="428">428: [manual_acl] sp=1 qual=2 gr=1 — What paper compares humans' and language models' n…</option><option value="429">429: [manual_acl] sp=1 qual=2 gr=1 — What work attempts to explore multi-hop reasoning …</option><option value="430">430: [manual_acl] sp=1 qual=1 gr=1 — Which article first proposed shuffled-group-whiten…</option><option value="431">431: [manual_acl] sp=1 qual=1 gr=1 — Which dataset supports narration generation and te…</option><option value="432">432: [manual_acl] sp=0 qual=1 gr=1 — Which family of model generally perform the best f…</option><option value="434">434: [manual_acl] sp=1 qual=2 gr=1 — Which knowledge graph completion method focuses on…</option><option value="435">435: [manual_acl] sp=1 qual=1 gr=1 — Which language model distillation paper that first…</option><option value="436">436: [manual_acl] sp=1 qual=2 gr=1 — Which numerical reasoning paper first published a …</option><option value="437">437: [manual_acl] sp=1 qual=2 gr=1 — Which paper about parameter-efficient finetuning f…</option><option value="438">438: [manual_acl] sp=1 qual=1 gr=1 — Which paper combines the advantages of different f…</option><option value="439">439: [manual_acl] sp=0 qual=1 gr=1 — Which paper did a comprehensive survey of the code…</option><option value="440">440: [manual_acl] sp=1 qual=2 gr=1 — Which paper employs a two-stage approach in genera…</option><option value="441">441: [manual_acl] sp=1 qual=1 gr=1 — Which paper enables interactive semantic parsing b…</option><option value="442">442: [manual_acl] sp=1 qual=1 gr=1 — Which paper explored training a GPT-2 for automati…</option><option value="443">443: [manual_acl] sp=1 qual=1 gr=1 — Which paper first aggregates statements to represe…</option><option value="444">444: [manual_acl] sp=1 qual=2 gr=1 — Which paper first applied the chain-of-thought tec…</option><option value="445">445: [manual_acl] sp=1 qual=2 gr=1 — Which paper first apply mixture of experts idea to…</option><option value="446">446: [manual_acl] sp=1 qual=2 gr=1 — Which paper first attempts to take potential depen…</option><option value="447">447: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines different methods for u…</option><option value="448">448: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines rewriting and expansion…</option><option value="449">449: [manual_acl] sp=1 qual=1 gr=1 — Which paper first conducted the positioned error t…</option><option value="450">450: [manual_acl] sp=1 qual=1 gr=1 — Which paper first construct large-scale corpus to …</option><option value="452">452: [manual_acl] sp=1 qual=1 gr=1 — Which paper first explored In-context learning in …</option><option value="453">453: [manual_acl] sp=1 qual=1 gr=1 — Which paper first found that multilingual models c…</option><option value="454">454: [manual_acl] sp=1 qual=2 gr=1 — Which paper first introduced document content as a…</option><option value="455">455: [manual_acl] sp=1 qual=1 gr=1 — Which paper first propose to mask positions to pre…</option><option value="456">456: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed a cross-domain language…</option><option value="457">457: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed extracting the pair of …</option><option value="459">459: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed to combine pretrained m…</option><option value="460">460: [manual_acl] sp=0 qual=1 gr=1 — Which paper first proposed to only update some ori…</option><option value="461">461: [manual_acl] sp=1 qual=1 gr=1 — Which paper first published a real-world Chinese-E…</option><option value="462">462: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that it is possible to mai…</option><option value="463">463: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that large language models…</option><option value="464">464: [manual_acl] sp=1 qual=2 gr=1 — Which paper first studied the efficiency robustnes…</option><option value="465">465: [manual_acl] sp=1 qual=2 gr=1 — Which paper first use the attention weights to gui…</option><option value="466">466: [manual_acl] sp=0 qual=1 gr=1 — Which paper first used structural information for …</option><option value="468">468: [manual_acl] sp=0 qual=1 gr=1 — Which paper highlights the need for leveraging all…</option><option value="469">469: [manual_acl] sp=1 qual=2 gr=1 — Which paper introduce a DRO (distribution robust o…</option><option value="470">470: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduced the human-evaluated timelin…</option><option value="471">471: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduces the R-GCN technique into do…</option><option value="472">472: [manual_acl] sp=1 qual=1 gr=1 — Which paper investigates the influence of the dive…</option><option value="474">474: [manual_acl] sp=0 qual=1 gr=1 — Which paper is the first to comprehensively review…</option><option value="476">476: [manual_acl] sp=1 qual=2 gr=1 — Which paper measured how well the source-translati…</option><option value="477">477: [manual_acl] sp=1 qual=2 gr=1 — Which paper presents an easy to implement and high…</option><option value="478">478: [manual_acl] sp=1 qual=2 gr=1 — Which paper produces a dataset for text simplifica…</option><option value="479">479: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed a learning-based data augment…</option><option value="480">480: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed decomposing the logit update …</option><option value="481">481: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed dictionary-based Bayesian inf…</option><option value="482">482: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed the integration of human tran…</option><option value="483">483: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes a memory-efficient optimizer …</option><option value="484">484: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes the two-stage training method…</option><option value="485">485: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes to use rewriting based approa…</option><option value="486">486: [manual_acl] sp=1 qual=2 gr=1 — Which paper showed that social relationships were …</option><option value="487">487: [manual_acl] sp=1 qual=2 gr=1 — Which paper shows assessment of training instabili…</option><option value="488">488: [manual_acl] sp=1 qual=1 gr=1 — Which paper shows that in instruction tuning, the …</option><option value="489">489: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies how current retrieval systems …</option><option value="490">490: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies the concept of enhancing the c…</option><option value="491">491: [manual_acl] sp=1 qual=2 gr=1 — Which paper surveyed the datasets and tasks of ask…</option><option value="492">492: [manual_acl] sp=1 qual=2 gr=1 — Which paper used both automatically generated and …</option><option value="493">493: [manual_acl] sp=1 qual=2 gr=1 — Which paper utilizes language models to generate s…</option><option value="494">494: [manual_acl] sp=1 qual=1 gr=1 — Which paper was the first to propose combining hum…</option><option value="495">495: [manual_acl] sp=0 qual=2 gr=1 — Which papers develop methods to make in-context le…</option><option value="496">496: [manual_acl] sp=0 qual=1 gr=1 — Which papers were among the first to explore the t…</option><option value="497">497: [manual_acl] sp=1 qual=1 gr=1 — Which pre-trained model is specifically designed f…</option><option value="499">499: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model can demonstrate that v…</option><option value="500">500: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model paper in 2023 develope…</option><option value="501">501: [manual_acl] sp=1 qual=2 gr=1 — Which was the first paper to explore the online ad…</option><option value="503">503: [manual_acl] sp=1 qual=1 gr=1 — Which work proposes an approach to improve candida…</option><option value="504">504: [manual_acl] sp=1 qual=2 gr=1 — what's the first paper that manages to handle KBQA…</option><option value="505">505: [manual_acl] sp=1 qual=1 gr=1 — which paper first focuses on addressing the over-s…</option><option value="506">506: [manual_iclr] sp=1 qual=1 gr=1 — Can we reduce visual tokens in vision transformers…</option><option value="507">507: [manual_iclr] sp=1 qual=1 gr=1 — Can we learn to represent an image with arbitary n…</option><option value="508">508: [manual_iclr] sp=1 qual=2 gr=1 — Are there any papers that construct convolutional …</option><option value="509">509: [manual_iclr] sp=1 qual=1 gr=1 — Are there any papers that study whether you can id…</option><option value="511">511: [manual_iclr] sp=1 qual=1 gr=1 — Are there datasets and benchmarks available for me…</option><option value="512">512: [manual_iclr] sp=1 qual=2 gr=1 — Are there sequential learning guarantees for confi…</option><option value="513">513: [manual_iclr] sp=1 qual=2 gr=1 — Can we find the solution of the Bilevel optimizati…</option><option value="514">514: [manual_iclr] sp=0 qual=2 gr=1 — Can you find a dataset that shows LLM-based evalua…</option><option value="515">515: [manual_iclr] sp=1 qual=2 gr=1 — Can you find a research paper that discusses using…</option><option value="516">516: [manual_iclr] sp=1 qual=1 gr=1 — I'm using Local SGD with a decaying learning rate …</option><option value="517">517: [manual_iclr] sp=1 qual=1 gr=1 — In video diffusion models, is there any paper that…</option><option value="518">518: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper illustrating that pre-trained tra…</option><option value="519">519: [manual_iclr] sp=1 qual=2 gr=1 — Is there a paper that takes a mixed machine learni…</option><option value="520">520: [manual_iclr] sp=1 qual=1 gr=1 — Is there a paper which applies Bayesian optimizati…</option><option value="521">521: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper which proposes a general data sel…</option><option value="523">523: [manual_iclr] sp=0 qual=1 gr=1 — Is there a single GNN model that can inductively g…</option><option value="524">524: [manual_iclr] sp=1 qual=2 gr=1 — Is there a theory paper that explains why sometime…</option><option value="525">525: [manual_iclr] sp=1 qual=1 gr=1 — Is there an existing dataset of images with alt-te…</option><option value="526">526: [manual_iclr] sp=1 qual=1 gr=1 — Is there any generalizable NeRF paper that disenta…</option><option value="527">527: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper applies off-shelf GPT-2 model i…</option><option value="528">528: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper improves adversarial training b…</option><option value="529">529: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that explores ways to parameter…</option><option value="530">530: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that previously proposed to con…</option><option value="531">531: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that seamlessly integrates the …</option><option value="532">532: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that theoretically explains why…</option><option value="533">533: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that uses Lipschitz continuity …</option><option value="534">534: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper trying to improve MLE for auto-…</option><option value="535">535: [manual_iclr] sp=1 qual=1 gr=1 — Name a paper which proposes a probabilsitic formul…</option><option value="536">536: [manual_iclr] sp=1 qual=2 gr=1 — What are some evaluation benchmarks for LLM privac…</option><option value="537">537: [manual_iclr] sp=1 qual=1 gr=1 — What are the key advantages of coupling neural SDE…</option><option value="538">538: [manual_iclr] sp=1 qual=2 gr=1 — What is a paper studying data being collected in b…</option><option value="539">539: [manual_iclr] sp=1 qual=1 gr=1 — What is the first paper that theoretically studies…</option><option value="540">540: [manual_iclr] sp=1 qual=2 gr=1 — What is the first paper that uses the generalized …</option><option value="541">541: [manual_iclr] sp=1 qual=1 gr=1 — What molecular representation learning paper intro…</option><option value="542">542: [manual_iclr] sp=1 qual=2 gr=1 — What open-source dataset combined knowledge retrie…</option><option value="543">543: [manual_iclr] sp=0 qual=1 gr=1 — What paper considers sensitive data issue when pro…</option><option value="544">544: [manual_iclr] sp=0 qual=1 gr=1 — What paper evaluated the ability of visual few-sho…</option><option value="545">545: [manual_iclr] sp=1 qual=1 gr=1 — What paper first adapted ControlNet to generate co…</option><option value="546">546: [manual_iclr] sp=1 qual=1 gr=1 — What paper first associate the modeling frequency …</option><option value="547">547: [manual_iclr] sp=1 qual=2 gr=1 — What paper first extends rotary positional encodin…</option><option value="548">548: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposed a robust perceptual simi…</option><option value="549">549: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposes that simply reversing th…</option><option value="551">551: [manual_iclr] sp=1 qual=1 gr=1 — What paper first used the technique of prompt engi…</option><option value="552">552: [manual_iclr] sp=1 qual=1 gr=1 — What paper first uses decoupled workers in distrib…</option><option value="553">553: [manual_iclr] sp=1 qual=2 gr=1 — What paper investigated the effect of the relative…</option><option value="554">554: [manual_iclr] sp=1 qual=1 gr=1 — What paper is the first to prove finetuned LLM can…</option><option value="555">555: [manual_iclr] sp=1 qual=1 gr=1 — What paper mitigates language model sampling error…</option><option value="556">556: [manual_iclr] sp=1 qual=2 gr=1 — What paper mitigates the vocabulary size limitatio…</option><option value="558">558: [manual_iclr] sp=1 qual=1 gr=1 — What paper showed first that one can build a fully…</option><option value="559">559: [manual_iclr] sp=0 qual=1 gr=1 — What paper shows that RLAIF can fully replace RLHF…</option><option value="561">561: [manual_iclr] sp=1 qual=1 gr=1 — What work first uses LLM to code robotic simulatio…</option><option value="562">562: [manual_iclr] sp=1 qual=2 gr=1 — What work proposes a model to learn a latent regul…</option><option value="563">563: [manual_iclr] sp=1 qual=1 gr=1 — What work proposes to combine video foundation mod…</option><option value="565">565: [manual_iclr] sp=1 qual=1 gr=1 — Which foundation model paper first proposed a time…</option><option value="567">567: [manual_iclr] sp=1 qual=1 gr=1 — Which machine learning paper proposed certified ro…</option><option value="568">568: [manual_iclr] sp=0 qual=1 gr=1 — Which multimodal large language model represents v…</option><option value="569">569: [manual_iclr] sp=1 qual=1 gr=1 — Which neural theorem proving paper first attempted…</option><option value="570">570: [manual_iclr] sp=1 qual=2 gr=1 — Which paper considers both weights and activations…</option><option value="571">571: [manual_iclr] sp=1 qual=1 gr=1 — Which paper contains quantitative results demonstr…</option><option value="572">572: [manual_iclr] sp=1 qual=1 gr=1 — Which paper examined the scalability of instructio…</option><option value="573">573: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first applied the chain of thought con…</option><option value="574">574: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first derived online occupany estimati…</option><option value="575">575: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first found that REINFORCE works bette…</option><option value="576">576: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first found that when transformers are…</option><option value="577">577: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first investigates the knowledge prefe…</option><option value="578">578: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first proposes a unified framework for…</option><option value="579">579: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first proved that wide-enough transfor…</option><option value="580">580: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first showed that task-specific knowle…</option><option value="581">581: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first studied differential privacy for…</option><option value="582">582: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first study POMDP with enhanced feedba…</option><option value="583">583: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first tried to fine-tune LLMs with cha…</option><option value="584">584: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first used language models to emulate …</option><option value="585">585: [manual_iclr] sp=1 qual=1 gr=1 — Which paper formally defines the problem of model …</option><option value="586">586: [manual_iclr] sp=1 qual=2 gr=1 — Which paper found that using common character enco…</option><option value="587">587: [manual_iclr] sp=1 qual=1 gr=1 — Which paper in human motion generation can control…</option><option value="588">588: [manual_iclr] sp=1 qual=2 gr=1 — Which paper is the first to model the helpfulness …</option><option value="589">589: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes an alignment framework that s…</option><option value="590">590: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes to integrate black-box LLMs w…</option><option value="591">591: [manual_iclr] sp=1 qual=1 gr=1 — Which paper studies how difficult is a policy lear…</option><option value="592">592: [manual_iclr] sp=1 qual=2 gr=1 — Which paper trains on linear regression to hypothe…</option><option value="593">593: [manual_iclr] sp=1 qual=1 gr=1 — Which paper uses the latent diffusion model for th…</option><option value="594">594: [manual_iclr] sp=1 qual=2 gr=1 — Which paper utilized MMD flows with Riesz kernels …</option><option value="596">596: [manual_iclr] sp=1 qual=2 gr=1 — Which paper systematically examed the input mismat…</option></select>
    <p id="queryText" class="small has-abs" data-abs="">Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?</p>
    <p id="queryMeta" class="small">set:manual_acl | spec:0 | qual:2 | gr:0</p>

    <label>Relevant document (rank&nbsp;+&nbsp;title)</label>
    <select id="docSel"><option value="0" data-id="258547050" title="Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answerunaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answerunaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question (boolean/span-based) to be generated implicitly. Modeling the question type explicitly is crucial in this (answer-unaware) setting, as the answer which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves stateof-the-art performance.">0: rank=∞  Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation</option></select>
    <p id="docTitle" class="small has-abs" data-abs="Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answerunaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answerunaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question (boolean/span-based) to be generated implicitly. Modeling the question type explicitly is crucial in this (answer-unaware) setting, as the answer which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves stateof-the-art performance.">0: rank=∞  Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation</p>
  </div>

  <!-- PROMPT / LLM / ANNOTATION CARD -->
  <div class="card" style="flex:2 1 520px;">
    <h2>Prompt &amp; LLM setup</h2>

    <label>Prompt</label>
    <div style="display:flex;gap:.5rem;">
      <select id="promptSel" style="flex:1;"><option value="dummy">dummy</option><option value="full_text">full_text</option><option value="title_abstract">title_abstract</option></select>
      <button id="newPromptBtn">+&nbsp;New</button>
    </div>

    <label>Extractor</label>
    <select id="extSel"><option value="dummy">dummy</option><option value="json_list_extractor">json_list_extractor</option></select>

    <label>k (top‑k retrieval)</label>
    <input id="kInp" type="number" value="50" min="1">

    <label>Prompt text</label>
    <textarea id="promptBox"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="savePromptBtn">💾&nbsp;Save</button>
      <button id="reloadPromptBtn">⟳&nbsp;Reload</button>
    </div>

    <details style="margin-top:.75rem;">
      <summary><strong>LLM config</strong></summary>
      <label>API key</label><input id="apiKey" type="text" placeholder="sk‑…">
      <label>Model</label><input id="model" type="text" value="gpt-4o-mini">
      <label>Temperature</label><input id="temp" type="number" value="0" step=".1">
      <label>Max tokens</label><input id="maxTok" type="number" value="2048">
      <label style="display:flex;align-items:center;gap:.5rem;margin-top:.5rem;">
        <input id="wantJson" type="checkbox"> Expect JSON object response
      </label>
    </details>

    <label>Your annotation</label>
    <textarea id="noteBox" placeholder="Add notes about this run…"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="saveNoteBtn" style="background:#0b63ff;color:#fff;">Save&nbsp;annotation</button>
      <button id="runBtn" style="background:#14a44d;color:#fff;">Run</button>
    </div>
  </div>
</section>

<!-- BEFORE / AFTER TABLES ---------------------------------------------->
<section class="flex">
  <div class="card">
    <h2>Before&nbsp;(original)</h2>
    <table id="beforeTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody><tr class="has-abs" data-abs="We aim to overcome the lack of diversity in responses of current dialogue systems and to develop a dialogue system that is engaging as a conversational partner. We propose a generator-evaluator model that evaluates multiple responses generated by a response generator and selects the best response by an evaluator. By generating multiple responses, we obtain diverse responses. We conduct human evaluations to compare the output of the proposed system with that of a baseline system. The results of the human evaluations showed that the proposed system's responses were often judged to be better than the baseline system's, and indicated the effectiveness of the proposed method."><td>1</td><td>Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation</td><td>0.493</td></tr><tr class="has-abs" data-abs="Encoder-decoder based neural architectures serve as the basis of state-of-the-art approaches in end-to-end open domain dialog systems. Since most of such systems are trained with a maximum likelihood (MLE) objective they suffer from issues such as lack of generalizability and the generic response problem, i.e., a system response that can be an answer to a large number of user utterances, e.g., &quot;Maybe, I don't know.&quot; Having explicit feedback on the relevance and interestingness of a system response at each turn can be a useful signal for mitigating such issues and improving system quality by selecting responses from different approaches. Towards this goal, we present a system that evaluates chatbot responses at each dialog turn for coherence and engagement. Our system provides explicit turn-level dialog quality feedback, which we show to be highly correlated with human evaluation. To show that incorporating this feedback in the neural response generation models improves dialog quality, we present two different and complementary mechanisms to incorporate explicit feedback into a neural response generation model: reranking and direct modification of the loss function during training. Our studies show that a response generation model that incorporates these combined feedback mechanisms produce more engaging and coherent responses in an open-domain spoken dialog setting, significantly improving the response quality using both automatic and human evaluation."><td>2</td><td>Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators</td><td>0.493</td></tr><tr class="has-abs" data-abs="Automatic evaluation of open-domain dialogs remains an unsolved problem. Moreover, existing methods do not correlate strongly with human annotations. This paper presents a new automated evaluation method using followups: we measure the probability that a language model will continue the conversation with a fixed set of follow-ups (e.g. Not really relevant here, What are you trying to say?). When compared against twelve existing methods, our new evaluation achieves the highest correlation with human evaluations. domain chatbot. CoRR, abs/2001.09977."><td>3</td><td>Open-Domain Dialog Evaluation using Follow-Ups Likelihood</td><td>0.495</td></tr><tr class="has-abs" data-abs="Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation."><td>4</td><td>Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation</td><td>0.506</td></tr><tr class="has-abs" data-abs="In this paper, a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other. For the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning. Under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents. The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct. These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient. Comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly."><td>5</td><td>Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment</td><td>0.507</td></tr><tr class="has-abs" data-abs="Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large scale experiments. Though researchers have attempted to use metrics for language generation tasks (e.g., perplexity, BLEU) or some model-based reinforcement learning methods (e.g., self-play evaluation) for automatic evaluation, these methods only show very weak correlation with the actual human evaluation in practice. To bridge such a gap, we propose a new framework named ENIGMA for estimating human evaluation scores based on recent advances of off-policy evaluation in reinforcement learning. ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data (see details in Section 2), which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors. Our experiments show that ENIGMA significantly outperforms existing methods in terms of correlation with human evaluation scores.Conversation Transition :P (s = s ∪ a ∪ e|s, a, incomplete conv.) = E(e|s, a), Padding with Pseudo States : P (s =PadT +1|s, a, complete conv. with T turns)=1, P (s =Pad k+1 |s = Pad k , a = NextPad, k &lt; Tmax)=1, Concatenate Conversations : P (s |s = PadT max , a = NextPad) = µ0 s ."><td>6</td><td>Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach</td><td>0.507</td></tr><tr class="has-abs" data-abs="A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval 1 , a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level."><td>7</td><td>DynaEval: Unifying Turn and Dialogue Level Evaluation</td><td>0.510</td></tr><tr class="has-abs" data-abs="A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans(Sordoni et al., 2015;Vinyals &amp; Le, 2015;Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks(Weston et al., 2015b)which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering ∼75k movie entities and with ∼3.5M training examples. We present results of various models on these tasks, and evaluate their performance. * The first three authors contributed equally."><td>8</td><td>EVALUATING PREREQUISITE QUALITIES FOR LEARN- ING END-TO-END DIALOG SYSTEMS</td><td>0.520</td></tr><tr class="has-abs" data-abs="Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model development and whether current systems can well generalize to real-world human-machine conversations. In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems, where human evaluators converse with models and judge the correctness of their answers. We find that the distribution of humanmachine conversations differs drastically from that of human-human conversations, and there is a disagreement between human and goldhistory evaluation in terms of model ranking. We further investigate how to improve automatic evaluations, and propose a question rewriting mechanism based on predicted history, which better correlates with human judgments. Finally, we analyze the impact of various modeling strategies and discuss future directions towards building better conversational question answering systems. 1 * The first two authors contributed equally. 1 Our data and code are publicly available at https:// github.com/princeton-nlp/EvalConvQA."><td>9</td><td>Ditch the Gold Standard: Re-evaluating Conversational Question Answering</td><td>0.521</td></tr><tr class="has-abs" data-abs="We propose LLM-EVAL, a unified multidimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single promptbased evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-EVAL on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-EVAL offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios."><td>10</td><td>LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</td><td>0.525</td></tr><tr class="has-abs" data-abs="As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020."><td>11</td><td>Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of Current Evaluation Protocols</td><td>0.525</td></tr><tr class="has-abs" data-abs="Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating dialog. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these metrics have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of datasets. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different datasets. Furthermore, the metrics are assessed in different settings, to better qualify their respective strengths and weaknesses. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess evaluation metrics and indicates promising directions for future work."><td>12</td><td>A Comprehensive Assessment of Dialog Evaluation Metrics</td><td>0.527</td></tr><tr class="has-abs" data-abs="The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multireference evaluation. Existing metrics have been shown to correlate poorly with human judgement, particularly in open-domain dialog. One alternative is to collect human annotations for evaluation, which can be expensive and time consuming. To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references. A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the quality and the diversity of system output."><td>13</td><td>Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References</td><td>0.529</td></tr><tr class="has-abs" data-abs="When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents. However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality. An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output. In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output. The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users."><td>14</td><td>Automated Metrics That Agree With Human Judgements On Generated Output for an Embodied Conversational Agent</td><td>0.530</td></tr><tr class="has-abs" data-abs="A good conversation requires balance -between simplicity and detail; staying on topic and changing it; asking questions and answering them. Although dialogue agents are commonly evaluated via human judgments of overall quality, the relationship between quality and these individual factors is less well-studied. In this work, we examine two controllable neural text generation methods, conditional training and weighted decoding, in order to control four important attributes for chitchat dialogue: repetition, specificity, response-relatedness and question-asking. We conduct a large-scale human evaluation to measure the effect of these control parameters on multi-turn interactive conversations on the PersonaChat task. We provide a detailed analysis of their relationship to high-level aspects of conversation, and show that by controlling combinations of these variables our models obtain clear improvements in human quality judgments."><td>15</td><td>What makes a good conversation? How controllable attributes affect human judgments</td><td>0.533</td></tr><tr class="has-abs" data-abs="At the heart of improving conversational AI is the open problem of how to evaluate conversations. Issues with automatic metrics are well known(Liu et al., 2016), with human evaluations still considered the gold standard. Unfortunately, how to perform human evaluations is also an open problem: differing data collection methods have varying levels of human agreement and statistical sensitivity, resulting in differing amounts of human annotation hours and labor costs. In this work we compare five different crowdworker-based human evaluation methods and find that different methods are best depending on the types of models compared, with no clear winner across the board. While this highlights the open problems in the area, our analysis leads to advice of when to use which one, and possible future directions."><td>16</td><td>Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents</td><td>0.537</td></tr><tr class="has-abs" data-abs="Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics."><td>17</td><td>HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation</td><td>0.537</td></tr><tr class="has-abs" data-abs="We present &quot;AutoJudge&quot;, an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research."><td>18</td><td>Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement</td><td>0.538</td></tr><tr class="has-abs" data-abs="Open-domain chatbots are supposed to converse freely with humans without being restricted to a topic, task or domain. However, the boundaries and/or contents of opendomain conversations are not clear. To clarify the boundaries of &quot;openness&quot;, we conduct two studies: First, we classify the types of &quot;speech events&quot; encountered in a chatbot evaluation data set (i.e., Meena by Google) and find that these conversations mainly cover the &quot;small talk&quot; category and exclude the other speech event categories encountered in real life human-human communication. Second, we conduct a small-scale pilot study to generate online conversations covering a wider range of speech event categories between two humans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook). A human evaluation of these generated conversations indicates a preference for human-human conversations, since the human-chatbot conversations lack coherence in most speech event categories. Based on these results, we suggest (a) using the term &quot;small talk&quot; instead of &quot;opendomain&quot; for the current chatbots which are not that &quot;open&quot; in terms of conversational abilities yet, and (b) revising the evaluation methods to test the chatbot conversations against other speech events."><td>19</td><td>How "open" are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation</td><td>0.540</td></tr><tr class="has-abs" data-abs="Multiple different responses are often plausible for a given open domain dialog context. Prior work has shown the importance of having multiple valid reference responses for meaningful and robust automated evaluations. In such cases, common practice has been to collect more human written references. However, such collection can be expensive, time consuming, and not easily scalable. Instead, we propose a novel technique for automatically expanding a human generated reference to a set of candidate references. We fetch plausible references from knowledge sources, and adapt them so that they are more fluent in context of the dialog instance in question. More specifically, we use (1) a commonsense knowledge base to elicit a large number of plausible reactions given the dialog history (2) relevant instances retrieved from dialog corpus, using similar past as well as future contexts. We demonstrate that our automatically expanded reference sets lead to large improvements in correlations of automated metrics with human ratings of system outputs for DailyDialog dataset. 1"><td>20</td><td>Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation</td><td>0.540</td></tr><tr class="has-abs" data-abs="In dialogue systems, automatically evaluating machine-generated responses is critical and challenging.Despite the tremendous progress in dialogue generation research, its evaluation heavily depends on human judgments.The standard word-overlapping based evaluation metrics are ineffective for dialogues.As a result, most of the recently proposed metrics are model-based and reference-free, which learn to score different aspects of a conversation.However, understanding each aspect requires a separate model, which makes them computationally expensive.To this end, we propose Dial-M, a Masking-based reference-free framework for Dialogue evaluation.The main idea is to mask the keywords of the current utterance and predict them, given the dialogue history and various conditions (like knowledge, persona, etc.), thereby making the evaluation framework simple and easily extensible for multiple datasets.Regardless of its simplicity, Dial-M achieves comparable performance to state-ofthe-art metrics on several dialogue evaluation datasets.We also discuss the interpretability of our proposed metric along with error analysis."><td>21</td><td>Dial-M: A Masking-based Framework for Dialogue Evaluation</td><td>0.543</td></tr><tr class="has-abs" data-abs="Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation. Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost. Self-replication experiments reveal almost perfectly repeatable results with a correlation of r = 0.969. Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests. Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed. We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics. Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected."><td>22</td><td>Achieving Reliable Human Assessment of Open-Domain Dialogue Systems</td><td>0.543</td></tr><tr class="has-abs" data-abs="Evaluation of complex, collaborative dialogue systems is a difficult task. Traditionally, developers have relied upon subjective feedback from the user, and parametrisation over observable metrics. However, both models place some reliance on the notion of a task; that is, the system is helping to user achieve some clearly defined goal, such as book a flight or complete a banking transaction. It is not clear that such metrics are as useful when dealing with a system that has a more complex task, or even no definable task at all, beyond maintain and performing a collaborative dialogue. Working within the EU funded COMPANIONS program, we investigate the use of appropriateness as a measure of conversation quality, the hypothesis being that good companions need to be good conversational partners . We report initial work in the direction of annotating dialogue for indicators of good conversation, including the annotation and comparison of the output of two generations of the same dialogue system."><td>23</td><td>Evaluating Human-Machine Conversation for Appropriateness</td><td>0.543</td></tr><tr class="has-abs" data-abs="The paper first addresses a series of issues basic to evaluating the usability of spoken language dialogue systems, including types and purpose of evaluation, when to evaluate and which methods to use, user involvement, how to evaluate and what to evaluate. We then go on to present and discuss a comprehensive set of usability evaluation criteria for spoken language dialogue systems."><td>24</td><td>Usability Evaluation in Spoken Language Dialogue Systems</td><td>0.544</td></tr><tr class="has-abs" data-abs="Human Evaluation (HE) of automatically generated responses is necessary for the advancement of human-machine dialogue research. Current automatic evaluation measures are poor surrogates, at best. There are no agreed-upon HE protocols and it is difficult to develop them. As a result, researchers either perform nonreplicable, non-transparent and inconsistent procedures or, worse, limit themselves to automated metrics. We propose to standardize the human evaluation of response generation models by publicly sharing a detailed protocol. The proposal includes the task design, annotators recruitment, task execution, and annotation reporting. Such protocol and process can be used as-is, as-a-whole, in-part, or modified and extended by the research community. We validate the protocol by evaluating two conversationally fine-tuned state-of-the-art models (GPT-2 and T5) for the complex task of personalized response generation. We invite the community to use this protocol -or its future community amended versions -as a transparent, replicable, and comparable approach to HE of generated responses 1 ."><td>25</td><td>Evaluation of Response Generation Models: Shouldn't It Be Shareable and Replicable?</td><td>0.544</td></tr><tr class="has-abs" data-abs="The evaluation of dialogue systems in interaction with simulated users has been proposed to improve turn-level, corpus-based metrics which can only evaluate test cases encountered in a corpus and cannot measure system's ability to sustain multi-turn interactions. Recently, little emphasis was put on automatically assessing the quality of the user model itself, so unless correlations with human studies are measured, the reliability of user model based evaluation is unknown. We propose GCDF1, a simple but effective measure of the quality of semantic-level conversations between a goaldriven user agent and a system agent. In contrast with previous approaches we measure the F-score at dialogue level and consider user and system behaviours to improve recall and precision estimation. We facilitate scores interpretation by providing a rich hierarchical structure with information about conversational patterns present in the test data and tools to efficiently query the conversations generated. We apply our framework to assess the performance and weaknesses of a Convlab2 user model 1 ."><td>26</td><td>GCDF1: A Goal-and Context-Driven F-Score for Evaluating User Models</td><td>0.545</td></tr><tr class="has-abs" data-abs="This paper introduces an adversarial method to stress-test trained metrics to evaluate conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by relatively simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans. Jan Deriu and Mark Cieliebak. 2019. Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement. In"><td>27</td><td>Probing the Robustness of Trained Metrics for Conversational Dialogue Systems</td><td>0.545</td></tr><tr class="has-abs" data-abs="Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. 1Original sentence: Cameron is the director of Titanic Change names:Kate is the director of Titanic (incorrect)Task Criteria Machine TranslationAdequacy: The generated translation should adequately represent all the information present in the reference.Question GenerationRelevance: Is the question related to the source material they are based upon. Answerability: Is the generated question answerable given the context. Informativeness: The summary should convey the key points of the text. Non-redundancy: The summary should not repeat any points, and ideally have maximal information coverage within the limited text length.Abstractive SummarizationReferential clarity: Any intra-sentence or cross-sentence references in the summary should be unambiguous and within the scope of the summary. Focus: The summary needs to have a focus and all the sentences need to contain information related to this focal point. Structure and Coherence: The summary should be a well-organized and coherent body of informationDialogue GenerationMaking sense: Does the bot say things that don't make sense? Engagingness: Is the dialogue agent enjoyable to talk to? Interestingness: Did you find the bot interesting to talk to? Inquisitivenes: Does the bot ask a good amount of questions? Listening: Does the bot pay attention to what you say? Avoiding Repetition: Does the bot repeat itself? (either within or across utterances) Humanness: Is the conversation with a person or a bot?"><td>28</td><td>Perturbation CheckLists for Evaluating NLG Evaluation Metrics</td><td>0.545</td></tr><tr class="has-abs" data-abs="Incorporating personas information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have primarily focused on self personas and have overlooked the value of partner personas. Moreover, in practical applications, the availability of the gold partner personas is often not the case. This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation. Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement. Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant, interesting, coherent and informative partner personas, even compared to the ground truth partner personas. This enhances the succeeding dialogue response generation, which surpasses our competitive baselines that condition on the ground truth partner personas."><td>29</td><td>Partner Personas Generation for Dialogue Response Generation</td><td>0.546</td></tr><tr class="has-abs" data-abs="There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent. 1"><td>30</td><td>Plug-and-Play Conversational Models</td><td>0.547</td></tr><tr class="has-abs" data-abs="Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for opendomain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of chat-oriented opendomain dialogue systems that reliably measures several aspects of dialogue capabilities is desired. This paper presents a novel human evaluation method to estimate the rates of many dialogue system behaviors. Our method is used to evaluate four state-of-the-art open-domain dialogue systems and compared with existing approaches. The analysis demonstrates that our behavior method is more suitable than alternative Likert-style or comparative approaches for dimensional evaluation of these systems.A detailed validation of human evaluation meth-ods, including likert scales and pairwise comparisons (Section 7).A comprehensive evaluation of four MTOD chatbots using validated metrics (Section 8).By presenting a detailed picture of MTOD chatbot performance and standard methods to evaluate them, we aid future work's efforts to further understand and improve human-computer interaction. Our evaluation platform, analyses, and data are available at https://github.com/emorynlp/ ChatEvaluationPlatform.ChatbotsTo evaluate the strengths and weaknesses of MTOD models, we select the chatbots for our study using a 15044 two-stage process: (1) a literature review to identify chatbot candidates, and (2) a pilot evaluation to select the final set of bots for our full study.Literature Review To promote diversity among the selected chatbots, we focus our review on four popular themes of the human-computer chat: (1) Knowledge-grounded chat, (2) Empathetic chat, (3) Self-consistent chat, and(4)General open-domain chat with large pre-training resources like Reddit. Candidate chatbots are selected from each theme using the following criteria:1. The bot must demonstrate state-of-the-art performance in a task related to the theme. 1 2. The implementation must be provided. 2 3. The response latency of the bot must be &lt;10 seconds using modern GPU hardware."><td>31</td><td>Chat-Oriented Dialogue Systems</td><td>0.549</td></tr><tr class="has-abs" data-abs="While rich, open-domain textual data are generally available and may include interesting phenomena (humor, sarcasm, empathy, etc.)   most are designed for language processing tasks, and are usually in a non-conversational format. In this work, we take a step towards automatically generating conversational data using Generative Conversational Networks, aiming to benefit from the breadth of available language and knowledge data, and train open domain social conversational agents. We evaluate our approach on conversations with and without knowledge on the Topical Chat dataset using automatic metrics and human evaluators. Our results show that for conversations without knowledge grounding, GCN can generalize from the seed data, producing novel conversations that are less relevant but more engaging and for knowledge-grounded conversations, it can produce more knowledge-focused, fluent, and engaging conversations. Specifically, we show that for open-domain conversations with 10% of seed data, our approach performs close to the baseline that uses 100% of the data, while for knowledge-grounded conversations, it achieves the same using only 1% of the data, on human ratings of engagingness, fluency, and relevance."><td>32</td><td>Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks</td><td>0.549</td></tr><tr class="has-abs" data-abs="To evaluate the performance of a multi-domain goal-oriented Dialogue System (DS), it is important to understand what the users' goals are for the conversations and whether those goals are successfully achieved. The success rate of goals directly correlates with user satisfaction and perceived usefulness of the DS. In this paper, we propose a novel automatic dialogue evaluation framework that jointly performs two tasks: goal segmentation and goal success prediction. We extend the RoBERTa-IQ model (Gupta et al., 2021) by adding multi-task learning heads for goal segmentation and success prediction. Using an annotated dataset from a commercial DS, we demonstrate that our proposed model reaches an accuracy that is on-par with single-pass human annotation comparing to a three-pass gold annotation benchmark."><td>33</td><td>Joint Goal Segmentation and Goal Success Prediction on Multi-Domain Conversations</td><td>0.549</td></tr><tr class="has-abs" data-abs="We propose a set of generic conversational strategies to handle possible system breakdowns in non-task-oriented dialog systems. We also design policies to select these strategies according to dialog context. We combine expert knowledge and the statistical findings derived from data in designing these policies. The policy learned via reinforcement learning outperforms the random selection policy and the locally greedy policy in both simulated and real-world settings. In addition, we propose three metrics for conversation quality evaluation which consider both the local and global quality of the conversation."><td>34</td><td>Strategy and Policy Learning for Non-Task-Oriented Conversational Systems</td><td>0.549</td></tr><tr class="has-abs" data-abs="This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts -new factual content and conversational history -generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi h ) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents."><td>35</td><td>Human-like informative conversations: Better acknowledgements using conditional mutual information</td><td>0.550</td></tr><tr class="has-abs" data-abs="Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference. . 2017. Frames: A corpus for adding memory to goal-oriented dialogue systems. arXiv. . 2019.Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv."><td>36</td><td>Learning an Unreferenced Metric for Online Dialogue Evaluation</td><td>0.550</td></tr><tr class="has-abs" data-abs="Many conversation datasets have been constructed in the recent years using crowdsourcing. However, the data collection process can be time consuming and presents many challenges to ensure data quality. Since language generation has improved immensely in recent years with the advancement of pretrained language models, we investigate how such models can be utilized to generate entire conversations, given only a summary of a conversation as the input. We explore three approaches to generate summary grounded conversations, and evaluate the generated conversations using automatic measures and human judgements. We also show that the accuracy of conversation summarization can be improved by augmenting a conversation summarization dataset with generated conversations."><td>37</td><td>Summary Grounded Conversation Generation</td><td>0.551</td></tr><tr class="has-abs" data-abs="We propose a novel text generation task, namely Curiosity-driven Question Generation. We start from the observation that the Question Generation task has traditionally been considered as the dual problem of Question Answering, hence tackling the problem of generating a question given the text that contains its answer. Such questions can be used to evaluate machine reading comprehension. However, in real life, and especially in conversational settings, humans tend to ask questions with the goal of enriching their knowledge and/or clarifying aspects of previously gathered information.We refer to these inquisitive questions as Curiosity-driven: these questions are generated with the goal of obtaining new information (the answer) which is not present in the input text. In this work, we experiment on this new task using a conversational Question Answering (QA) dataset; further, since the majority of QA dataset are not built in a conversational manner, we describe a methodology to derive data for this novel task from non-conversational QA data. We investigate several automated metrics to measure the different properties of Curious Questions, and experiment different approaches on the Curiosity-driven Question Generation task, including model pre-training and reinforcement learning. Finally, we report a qualitative evaluation of the generated outputs."><td>38</td><td>Ask to Learn: A Study on Curiosity-driven Question Generation</td><td>0.551</td></tr><tr class="has-abs" data-abs="This paper introduces a document grounded dataset for conversations. We define &quot;Document Grounded Conversations&quot; as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses."><td>39</td><td>A Dataset for Document Grounded Conversations</td><td>0.552</td></tr><tr class="has-abs" data-abs="Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q 2 , compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q 2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements."><td>40</td><td>Q 2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</td><td>0.553</td></tr><tr class="has-abs" data-abs="The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based automatic evaluation metric (CMN), which can robustly evaluate open-domain dialogues by augmenting Conditional Variational Autoencoders (CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual Information (MI) to model the semantic similarity of text in the latent space. Experimental results on two opendomain dialogue datasets demonstrate the superiority of our method compared with a wide range of baselines, especially in handling responses which are distant to the golden reference responses in semantics."><td>41</td><td>Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information</td><td>0.553</td></tr><tr class="has-abs" data-abs="The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, systemlevel: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog."><td>42</td><td>USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation</td><td>0.554</td></tr><tr class="has-abs" data-abs="We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems 1 ."><td>43</td><td>Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation</td><td>0.555</td></tr><tr class="has-abs" data-abs="Building an empathetic chatbot is an important objective in dialog generation research, with evaluation being one of the most challenging parts. By empathy, we mean the ability to understand and relate to the speakers' emotions, and respond to them appropriately. Human evaluation has been considered as the current standard for measuring the performance of open-domain empathetic chatbots. However, existing evaluation procedures suffer from a number of limitations we try to address in our current work. In this paper, we describe iEval, a novel interactive evaluation framework where the person chatting with the bots also rates them on different conversational aspects, as well as ranking them, resulting in greater consistency of the scores. We use iEval to benchmark several state-of-the-art empathetic chatbots, allowing us to discover some intricate details in their performance in different emotional contexts. Based on these results, we present key implications for further improvement of such chatbots. To facilitate other researchers using the iEval framework, we will release our dataset consisting of collected chat logs and human scores. 1 1 Our annotated dataset is publicly accessible at https://github.com/Sea94/ieval."><td>44</td><td>iEval: Interactive Evaluation Framework for Open-Domain Empathetic Chatbots</td><td>0.555</td></tr><tr class="has-abs" data-abs="When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of realworld experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With Opinion-Conv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision making."><td>45</td><td>OpinionConv: Conversational Product Search with Grounded Opinions</td><td>0.556</td></tr><tr class="has-abs" data-abs="Stylistic variation is critical to render the utterances generated by conversational agents natural and engaging. In this paper, we focus on sequence-to-sequence models for open-domain dialogue response generation and propose a new method to evaluate the extent to which such models are able to generate responses that reflect different personality traits."><td>46</td><td>Automatic Evaluation of Neural Personality-based Chatbots</td><td>0.556</td></tr><tr class="has-abs" data-abs="The popularity of image sharing on social media and the engagement it creates between users reflect the important role that visual context plays in everyday conversations. We present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiplereference dataset of crowd-sourced, eventcentric conversations on images. IGC falls on the continuum between chit-chat and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialog research."><td>47</td><td>Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation</td><td>0.556</td></tr><tr class="has-abs" data-abs="Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts' evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single &quot;overall score&quot; of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation. 1 Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summarisation models. arXiv preprint arXiv:2005.11739. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. ArXiv, abs/2004.05150."><td>48</td><td>A Critical Evaluation of Evaluations for Long-form Question Answering</td><td>0.557</td></tr><tr class="has-abs" data-abs="Generating diverse, interesting responses to chitchat conversations is a problem for neural conversational agents. This paper makes two substantial contributions to improving diversity in dialogue generation. First, we propose a novel metric which uses Natural Language Inference (NLI) to measure the semantic diversity of a set of model responses for a conversation. We evaluate this metric using an established framework (Tevet and Berant, 2021) and find strong evidence indicating NLI Diversity is correlated with semantic diversity. Specifically, we show that the contradiction relation is more useful than the neutral relation for measuring this diversity and that incorporating the NLI model's confidence achieves state-of-the-art results. Second, we demonstrate how to iteratively improve the semantic diversity of a sampled set of responses via a new generation procedure called Diversity Threshold Generation, which results in an average 137% increase in NLI Diversity compared to standard generation procedures."><td>49</td><td>Semantic Diversity in Dialogue with Natural Language Inference</td><td>0.557</td></tr><tr class="has-abs" data-abs="Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa."><td>50</td><td>CoQA: A Conversational Question Answering Challenge</td><td>0.558</td></tr><tr class="has-abs" data-abs="Dialogue state trackers have made significant progress on benchmark datasets, but their generalization capability to novel and realistic scenarios beyond the heldout conversations is less understood. We propose controllable counterfactuals (COCO) to bridge this gap and evaluate dialogue state tracking (DST) models on novel scenarios, i.e., would the system successfully tackle the request if the user responded differently but still consistently with the dialogue flow? COCO leverages turn-level belief states as counterfactual conditionals to produce novel conversation scenarios in two steps: (i) counterfactual goal generation at turnlevel by dropping and adding slots followed by replacing slot values, (ii) counterfactual conversation generation that is conditioned on (i) and consistent with the dialogue flow. Evaluating state-of-the-art DST models on MultiWOZ dataset with COCO-generated counterfactuals results in a significant performance drop of up to 30.8% (from 49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used techniques like paraphrasing only affect the accuracy by at most 2%. Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models."><td>51</td><td>COCO: CONTROLLABLE COUNTERFACTUALS FOR EVALUATING DIALOGUE STATE TRACKERS</td><td>0.558</td></tr><tr class="has-abs" data-abs="Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018)  demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model's scalability by conducting tests on the CoQA dataset. 1"><td>52</td><td>Fluent Response Generation for Conversational Question Answering</td><td>0.559</td></tr><tr class="has-abs" data-abs="The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation."><td>53</td><td>Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4</td><td>0.560</td></tr><tr class="has-abs" data-abs="Conversational question-answer generation is a task that automatically generates a largescale conversational question answering dataset based on input passages. In this paper, we introduce a novel framework that extracts questionworthy phrases from a passage and then generates corresponding questions considering previous conversations. In particular, our framework revises the extracted answers after generating questions so that answers exactly match paired questions. Experimental results show that our simple answer revision approach leads to significant improvement in the quality of synthetic data. Moreover, we prove that our framework can be effectively utilized for domain adaptation of conversational question answering."><td>54</td><td>Conversational QA Dataset Generation with Answer Revision</td><td>0.560</td></tr><tr class="has-abs" data-abs="The idea behind this proposal is to investigate the possibility of utilizing NLP tools, statistical topic modeling techniques and freely available online resources to propose a system able to provide dialogue contribution suggestions which are relevant to the context, yet out of the main activity of the dialogue (i.e. off-activity talk). The aim is to evaluate the effects of a tool that automatically suggests offactivity talks in form of some sentences relevant to the dialogue context. The evaluation is to be done over two test-sets of open domain and closed-domain in a conversational quiz-like setting. The outcome of this work will be a satisfactory point of entry to investigate the hypothesis that adding automatically generated offactivity talks feature to a conversational agent can lead to building up engagement of the dialogue partner(s)."><td>55</td><td>Thesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents</td><td>0.561</td></tr><tr class="has-abs" data-abs="We investigate the impact of search strategies in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a modelbased Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics: log-probabilities assigned by the model and utterance diversity. Our experiments reveal that better search algorithms lead to higher rated conversations. However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.Pierre-Emmanuel Mazaré, Samuel Humeau, MartinRaison, and Antoine Bordes. 2018. Training millions of personalized dialogue agents. arXiv preprint arXiv:1809.01984."><td>56</td><td>Importance of Search and Evaluation Strategies in Neural Dialogue Modeling</td><td>0.561</td></tr><tr class="has-abs" data-abs="There has been a massive surge of Natural Language Generation (NLG) models in the recent years, accelerated by deep learning and the availability of large-scale datasets. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions: (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing metrics? (v) What are the possible future directions of research?"><td>57</td><td>A Tutorial on Evaluation Metrics used in Natural Language Generation</td><td>0.561</td></tr><tr class="has-abs" data-abs="Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as incontext examples to synthesize a social conversation dataset using prompting 1 . We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multiparty conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset. * Work done during internship at Amazon Alexa AI 1 https://github.com/alexa/PLACES Yeah, I grew up there."><td>58</td><td>PLACES: Prompting Language Models for Social Conversation Synthesis</td><td>0.561</td></tr><tr class="has-abs" data-abs="With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. The new results that we bring come to reinforce methods of the current state of the art."><td>59</td><td>Towards a more Robust Evaluation for Conversational Question Answering</td><td>0.562</td></tr><tr class="has-abs" data-abs="End-to-end evaluations of conversational dialogue systems with naive users are currently uncovering severe usability problems that result in low task completion rates. Preliminary analyses suggest that these problems are related to the system's dialogue management and turntaking behavior. We present the results of experiments designed to take a detailed look at the effects of that behavior. Based on the resulting findings, we spell out a set of criteria which lie orthogonal to dialogue quality, but nevertheless constitute an integral part of a more comprehensive view on dialogue felicity as a function of dialogue quality and efficiency."><td>60</td><td>The Tao of CHI: Towards Effective Human-Computer Interaction</td><td>0.562</td></tr><tr class="has-abs" data-abs="Despite recent progress in open-domain dialogue evaluation, how to develop automatic metrics remains an open problem. We explore the potential of dialogue evaluation featuring dialog act information, which was hardly explicitly modeled in previous methods."><td>61</td><td>FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows</td><td>0.563</td></tr><tr class="has-abs" data-abs="We present three enhancements to existing encoder-decoder models for open-domain conversational agents, aimed at effectively modeling coherence and promoting output diversity: (1) We introduce a measure of coherence as the GloVe embedding similarity between the dialogue context and the generated response, (2) we filter our training corpora based on the measure of coherence to obtain topically coherent and lexically diverse context-response pairs, (3) we then train a response generator using a conditional variational autoencoder model that incorporates the measure of coherence as a latent variable and uses a context gate to guarantee topical consistency with the context and promote lexical diversity. Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity."><td>62</td><td>Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity</td><td>0.563</td></tr><tr class="has-abs" data-abs="To make the Human Computer Interaction more user friendly and persona aligned, detection of user persona is of utmost significance. Towards achieving this objective, we describe a novel approach to select the persona of a user from pre-determine list of personas and utilize it to generate personalized responses. This is achieved in two steps. Firstly, closest matching persona is detected from a set of predetermined persona for the user. The second step involves the use of a fine-tuned natural language generation (NLG) model to generate persona compliant responses. Through experiments, we demonstrate that the proposed architecture generates better responses than current approaches by using a detected persona. Experimental evaluation on the PersonaChat dataset has demonstrated notable performance in terms of perplexity and F1-score."><td>63</td><td>PAR: Persona Aware Response in Conversational Systems</td><td>0.564</td></tr><tr class="has-abs" data-abs="Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, Blended-SkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task."><td>64</td><td>Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills</td><td>0.564</td></tr><tr class="has-abs" data-abs="Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent's persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person's speech and better emulate them in generated responses. This work introduces the GENERATIVE CONVERSATION CONTROL model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor's persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control. * First two authors have contributed equally. † Research conducted during an internship at NVIDIA. . 2015.A neural network approach to context-sensitive generation of conversational responses. arXiv preprint arXiv:1506.06714. . 2018. Augmenting end-to-end dialogue systems with commonsense knowledge. In Thirty-Second AAAI Conference on Artificial Intelligence."><td>65</td><td>Large Scale Multi-Actor Generative Dialog Modeling</td><td>0.564</td></tr><tr class="has-abs" data-abs="Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pretrained language models.However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances.In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training.Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task.Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation (r = 0.9) with human ratings among 11 chatbots.Code and pre-trained models will be public. 1"><td>66</td><td>Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances</td><td>0.564</td></tr><tr class="has-abs" data-abs="Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to taskoriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful opendomain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches."><td>67</td><td>SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues</td><td>0.565</td></tr><tr class="has-abs" data-abs="User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the simulator directly impacts the RL policy. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these simulators both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems. 1"><td>68</td><td>How to Build User Simulators to Train RL-based Dialog Systems</td><td>0.565</td></tr><tr class="has-abs" data-abs="An important aspect of developing dialogue systems is how to evaluate and compare the performance of different systems. Existing automatic evaluation metrics are based on turnlevel quality evaluation and use average scores for system-level comparison. In this paper, we propose to measure the performance of a dialogue system by computing the distributionwise distance between its generated conversations and real-world conversations. Specifically, two distribution-wise metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics."><td>69</td><td>Assessing Dialogue Systems with Distribution Distances</td><td>0.565</td></tr><tr class="has-abs" data-abs="In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically &quot;generate and hope&quot; generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction. * Joint first authors."><td>70</td><td>OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS</td><td>0.565</td></tr><tr class="has-abs" data-abs="Evaluation metrics shine the light on the best models and thus strongly influence the research directions, such as the recently developed dialogue metrics USR, FED, and GRADE. However, most current metrics evaluate the dialogue data as isolated and static because they only focus on a single quality or several qualities. To mitigate the problem, this paper proposes an interpretable, multi-faceted, and controllable framework IM 2 (Interpretable and M ulti-category Integrated M etric) to combine a large number of metrics which are good at measuring different qualities. The IM 2 framework first divides current popular dialogue qualities into different categories and then applies or proposes dialogue metrics to measure the qualities within each category and finally generates an overall IM 2 score. An initial version of IM 2 was submitted to the AAAI 2022 Track5.1@DSTC10 challenge 1 and took the 2 nd place on both of the development and test leaderboard. After the competition, we develop more metrics and improve the performance of our model. We compare IM 2 with other 13 current dialogue metrics and experimental results show that IM 2 correlates more strongly with human judgments than any of them on each evaluated dataset 2 . * Corresponding author: Dongning Rao. 1  The full name of Track5.1@DSTC10 is Automatic Evaluation and Moderation of Open-domain Dialogue Systems (subtask 1) on the AAAI DSTC-10 (Dialog System Technology Challenges 2022) challenge. The Leaderboard: https://chateval.org/dstc10. 2 Our code and data are available at: https://github.com/Jnunlplab/IM2."><td>71</td><td>IM 2 : an Interpretable and M ulti-category Integrated M etric Framework for Automatic Dialogue Evaluation</td><td>0.566</td></tr><tr class="has-abs" data-abs="One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others' feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EMPATHETICDIALOGUES, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy retraining of the full model."><td>72</td><td>Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset</td><td>0.566</td></tr><tr class="has-abs" data-abs="We present on-going work of evaluating the, to our knowledge, first large generative language model trained to converse in Swedish, using data from the online discussion forum Flashback. We conduct a human evaluation pilot study that indicates the model is often able to respond to conversations in both a human-like and informative manner, on a diverse set of topics. While data from online forums can be useful to build conversational systems, we reflect on the negative consequences that incautious application might have, and the need for taking active measures to safeguard against them.2. Do you think that the last message adds information to the discussion?References"><td>73</td><td>Building a Swedish Open-Domain Conversational Language Model</td><td>0.566</td></tr><tr class="has-abs" data-abs="Recently, with the advent of high-performance generative language models, artificial agents that communicate directly with the users have become more human-like. This development allows users to perform a diverse range of trials with the agents, and the responses are sometimes displayed online by users who share or show-off their experiences. In this study, we explore dialogues with a social chatbot uploaded to an online community, with the aim of understanding how users game human-like agents and display their conversations. Having done this, we assert that user postings can be investigated from two aspects, namely conversation topic and purpose of testing, and suggest a categorization scheme for the analysis. We analyze 639 dialogues to develop an annotation protocol for the evaluation, and measure the agreement to demonstrate the validity. We find that the dialogue content does not necessarily reflect the purpose of testing, and also that users come up with creative strategies to game the agent without being penalized."><td>74</td><td>Evaluating How Users Game and Display Conversation with Human-Like Agents</td><td>0.566</td></tr><tr class="has-abs" data-abs="(no abstract)"><td>75</td><td>Management and Evaluation of Interactive Dialog in the Air Travel Domain</td><td>0.567</td></tr><tr class="has-abs" data-abs="There has been a significant investment in dialog systems (tools and runtime) for building conversational systems by major companies including Google, IBM, Microsoft, and Amazon. The question remains whether these tools are up to the task of building conversational, task-oriented dialog applications at the enterprise level. In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development: accuracy, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers. In this paper, we provide both quantitative and qualitative results in three main areas: natural language understanding, dialog, and text generation. While existing toolsets were all incomplete, we hope this paper will provide a roadmap of where they need to go to meet the goal of building effective dialog systems."><td>76</td><td>Are the Tools up to the Task? An evaluation of commercial dialog tools in developing conversational enterprise-grade dialog systems</td><td>0.568</td></tr><tr class="has-abs" data-abs="We present a new method based on episodic Knowledge Graphs (eKGs) for evaluating (multimodal)  conversational agents in open domains. This graph is generated by interpreting raw signals during conversation and is able to capture the accumulation of knowledge over time. We apply structural and semantic analysis of the resulting graphs and translate the properties into qualitative measures. We compare these measures with existing automatic and manual evaluation metrics commonly used for conversational agents. Our results show that our Knowledge-Graph-based evaluation provides more qualitative insights into interaction and the agent's behavior."><td>77</td><td>Evaluating Agent Interactions Through Episodic Knowledge Graphs</td><td>0.568</td></tr><tr class="has-abs" data-abs="Inspired by recent work in meta-learning and generative teaching networks, we propose a framework called Generative Conversational Networks, in which conversational agents learn to generate their own labelled training data (given some seed data) and then train themselves from that data to perform a given task. We use reinforcement learning to optimize the data generation process where the reward signal is the agent's performance on the task. The task can be any language-related task, from intent detection to full task-oriented conversations. In this work, we show that our approach is able to generalise from seed data and performs well in limited data and limited computation settings, with significant gains for intent detection and slot tagging across multiple datasets: ATIS, TOD, SNIPS, and Restau-rants8k. We show an average improvement of 35% in intent detection and 21% in slot tagging over a baseline model trained from the seed data. We also conduct an analysis of the novelty of the generated data and provide generated examples for intent detection, slot tagging, and non-goal oriented conversations."><td>78</td><td>Generative Conversational Networks</td><td>0.569</td></tr><tr class="has-abs" data-abs="Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the wellperformed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability 1 ."><td>79</td><td>DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering</td><td>0.570</td></tr><tr class="has-abs" data-abs="The notorious one-to-many nature of opendomain dialogues poses huge challenges for automatic evaluation methods. Recent studies attempt to mitigate this issue by considering the similarity of the generated response with the conversational context and design discriminative models to learn from multiple positive responses. Despite the promising results, they can not be applied to general scenarios where training data with multiple responses is unavailable. To this end, in this paper, we propose a self-supervised setting to obtain a smooth latent space that can both capture discourse-level context information and implicitly model more references in latent space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines."><td>80</td><td>Enhancing the Open-Domain Dialogue Evaluation in Latent Space</td><td>0.570</td></tr><tr class="has-abs" data-abs="Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses."><td>81</td><td>Evaluating Coherence in Dialogue Systems using Entailment</td><td>0.570</td></tr><tr class="has-abs" data-abs="Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development."><td>82</td><td>Evaluation Metrics for Generation</td><td>0.571</td></tr><tr class="has-abs" data-abs="Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models."><td>83</td><td>Automating Human Evaluation of Dialogue Systems</td><td>0.571</td></tr><tr class="has-abs" data-abs="We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems."><td>84</td><td>How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</td><td>0.571</td></tr><tr class="has-abs" data-abs="Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems' logs. However, the validity of these automatic measures has not been fully proven. In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model."><td>85</td><td>Assessing Dialog System User Simulation Evaluation Measures Using Human Judges</td><td>0.572</td></tr><tr class="has-abs" data-abs="We will discuss an approach to dialogue act generation that reflects the multidimensionality of communication. Dialogue acts from different dimensions in the taxonomy used are generated in parallel, resulting in a buffer of candidates. The main focus of the paper will be on an additional process of evaluating these candidates, resulting in definitive combinations of dialogue acts to be generated. This evaluation process is required to deal with the interdependencies there still are between dimensions, and involves logical, strategic and pragmatic considerations."><td>86</td><td>Evaluating combinations of dialogue acts for generation</td><td>0.573</td></tr><tr class="has-abs" data-abs="With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal(LeCun et al., 2015). However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.DESIDERATA FOR THE EVALUATION OF MACHINE INTELLIGENCERather than trying to define intelligence in abstract terms, we take a pragmatic approach: we would like to develop AIs that are useful for us. This naturally leads to the following desiderata.Communication through natural language An AI will be useful to us only if we are able to communicate with it: assigning it tasks, understanding the information it returns, and teaching it new skills. Since natural language is by far the easiest way for us to communicate, we require our useful AI to be endowed with basic linguistic abilities. The language the machine is exposed to in the testing environment will inevitably be very limited. However, given that we want the machine to also be a powerful, fast learner (see next point), humans should later be able to teach it more sophisticated language skills as they become important to instruct the machine in new domains. In concrete, the environment should not only expose the machine to a set of tasks, but provide instructions and feedback about the tasks in simple natural language. The machine should rely on this form of linguistic interaction to efficiently solve the tasks."><td>87</td><td>COMMAI: EVALUATING THE FIRST STEPS TOWARDS A USEFUL GENERAL AI</td><td>0.573</td></tr><tr class="has-abs" data-abs="Recent progress in dialogue generation has inspired a number of studies on dialogue systems that are capable of accomplishing tasks through natural language interactions. A promising direction among these studies is the use of reinforcement learning techniques, such as self-play, for training dialogue agents. However, current datasets are limited in size, and the environment for training agents and evaluating process is relatively unsophisticated. We present AirDialogue, a large dataset that contains 402,038 goal-oriented conversations. To collect this dataset, we create a contextgenerator which provides travel and flight restrictions. We then ask human annotators to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions. Key to our environment is the ease of evaluating the success of the dialogue, which is achieved by using ground-truth states (e.g., the flight being booked) generated by the restrictions. Any dialogue agent that does not generate the correct states is considered to fail. Our experimental results indicate that state-of-the-art dialogue models on the test dataset can only achieve a scaled score of 0.22 and an exact match score of 0.1 while humans can reach a score of 0.94 and 0.93 respectively, which suggests significant opportunities for future improvement."><td>88</td><td>AirDialogue: An Environment for Goal-Oriented Dialogue Research</td><td>0.573</td></tr><tr class="has-abs" data-abs="Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose P 2 BOT, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P 2 BOT incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, PERSONA-CHAT, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-theart baselines across both automatic metrics and human evaluations."><td>89</td><td>You Impress Me: Dialogue Generation via Mutual Persona Perception</td><td>0.573</td></tr><tr class="has-abs" data-abs="We present a semi-automated framework for constructing factoid question answering (QA) datasets, where an array of question characteristics are formalized, including structure complexity, function, commonness, answer cardinality, and paraphrasing. Instead of collecting questions and manually characterizing them, we employ a reverse procedure, first generating a kind of graph-structured logical forms from a knowledge base, and then converting them into questions. Our work is the first to generate questions with explicitly specified characteristics for QA evaluation. We construct a new QA dataset with over 5,000 logical form-question pairs, associated with answers from the knowledge base, and show that datasets constructed in this way enable finegrained analyses of QA systems. The dataset can be found in https://github.com/ ysu1989/GraphQuestions."><td>90</td><td>On Generating Characteristic-rich Question Sets for QA Evaluation</td><td>0.573</td></tr><tr class="has-abs" data-abs="Open-domain dialog systems (i.e., chatbots) are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure standardization and transparency. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https://chateval.org."><td>91</td><td>ChatEval: A Tool for Chatbot Evaluation</td><td>0.573</td></tr><tr class="has-abs" data-abs="Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike translation, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the single turn setting. We add self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequenceto-sequence models with explicit lengthpromotion. A back-off strategy produces better responses overall, in the full spectrum of lengths. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency)."><td>92</td><td>Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models</td><td>0.574</td></tr><tr class="has-abs" data-abs="Synthesizing datasets for conversational question answering (CQA) from unlabeled documents remains challenging due to its interactive nature. Moreover, while modeling information needs is an essential key, only few studies have discussed it. In this paper, we introduce a novel framework, SIMSEEK, (Simulating information-Seeking conversation from unlabeled documents), and compare its two variants. In our baseline SIMSEEK-SYM, a questioner generates follow-up questions upon the predetermined answer by an answerer. On the contrary, SIMSEEK-ASYM first generates the question and then finds its corresponding answer under the conversational context. Our experiments show that they can synthesize effective training resources for CQA and conversational search tasks. As a result, conversations from SIMSEEK-ASYM not only make more improvements in our experiments but also are favorably reviewed in a human evaluation. We finally release a large-scale resource of synthetic conversations, WIKI-SIMSEEK, containing 2 million CQA pairs built upon Wikipedia documents. With the dataset, our CQA model achieves the state-of-the-art performance on a recent CQA benchmark, QuAC (Choi et al., 2018) 1 ."><td>93</td><td>Generating Information-Seeking Conversations from Unlabeled Documents</td><td>0.574</td></tr><tr class="has-abs" data-abs="This paper presents an automatic corpus-based process to author an open-domain conversational strategy usable both in chatterbot systems and as a fallback strategy for out-of-domain human utterances. Our approach is implemented on a corpus of television drama subtitles. This system is used as a chatterbot system to collect a corpus of 41 open-domain textual dialogues with 27 human participants. The general capabilities of the system are studied through objective measures and subjective self-reports in terms of understandability, repetition and coherence of the system responses selected in reaction to human utterances. Subjective evaluations of the collected dialogues are presented with respect to amusement, engagement and enjoyability. The main factors influencing those dimensions in our chatterbot experiment are discussed."><td>94</td><td>Purely Corpus-based Automatic Conversation Authoring</td><td>0.574</td></tr><tr class="has-abs" data-abs="User attributes provide rich and useful information for user understanding, yet structured and easy-to-use attributes are often sparsely populated. In this paper, we leverage dialogues with conversational agents, which contain strong suggestions of user information, to automatically extract user attributes. Since no existing dataset is available for this purpose, we apply distant supervision to train our proposed two-stage attribute extractor, which surpasses several retrieval and generation baselines on human evaluation. Meanwhile, we discuss potential applications (e.g., personalized recommendation and dialogue systems) of such extracted user attributes, and point out current limitations to cast light on future work."><td>95</td><td>Getting To Know You: User Attribute Extraction from Dialogues</td><td>0.575</td></tr><tr class="has-abs" data-abs="Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialoglevel annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialoglevel attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms."><td>96</td><td>Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs</td><td>0.575</td></tr><tr class="has-abs" data-abs="Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."><td>97</td><td>Recipes for building an open-domain chatbot</td><td>0.575</td></tr><tr class="has-abs" data-abs="Open-domain dialog systems are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing chatbot tools, and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure evaluation is performed in a standardized and transparent way. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https: //chateval.org."><td>98</td><td>ChatEval: A Tool for the Systematic Evaluation of Chatbots</td><td>0.576</td></tr><tr class="has-abs" data-abs="Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue, a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically. We define the task, highlight key desired aspects of a good GFQ, and propose a model that satisfies these. Finally, we provide an evaluation by human annotators of our generated questions compared against human generated ones, demonstrating competitive performance."><td>99</td><td>Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment</td><td>0.576</td></tr><tr class="has-abs" data-abs="Language models for text-to-image generation can output good quality images when referential aspects of pictures are evaluated. The generation of creative images is not under scrutiny at the moment, but it poses interesting challenges: should we expect more creative images using more creative prompts? What is the relationship between prompts and images in the global process of human evaluation? In this paper, we want to highlight several criteria that should be taken into account for building a creative text-to-image generation benchmark, collecting insights from multiple disciplines (e.g., linguistics, cognitive psychology, philosophy, psychology of art)."><td>100</td><td>Creative Text-to-Image Generation: Suggestions for a Benchmark</td><td>0.576</td></tr></tbody></table>
  </div>
  <div class="card">
    <h2>After&nbsp;(augmented)</h2>
    <table id="afterTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody></tbody></table>
  </div>
</section>

<!-- METRICS ------------------------------------------------------------->
<section id="metricsCard" class="card" style="">
  <h2>Metrics</h2>
  <div id="recallLine" class="small">Recall </div>

  <div class="tabs">
    <button id="tab1" class="">Δ recall</button>
    <button id="tab2" class="active">Rank maps</button>
  </div>

  <div id="body1" class="tab-body active">
    <p id="deltaLine" class="small"></p>
  </div>
  <div id="body2" class="tab-body active">
    <h3 class="small" style="margin-top:0;">ranks_before</h3>
    <pre id="rb" class="small">{
  "258547050": 9999999999
}</pre>
    <h3 class="small">ranks_after</h3>
    <pre id="ra" class="small"></pre>
  </div>
</section>




```
</body></html>