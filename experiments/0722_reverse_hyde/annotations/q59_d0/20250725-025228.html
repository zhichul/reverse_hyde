<!doctype html>
<html lang="en"><head></head><body>```html



<meta charset="utf-8">
<title>Prompt‑HyDE playground</title>
<style>
  body{font-family:system-ui,sans-serif;margin:0;padding:1rem 2rem;}
  h2{margin-top:2rem;}
  label{display:block;margin-top:.75rem;font-weight:600;}
  select,input[type=text],input[type=number],textarea{width:100%;padding:.4rem;}
  textarea{height:120px;font-family:monospace;}
  button{margin-top:1rem;padding:.5rem 1rem;cursor:pointer;}
  table{border-collapse:collapse;width:100%;margin-top:.5rem;}
  th,td{border:1px solid #ccc;padding:.25rem .5rem;text-align:left;vertical-align:top;}
  em.rev{font-style:italic;color:#9146ff;}
  strong.rel{color:#0b63ff;font-weight:700;}
  .flex{display:flex;gap:2rem;flex-wrap:wrap;}
  .card{flex:1 1 320px;border:1px solid #ddd;padding:1rem;}
  .small{font-size:.85rem;color:#555;margin:.25rem 0;}

  /* tooltip for abstracts */
  .has-abs{position:relative;}
  .has-abs:hover::after{
    content:attr(data-abs);
    position:absolute;left:0;top:100%;z-index:999;
    max-width:420px;white-space:pre-wrap;font-size:.8rem;
    background:#333;color:#fff;padding:.5rem;border-radius:.25rem;
    box-shadow:0 2px 6px rgba(0,0,0,.35);
  }

  /* tab styling */
  .tabs{display:flex;gap:.5rem;margin-bottom:.5rem;}
  .tabs button{padding:.25rem .75rem;border:1px solid #aaa;background:#eee;}
  .tabs button.active{background:#fff;border-bottom:none;font-weight:600;}
  .tab-body{display:none;}
  .tab-body.active{display:block;}
</style>


<h1>Prompt‑HyDE UI (vanilla&nbsp;JS)</h1>

<!-- CONFIG -------------------------------------------------------------->
<section class="card">
  <h2>Server configuration</h2>
  <pre id="cfg" class="small">{
  "embed_model": "grit",
  "index_path": "../0721_litsearch_example/faiss/litsearch.index",
  "backend": "faiss",
  "query_dataset": "../0721_litsearch_example/query_with_score.parquet",
  "corpus_dataset": "../0721_litsearch_example/corpus_clean_dedup.parquet",
  "prompt_dir": "prompts",
  "extractor_dir": "extractors",
  "annotation_dir": "annotations",
  "ui_config": "configs/ui_config.json",
  "host": "0.0.0.0",
  "port": 8200,
  "id_field": "corpusid",
  "relevant_documents_field": "corpusids"
}</pre>
</section>

<section class="flex">
  <!-- DATA CARD -->
  <div class="card" style="max-width:440px;">
    <h2>Data</h2>

    <label>Query</label>
    <select id="querySel"><option value="0">0: [inline_acl] sp=0 qual=2 gr=0 — Are there any research papers on methods to compre…</option><option value="3">3: [inline_acl] sp=1 qual=2 gr=0 — Are there any tools or studies that have focused o…</option><option value="4">4: [inline_acl] sp=1 qual=2 gr=0 — Are there papers that propose contextualized calib…</option><option value="10">10: [inline_acl] sp=1 qual=2 gr=0 — Can you point me to a work that uses diagnostic to…</option><option value="11">11: [inline_acl] sp=0 qual=1 gr=0 — Can you point me to studies discussing methods for…</option><option value="17">17: [inline_acl] sp=1 qual=2 gr=0 — Can you recommend a paper that uses an NLI model f…</option><option value="24">24: [inline_acl] sp=0 qual=1 gr=0 — Can you suggest recent studies that have integrate…</option><option value="30">30: [inline_acl] sp=0 qual=1 gr=0 — Could you point me to studies that have investigat…</option><option value="35">35: [inline_acl] sp=1 qual=2 gr=0 — Could you suggest a paper that introduces an appro…</option><option value="39">39: [inline_acl] sp=1 qual=2 gr=0 — I am looking for research that has explored topic …</option><option value="44">44: [inline_acl] sp=1 qual=2 gr=0 — I'm exploring research that utilizes large dataset…</option><option value="49">49: [inline_acl] sp=1 qual=2 gr=0 — I'm looking for innovative approaches to data anno…</option><option value="55">55: [inline_acl] sp=1 qual=2 gr=0 — In discourse parsing literature, which works have …</option><option value="56">56: [inline_acl] sp=1 qual=2 gr=0 — In researching metrics for human-interaction with …</option><option value="58">58: [inline_acl] sp=1 qual=2 gr=0 — In the context of Named Entity Recognition tasks a…</option><option value="59">59: [inline_acl] sp=0 qual=1 gr=0 — In the context of machine translation, can you poi…</option><option value="61">61: [inline_acl] sp=1 qual=2 gr=0 — In the context of simultaneous machine translation…</option><option value="62">62: [inline_acl] sp=1 qual=2 gr=0 — In the field of reinforcement learning models for …</option><option value="63">63: [inline_acl] sp=1 qual=2 gr=0 — What approaches have been used to address the limi…</option><option value="71">71: [inline_acl] sp=1 qual=2 gr=0 — What are the recent developments in evaluating the…</option><option value="75">75: [inline_acl] sp=0 qual=1 gr=0 — What research could I reference to understand the …</option><option value="84">84: [inline_acl] sp=1 qual=2 gr=0 — Where can I find a multilingual corpus that includ…</option><option value="109">109: [inline_nonacl] sp=1 qual=2 gr=0 — Can you recommend research that uses an LLM to gen…</option><option value="110">110: [inline_nonacl] sp=0 qual=1 gr=0 — Can you show me a paper that built a large structu…</option><option value="111">111: [inline_nonacl] sp=1 qual=2 gr=0 — Can you suggest research that deals with the multi…</option><option value="119">119: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that examines how inco…</option><option value="123">123: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that explores how lang…</option><option value="124">124: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores mitigati…</option><option value="126">126: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that explores the diff…</option><option value="128">128: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that initializes embed…</option><option value="132">132: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend a study that investigates enha…</option><option value="135">135: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="136">136: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates how …</option><option value="137">137: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates how …</option><option value="142">142: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that investigates the …</option><option value="143">143: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend a study that investigates the …</option><option value="145">145: [inline_nonacl] sp=0 qual=2 gr=0 — Could you recommend a study that uses feedback-dri…</option><option value="149">149: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research papers that investiga…</option><option value="155">155: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that evaluates the pe…</option><option value="157">157: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that examines how dec…</option><option value="163">163: [inline_nonacl] sp=1 qual=2 gr=0 — Could you recommend research that explores how the…</option><option value="168">168: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates app…</option><option value="169">169: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates gen…</option><option value="176">176: [inline_nonacl] sp=1 qual=1 gr=0 — Could you recommend research that investigates the…</option><option value="179">179: [inline_nonacl] sp=0 qual=1 gr=0 — Could you recommend research that proposed enhanci…</option><option value="194">194: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest a study that explores data annot…</option><option value="195">195: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that explores employing …</option><option value="197">197: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest a study that explores the idea o…</option><option value="199">199: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest a study that investigates the in…</option><option value="205">205: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest papers that tackle conversationa…</option><option value="208">208: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that concentrates on pi…</option><option value="209">209: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines a system …</option><option value="213">213: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that examines how stran…</option><option value="221">221: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that examines the effec…</option><option value="224">224: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores generatin…</option><option value="226">226: [inline_nonacl] sp=0 qual=2 gr=0 — Could you suggest research that explores the idea …</option><option value="228">228: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that includes an online…</option><option value="231">231: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest research that investigates apply…</option><option value="238">238: [inline_nonacl] sp=1 qual=2 gr=0 — Could you suggest research that investigates how c…</option><option value="244">244: [inline_nonacl] sp=1 qual=1 gr=0 — Could you suggest research that investigates the u…</option><option value="251">251: [inline_nonacl] sp=0 qual=1 gr=0 — Could you suggest studies focused on emotion-class…</option><option value="252">252: [inline_nonacl] sp=1 qual=2 gr=0 — Has any research explored using other off-the-shel…</option><option value="260">260: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research efforts been made to gather dial…</option><option value="261">261: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers been published on models …</option><option value="265">265: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers examined whether using la…</option><option value="269">269: [inline_nonacl] sp=0 qual=2 gr=0 — Have any research papers investigated the creation…</option><option value="272">272: [inline_nonacl] sp=1 qual=2 gr=0 — Have any research papers tried to create conversat…</option><option value="278">278: [inline_nonacl] sp=1 qual=2 gr=0 — I know about prompt tuning, but have any works tri…</option><option value="297">297: [inline_nonacl] sp=1 qual=2 gr=0 — What are some scholarly articles that explore the …</option><option value="306">306: [inline_nonacl] sp=0 qual=1 gr=0 — What papers discuss the effect of false negatives …</option><option value="307">307: [inline_nonacl] sp=1 qual=2 gr=0 — What papers explore replacing schema linking with …</option><option value="311">311: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists comparing adapter-based tunin…</option><option value="312">312: [inline_nonacl] sp=1 qual=1 gr=0 — What research exists on employing generative model…</option><option value="325">325: [inline_nonacl] sp=0 qual=1 gr=0 — What research is available on acquiring sentence e…</option><option value="326">326: [inline_nonacl] sp=0 qual=2 gr=0 — What research is available on hybrid approaches th…</option><option value="331">331: [inline_nonacl] sp=0 qual=1 gr=0 — What techniques and frameworks have been suggested…</option><option value="346">346: [inline_nonacl] sp=1 qual=1 gr=0 — Which work shows that only emplying instance-level…</option><option value="347">347: [inline_nonacl] sp=1 qual=2 gr=0 — Which work shows that reducing the number of train…</option><option value="353">353: [manual_acl] sp=1 qual=2 gr=0 — Are there any papers that build dense retrievers w…</option><option value="360">360: [manual_acl] sp=1 qual=2 gr=0 — If one would like to train (or evaluate) a helpful…</option><option value="364">364: [manual_acl] sp=1 qual=2 gr=0 — Is there a decoder-only language model that does n…</option><option value="366">366: [manual_acl] sp=1 qual=2 gr=0 — Is there a method for measuring the critical error…</option><option value="369">369: [manual_acl] sp=0 qual=1 gr=0 — Is there a paper exploring the curse of multilingu…</option><option value="372">372: [manual_acl] sp=0 qual=2 gr=0 — Is there a paper that links exposure bias to disti…</option><option value="378">378: [manual_acl] sp=1 qual=2 gr=0 — Is there a paper that uses similarity scores to ch…</option><option value="382">382: [manual_acl] sp=1 qual=2 gr=0 — Is there a tool that can automatically segment spe…</option><option value="389">389: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that aligns speech and text emb…</option><option value="395">395: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that constructs augmented train…</option><option value="404">404: [manual_acl] sp=0 qual=2 gr=0 — Is there any paper that proposes a set of criteria…</option><option value="411">411: [manual_acl] sp=1 qual=1 gr=0 — Is there any paper that utilizes Gaussian processe…</option><option value="414">414: [manual_acl] sp=1 qual=1 gr=0 — Is there any research paper that can extract attri…</option><option value="417">417: [manual_acl] sp=1 qual=2 gr=0 — Is there any works that explores how to achieve ba…</option><option value="423">423: [manual_acl] sp=0 qual=2 gr=0 — What are some methods for solving the class-increm…</option><option value="426">426: [manual_acl] sp=0 qual=1 gr=0 — What is the performance of large language models i…</option><option value="433">433: [manual_acl] sp=1 qual=2 gr=0 — Which is the first multimodal model combining text…</option><option value="451">451: [manual_acl] sp=1 qual=2 gr=0 — Which paper first constructed a structured knowled…</option><option value="458">458: [manual_acl] sp=1 qual=1 gr=0 — Which paper first proposed shared adapter module a…</option><option value="467">467: [manual_acl] sp=0 qual=1 gr=0 — Which paper found that mutual learning benefits mu…</option><option value="473">473: [manual_acl] sp=1 qual=1 gr=0 — Which paper is among the earliest to train on exte…</option><option value="475">475: [manual_acl] sp=0 qual=1 gr=0 — Which paper makes sure that the questions used in …</option><option value="498">498: [manual_acl] sp=1 qual=2 gr=0 — Which research paper leverages event structure inf…</option><option value="502">502: [manual_acl] sp=1 qual=2 gr=0 — Which work discusses an analysis of source and tar…</option><option value="510">510: [manual_iclr] sp=1 qual=2 gr=0 — Are there any papers that use a world model for pl…</option><option value="522">522: [manual_iclr] sp=1 qual=2 gr=0 — Is there a parameter-efficient fine-tuning method …</option><option value="550">550: [manual_iclr] sp=1 qual=1 gr=0 — What paper first showed that you can score the cod…</option><option value="557">557: [manual_iclr] sp=0 qual=1 gr=0 — What paper proposes breaking down programming prob…</option><option value="560">560: [manual_iclr] sp=1 qual=1 gr=0 — What research first proposed a new kind of cascade…</option><option value="564">564: [manual_iclr] sp=1 qual=2 gr=0 — Which backdoor paper first used the CLIP to suppre…</option><option value="566">566: [manual_iclr] sp=1 qual=1 gr=0 — Which is one of the first papers to highlight and …</option><option value="595">595: [manual_iclr] sp=0 qual=2 gr=0 — What paper provides generalization bounds for self…</option><option value="18">18: [inline_acl] sp=0 qual=2 gr=0.25 — Can you recommend some literature that focuses on …</option><option value="2">2: [inline_acl] sp=0 qual=2 gr=0.5 — Are there any studies that explore post-hoc techni…</option><option value="12">12: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me to studies that explore the impac…</option><option value="13">13: [inline_acl] sp=1 qual=2 gr=0.5 — Can you point me towards research on contrastive l…</option><option value="53">53: [inline_acl] sp=1 qual=2 gr=0.5 — I'm researching on the efficacy of recurrent netwo…</option><option value="74">74: [inline_acl] sp=1 qual=2 gr=0.5 — What prior works suggested that exposure bias coul…</option><option value="92">92: [inline_acl] sp=0 qual=1 gr=0.5 — Where might I find research on the evaluation of c…</option><option value="102">102: [inline_nonacl] sp=1 qual=2 gr=0.5 — Are there any studies investigating example-based …</option><option value="113">113: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you direct me to studies investigating the e…</option><option value="150">150: [inline_nonacl] sp=0 qual=1 gr=0.5 — Could you recommend research that analyses prompt …</option><option value="156">156: [inline_nonacl] sp=0 qual=2 gr=0.5 — Could you recommend research that examines how an …</option><option value="235">235: [inline_nonacl] sp=1 qual=1 gr=0.5 — Could you suggest research that investigates enhan…</option><option value="257">257: [inline_nonacl] sp=0 qual=2 gr=0.5 — Have any new metrics been developed to assess the …</option><option value="328">328: [inline_nonacl] sp=0 qual=2 gr=0.5 — What research should I consult regarding the appli…</option><option value="334">334: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques exist to enhance the few-shot fine…</option><option value="335">335: [inline_nonacl] sp=0 qual=2 gr=0.5 — What techniques have been investigated to enhance …</option><option value="96">96: [inline_acl] sp=0 qual=2 gr=0.6 — Which studies should I look into that have explore…</option><option value="8">8: [inline_acl] sp=0 qual=1 gr=0.6666666666666666 — Can you list some publications that discuss the ev…</option><option value="23">23: [inline_acl] sp=0 qual=2 gr=0.6666666666666666 — Can you suggest literature on enhanced semantic pa…</option><option value="1">1: [inline_acl] sp=1 qual=2 gr=1 — Are there any resources available for translating …</option><option value="5">5: [inline_acl] sp=1 qual=2 gr=1 — Are there studies that combine convolutional and r…</option><option value="6">6: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to research that explores method…</option><option value="7">7: [inline_acl] sp=0 qual=2 gr=1 — Can you direct me to studies that explore techniqu…</option><option value="9">9: [inline_acl] sp=0 qual=2 gr=1 — Can you point me to a paper that discussed transfo…</option><option value="14">14: [inline_acl] sp=0 qual=1 gr=1 — Can you point to studies or tasks focused on detec…</option><option value="15">15: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a conversational QA dataset wher…</option><option value="16">16: [inline_acl] sp=1 qual=2 gr=1 — Can you recommend a foundational paper that provid…</option><option value="19">19: [inline_acl] sp=1 qual=2 gr=1 — Can you refer me to research that adapts the conce…</option><option value="20">20: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest a corpus that contains French ency…</option><option value="21">21: [inline_acl] sp=0 qual=2 gr=1 — Can you suggest any literature that explores the i…</option><option value="22">22: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest literature on a dataset that categ…</option><option value="25">25: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some literature that evaluates the…</option><option value="26">26: [inline_acl] sp=1 qual=2 gr=1 — Can you suggest some recent datasets that have bee…</option><option value="27">27: [inline_acl] sp=1 qual=2 gr=1 — Could you direct me towards a study that explores …</option><option value="28">28: [inline_acl] sp=1 qual=2 gr=1 — Could you point me to research on binary classific…</option><option value="29">29: [inline_acl] sp=0 qual=2 gr=1 — Could you point me to studies that discuss the dev…</option><option value="31">31: [inline_acl] sp=1 qual=1 gr=1 — Could you point me toward some large-scale multili…</option><option value="32">32: [inline_acl] sp=1 qual=2 gr=1 — Could you provide me with a reference that discuss…</option><option value="33">33: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend datasets that include SQL anno…</option><option value="34">34: [inline_acl] sp=1 qual=2 gr=1 — Could you recommend studies that provide a baselin…</option><option value="36">36: [inline_acl] sp=0 qual=1 gr=1 — Could you suggest studies that employ novel method…</option><option value="37">37: [inline_acl] sp=1 qual=2 gr=1 — Has there been any recent work or competitions foc…</option><option value="38">38: [inline_acl] sp=0 qual=1 gr=1 — I am exploring state-of-the-art techniques in lang…</option><option value="40">40: [inline_acl] sp=0 qual=2 gr=1 — I am looking to understand more about sequence-to-…</option><option value="41">41: [inline_acl] sp=1 qual=2 gr=1 — I would like to understand the theoretical basis f…</option><option value="42">42: [inline_acl] sp=1 qual=2 gr=1 — I'm conducting research on computational humor and…</option><option value="43">43: [inline_acl] sp=1 qual=1 gr=1 — I'm exploring efficient transformer architectures …</option><option value="45">45: [inline_acl] sp=0 qual=2 gr=1 — I'm exploring ways to enhance question answering s…</option><option value="46">46: [inline_acl] sp=1 qual=2 gr=1 — I'm interested in understanding how perplexity is …</option><option value="47">47: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a comprehensive dataset that has b…</option><option value="48">48: [inline_acl] sp=1 qual=2 gr=1 — I'm looking for a paper that discusses improvement…</option><option value="50">50: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into morphological embedding algorithm…</option><option value="51">51: [inline_acl] sp=1 qual=2 gr=1 — I'm looking into the distillation process of langu…</option><option value="52">52: [inline_acl] sp=0 qual=1 gr=1 — I'm researching insertion-based decoding methods f…</option><option value="54">54: [inline_acl] sp=1 qual=1 gr=1 — I'm searching for studies that explore advancement…</option><option value="57">57: [inline_acl] sp=1 qual=2 gr=1 — In the area of argument mining, could you point to…</option><option value="60">60: [inline_acl] sp=1 qual=2 gr=1 — In the context of natural language processing, I a…</option><option value="64">64: [inline_acl] sp=1 qual=1 gr=1 — What are some approaches to generating sports news…</option><option value="65">65: [inline_acl] sp=1 qual=2 gr=1 — What are some good datasets for conversational que…</option><option value="66">66: [inline_acl] sp=1 qual=2 gr=1 — What are some of the key papers to look at for und…</option><option value="67">67: [inline_acl] sp=0 qual=2 gr=1 — What are some recent advancements in training syst…</option><option value="68">68: [inline_acl] sp=1 qual=2 gr=1 — What are some soft-constrained methods proposed in…</option><option value="69">69: [inline_acl] sp=1 qual=2 gr=1 — What are some studies that leverage statistical ma…</option><option value="70">70: [inline_acl] sp=0 qual=2 gr=1 — What are some techniques or tools used in machine …</option><option value="72">72: [inline_acl] sp=1 qual=2 gr=1 — What paper should I look at if I am interested in …</option><option value="73">73: [inline_acl] sp=1 qual=2 gr=1 — What papers should I refer to if I want to explore…</option><option value="76">76: [inline_acl] sp=1 qual=2 gr=1 — What research has been done on annotating user com…</option><option value="77">77: [inline_acl] sp=0 qual=1 gr=1 — What research has been done on improving named ent…</option><option value="78">78: [inline_acl] sp=1 qual=1 gr=1 — What research should I explore to understand metho…</option><option value="79">79: [inline_acl] sp=1 qual=2 gr=1 — When using pretrained transformer models for gener…</option><option value="80">80: [inline_acl] sp=0 qual=1 gr=1 — Where can I find a corpus of CCG annotations for n…</option><option value="81">81: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a detailed discussion on automati…</option><option value="82">82: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a discourse treebank tailored to …</option><option value="83">83: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a large corpus of annotated socia…</option><option value="85">85: [inline_acl] sp=1 qual=2 gr=1 — Where can I find a paper that discusses annotating…</option><option value="86">86: [inline_acl] sp=0 qual=1 gr=1 — Where can I find guidelines on standard practices …</option><option value="87">87: [inline_acl] sp=0 qual=1 gr=1 — Where can I find information on self-attentive par…</option><option value="88">88: [inline_acl] sp=0 qual=1 gr=1 — Where can I find interdisciplinary research that i…</option><option value="89">89: [inline_acl] sp=1 qual=2 gr=1 — Where can I find multilingual datasets used for th…</option><option value="90">90: [inline_acl] sp=1 qual=2 gr=1 — Where can I find research about automatic evaluati…</option><option value="91">91: [inline_acl] sp=0 qual=1 gr=1 — Where might I find a dataset annotated specificall…</option><option value="93">93: [inline_acl] sp=1 qual=2 gr=1 — Which corpora are frequently used in research to b…</option><option value="94">94: [inline_acl] sp=1 qual=2 gr=1 — Which paper specifies the typical configurations u…</option><option value="95">95: [inline_acl] sp=0 qual=1 gr=1 — Which papers should I refer to for learning about …</option><option value="97">97: [inline_acl] sp=1 qual=2 gr=1 — Which work should I explore to understand the tech…</option><option value="98">98: [inline_nonacl] sp=1 qual=1 gr=1 — *Could you suggest a dataset with legally or ethic…</option><option value="99">99: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any papers on training video-language mo…</option><option value="100">100: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any recent papers investigating the use …</option><option value="101">101: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any research papers investigating the im…</option><option value="103">103: [inline_nonacl] sp=1 qual=2 gr=1 — Are there any studies investigating sentiment anal…</option><option value="104">104: [inline_nonacl] sp=1 qual=1 gr=1 — Are there any studies on incorporating external co…</option><option value="105">105: [inline_nonacl] sp=1 qual=2 gr=1 — Are there studies examining how well question answ…</option><option value="106">106: [inline_nonacl] sp=1 qual=1 gr=1 — Are there studies that investigate debiasing langu…</option><option value="107">107: [inline_nonacl] sp=0 qual=1 gr=1 — Can you give me a paper that does self-supervised …</option><option value="108">108: [inline_nonacl] sp=0 qual=1 gr=1 — Can you recommend a dialogue summarization dataset…</option><option value="112">112: [inline_nonacl] sp=1 qual=2 gr=1 — Could you direct me to research that evaluates few…</option><option value="114">114: [inline_nonacl] sp=1 qual=1 gr=1 — Could you point me to research that tackles the is…</option><option value="115">115: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a contemporary research paper …</option><option value="116">116: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a paper that builds a writing …</option><option value="117">117: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that does data-augment…</option><option value="118">118: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that examines how cros…</option><option value="120">120: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that examines the intr…</option><option value="121">121: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores a pre-tr…</option><option value="122">122: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that explores employin…</option><option value="125">125: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that explores strategi…</option><option value="127">127: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that explores the impr…</option><option value="129">129: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates empl…</option><option value="130">130: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates empl…</option><option value="131">131: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates enha…</option><option value="133">133: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates grap…</option><option value="134">134: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates guid…</option><option value="138">138: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend a study that investigates inco…</option><option value="139">139: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend a study that investigates know…</option><option value="140">140: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend a study that investigates repr…</option><option value="141">141: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates text…</option><option value="144">144: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend a study that investigates the …</option><option value="146">146: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend articles that explore the role…</option><option value="147">147: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research articles that explore…</option><option value="148">148: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research papers that explore a…</option><option value="151">151: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that assesses how wel…</option><option value="152">152: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that assesses how wel…</option><option value="153">153: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that assesses techniq…</option><option value="154">154: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that employs a relaxe…</option><option value="158">158: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how mul…</option><option value="159">159: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines how opt…</option><option value="160">160: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend research that examines how syn…</option><option value="161">161: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the cha…</option><option value="162">162: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that examines the eff…</option><option value="164">164: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that explores identif…</option><option value="165">165: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that has introduced a…</option><option value="166">166: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that improves knowled…</option><option value="167">167: [inline_nonacl] sp=0 qual=2 gr=1 — Could you recommend research that introduces a met…</option><option value="170">170: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="171">171: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates how…</option><option value="172">172: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates mer…</option><option value="173">173: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates met…</option><option value="174">174: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates tec…</option><option value="175">175: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="177">177: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend research that investigates the…</option><option value="178">178: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend research that investigates usi…</option><option value="180">180: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend scholarly articles that invest…</option><option value="181">181: [inline_nonacl] sp=1 qual=1 gr=1 — Could you recommend studies on hierarchical modeli…</option><option value="182">182: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that concentrate on an…</option><option value="183">183: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies that investigate fine-…</option><option value="184">184: [inline_nonacl] sp=1 qual=2 gr=1 — Could you recommend studies that tackle the issue …</option><option value="185">185: [inline_nonacl] sp=0 qual=1 gr=1 — Could you recommend studies which explore how to o…</option><option value="186">186: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a dataset containing diverse, in…</option><option value="187">187: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a dataset for question-answering…</option><option value="188">188: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a research article that explores…</option><option value="189">189: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study examining how transforme…</option><option value="190">190: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that evaluates cross-enc…</option><option value="191">191: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that examines how well c…</option><option value="192">192: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a cohesive…</option><option value="193">193: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a study that explores a compress…</option><option value="196">196: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores improved t…</option><option value="198">198: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest a study that explores the use of…</option><option value="200">200: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a study that proposes high-param…</option><option value="201">201: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest a thorough comparative analysis …</option><option value="202">202: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest a triplet-formatted structured d…</option><option value="203">203: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest an article that leverages the sp…</option><option value="204">204: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest datasets that can benchmark LLM …</option><option value="206">206: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research on detecting common err…</option><option value="207">207: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that assesses if langua…</option><option value="210">210: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how coref…</option><option value="211">211: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="212">212: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how promp…</option><option value="214">214: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that examines how the o…</option><option value="215">215: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="216">216: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines how well …</option><option value="217">217: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines how well …</option><option value="218">218: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that examines the appli…</option><option value="219">219: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that examines the chall…</option><option value="220">220: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that examines the diffi…</option><option value="222">222: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores a pre-tra…</option><option value="223">223: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores employing…</option><option value="225">225: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that explores the drawb…</option><option value="227">227: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that explores the impro…</option><option value="229">229: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates a clu…</option><option value="230">230: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates acros…</option><option value="232">232: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates aspec…</option><option value="233">233: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates effic…</option><option value="234">234: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates emplo…</option><option value="236">236: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates expan…</option><option value="237">237: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates graph…</option><option value="239">239: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how h…</option><option value="240">240: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that investigates how m…</option><option value="241">241: [inline_nonacl] sp=0 qual=2 gr=1 — Could you suggest research that investigates how n…</option><option value="242">242: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates how u…</option><option value="243">243: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that investigates impro…</option><option value="245">245: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that investigates train…</option><option value="246">246: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that offers an in-depth…</option><option value="247">247: [inline_nonacl] sp=1 qual=2 gr=1 — Could you suggest research that shows multilingual…</option><option value="248">248: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest research that trains language mo…</option><option value="249">249: [inline_nonacl] sp=0 qual=1 gr=1 — Could you suggest research that tries to interpret…</option><option value="250">250: [inline_nonacl] sp=1 qual=1 gr=1 — Could you suggest some work that develops multimod…</option><option value="253">253: [inline_nonacl] sp=1 qual=2 gr=1 — Has any research tried to mitigate overfitting in …</option><option value="254">254: [inline_nonacl] sp=1 qual=1 gr=1 — Has any study explored the zero-shot extraction of…</option><option value="255">255: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any research that uses multiple mod…</option><option value="256">256: [inline_nonacl] sp=1 qual=2 gr=1 — Has there been any work that improves the work on …</option><option value="258">258: [inline_nonacl] sp=1 qual=2 gr=1 — Have any papers tried to address the background-sh…</option><option value="259">259: [inline_nonacl] sp=1 qual=2 gr=1 — Have any recent publications explored the use of n…</option><option value="262">262: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers collected feedback from r…</option><option value="263">263: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers critically analyzed the p…</option><option value="264">264: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers examined the efficacy of …</option><option value="266">266: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers explored methods to impro…</option><option value="267">267: [inline_nonacl] sp=0 qual=1 gr=1 — Have any research papers introduced a dedicated pr…</option><option value="268">268: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers investigated human capaci…</option><option value="270">270: [inline_nonacl] sp=1 qual=2 gr=1 — Have any research papers suggested methods for sum…</option><option value="271">271: [inline_nonacl] sp=1 qual=1 gr=1 — Have any research papers suggested techniques for …</option><option value="273">273: [inline_nonacl] sp=1 qual=1 gr=1 — Have any studies explored the creation of memory m…</option><option value="274">274: [inline_nonacl] sp=1 qual=2 gr=1 — Have there been any advancements in language model…</option><option value="275">275: [inline_nonacl] sp=1 qual=2 gr=1 — How can I locate a dataset containing toxic senten…</option><option value="276">276: [inline_nonacl] sp=1 qual=2 gr=1 — How can SQL-to-text be utilized to improve text-to…</option><option value="277">277: [inline_nonacl] sp=0 qual=1 gr=1 — How can dense retrieval models for open-domain que…</option><option value="279">279: [inline_nonacl] sp=1 qual=2 gr=1 — In multi-hop question answering, is there a paper …</option><option value="280">280: [inline_nonacl] sp=1 qual=2 gr=1 — Is it possible to adatp named entity recognition s…</option><option value="281">281: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a benchmark designed to assess language m…</option><option value="282">282: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a comprehensive dataset available for sum…</option><option value="283">283: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a dataset available for open-domain targe…</option><option value="284">284: [inline_nonacl] sp=0 qual=1 gr=1 — Is there a dataset containing question-answer pair…</option><option value="285">285: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a research paper that has developed a cus…</option><option value="286">286: [inline_nonacl] sp=1 qual=1 gr=1 — Is there a specialized question answering dataset …</option><option value="287">287: [inline_nonacl] sp=1 qual=2 gr=1 — Is there a study that investigates if large langua…</option><option value="288">288: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any paper that tried fine-tuning mBERT to…</option><option value="289">289: [inline_nonacl] sp=1 qual=2 gr=1 — Is there any research that investigates how to use…</option><option value="290">290: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research examining if multilingual pre-tr…</option><option value="291">291: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research on a specialized language model …</option><option value="292">292: [inline_nonacl] sp=1 qual=2 gr=1 — Is there research that argues for transparency and…</option><option value="293">293: [inline_nonacl] sp=1 qual=1 gr=1 — Is there research that investigates embedding mult…</option><option value="294">294: [inline_nonacl] sp=1 qual=2 gr=1 — Is there work on text classification that explores…</option><option value="295">295: [inline_nonacl] sp=1 qual=2 gr=1 — What approaches have been suggested to lower the c…</option><option value="296">296: [inline_nonacl] sp=1 qual=1 gr=1 — What are some scholarly articles that explore scal…</option><option value="298">298: [inline_nonacl] sp=0 qual=2 gr=1 — What are some studies that explore data-poisoning …</option><option value="299">299: [inline_nonacl] sp=1 qual=1 gr=1 — What are the latest advancements in predicting sui…</option><option value="300">300: [inline_nonacl] sp=0 qual=1 gr=1 — What are the latest developments in conversational…</option><option value="301">301: [inline_nonacl] sp=0 qual=1 gr=1 — What benchmarks have prior research utilized to as…</option><option value="302">302: [inline_nonacl] sp=1 qual=2 gr=1 — What concerns or key points have been highlighted …</option><option value="303">303: [inline_nonacl] sp=0 qual=2 gr=1 — What difficulties do neural conversational models …</option><option value="304">304: [inline_nonacl] sp=1 qual=2 gr=1 — What literature is available on training semantic …</option><option value="305">305: [inline_nonacl] sp=1 qual=2 gr=1 — What methods exist for tailoring news suggestions …</option><option value="308">308: [inline_nonacl] sp=1 qual=1 gr=1 — What recent developments in transformer architectu…</option><option value="309">309: [inline_nonacl] sp=1 qual=1 gr=1 — What recent research has been conducted on improvi…</option><option value="310">310: [inline_nonacl] sp=1 qual=1 gr=1 — What research articles should I consult to underst…</option><option value="313">313: [inline_nonacl] sp=0 qual=1 gr=1 — What research exists on incorporating knowledge gr…</option><option value="314">314: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on leveraging syntactic roles…</option><option value="315">315: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on the impact of scaling on p…</option><option value="316">316: [inline_nonacl] sp=1 qual=1 gr=1 — What research exists on using reinforcement learni…</option><option value="317">317: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on applying contr…</option><option value="318">318: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on creating neura…</option><option value="319">319: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on determining th…</option><option value="320">320: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on enhancing conv…</option><option value="321">321: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on incorporating …</option><option value="322">322: [inline_nonacl] sp=1 qual=1 gr=1 — What research has been conducted on news recommend…</option><option value="323">323: [inline_nonacl] sp=0 qual=1 gr=1 — What research has been conducted on scaling within…</option><option value="324">324: [inline_nonacl] sp=0 qual=2 gr=1 — What research has been conducted on the impact of …</option><option value="327">327: [inline_nonacl] sp=1 qual=1 gr=1 — What research is available on the concept of using…</option><option value="329">329: [inline_nonacl] sp=1 qual=2 gr=1 — What resources or toolkits are available to facili…</option><option value="330">330: [inline_nonacl] sp=1 qual=1 gr=1 — What sources offer research on maintaining factual…</option><option value="332">332: [inline_nonacl] sp=0 qual=1 gr=1 — What techniques exist for efficiently fine-tuning …</option><option value="333">333: [inline_nonacl] sp=0 qual=2 gr=1 — What techniques exist for incorporating context in…</option><option value="336">336: [inline_nonacl] sp=1 qual=1 gr=1 — Where can I find a database of good prompts to use…</option><option value="337">337: [inline_nonacl] sp=0 qual=1 gr=1 — Where can I read about the using soft embeddings t…</option><option value="338">338: [inline_nonacl] sp=1 qual=2 gr=1 — Which method involves training additional prompt t…</option><option value="339">339: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper has conducted a thorough analysis of h…</option><option value="340">340: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper introduced the task of creating extend…</option><option value="341">341: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper presents a platform that emphasizes ev…</option><option value="342">342: [inline_nonacl] sp=0 qual=1 gr=1 — Which paper shows that generated captions of model…</option><option value="343">343: [inline_nonacl] sp=1 qual=2 gr=1 — Which paper shows that human experts and non-exper…</option><option value="344">344: [inline_nonacl] sp=0 qual=1 gr=1 — Which work introduces sparse attention modules and…</option><option value="345">345: [inline_nonacl] sp=1 qual=1 gr=1 — Which work pushes the limit of model quantization …</option><option value="348">348: [inline_nonacl] sp=1 qual=1 gr=1 — Which work suggests that machine translation model…</option><option value="349">349: [inline_nonacl] sp=1 qual=1 gr=1 — Which works shows that training large language mod…</option><option value="350">350: [inline_nonacl] sp=1 qual=2 gr=1 — ould you direct me to research that shows that the…</option><option value="351">351: [manual_acl] sp=1 qual=1 gr=1 — Are there any examples of using dense phrase retri…</option><option value="352">352: [manual_acl] sp=1 qual=1 gr=1 — Are there any large-scale and open-source text sim…</option><option value="354">354: [manual_acl] sp=1 qual=1 gr=1 — Could you recommend a dataset paper which presents…</option><option value="355">355: [manual_acl] sp=1 qual=1 gr=1 — Find the NLP paper that focuses on dialogue genera…</option><option value="356">356: [manual_acl] sp=0 qual=1 gr=1 — Give me a paper proposing to circumvent a single-t…</option><option value="357">357: [manual_acl] sp=1 qual=2 gr=1 — How to achieve zero-shot lip reading?…</option><option value="358">358: [manual_acl] sp=1 qual=2 gr=1 — How to better attract readers to news articles by …</option><option value="359">359: [manual_acl] sp=0 qual=1 gr=1 — How to faithfully and explicitly measure the helpf…</option><option value="361">361: [manual_acl] sp=1 qual=1 gr=1 — In multimodal (multilingual) abstractive summariza…</option><option value="362">362: [manual_acl] sp=1 qual=1 gr=1 — Is there a Chinese hate speech paper that construc…</option><option value="363">363: [manual_acl] sp=0 qual=1 gr=1 — Is there a dataset that allows to perform aspect-b…</option><option value="365">365: [manual_acl] sp=1 qual=1 gr=1 — Is there a dialogue dataset where a speaker's utte…</option><option value="367">367: [manual_acl] sp=1 qual=2 gr=1 — Is there a method that measures the information pr…</option><option value="368">368: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper comparing knowledge distillation …</option><option value="370">370: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that applies large language model…</option><option value="371">371: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that connects the basic elements …</option><option value="373">373: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that shows that language models' …</option><option value="374">374: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that supports the use of automate…</option><option value="375">375: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that uses Explainable AI techniqu…</option><option value="376">376: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses an app for a popular ta…</option><option value="377">377: [manual_acl] sp=0 qual=1 gr=1 — Is there a paper that uses evolutionary algorithms…</option><option value="379">379: [manual_acl] sp=1 qual=1 gr=1 — Is there a paper that uses the tree structure of m…</option><option value="380">380: [manual_acl] sp=1 qual=2 gr=1 — Is there a paper that utilizes the characteristics…</option><option value="381">381: [manual_acl] sp=1 qual=2 gr=1 — Is there a study that shows how to help the demons…</option><option value="383">383: [manual_acl] sp=1 qual=1 gr=1 — Is there an evaluation metric for natural language…</option><option value="384">384: [manual_acl] sp=1 qual=1 gr=1 — Is there any dataset that contains minimally-contr…</option><option value="385">385: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper about style transfer for storie…</option><option value="386">386: [manual_acl] sp=0 qual=2 gr=1 — Is there any paper exploring real speakers and thu…</option><option value="387">387: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper leverages knowledge distillatio…</option><option value="388">388: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that address attacks on code mo…</option><option value="390">390: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that applies curriculum learnin…</option><option value="391">391: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that applies symbolic distillat…</option><option value="392">392: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that attempts to evaluate the s…</option><option value="393">393: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that automatically creates a da…</option><option value="394">394: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that combines causal inference …</option><option value="396">396: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that employs code LLMs to itera…</option><option value="397">397: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores and annotates the…</option><option value="398">398: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that explores using only an enc…</option><option value="399">399: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that investigates backdoor atta…</option><option value="400">400: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that leverages graph neural net…</option><option value="401">401: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that leverages syntactic rules …</option><option value="402">402: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that performs adversarial train…</option><option value="403">403: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that proposes a new multimodal …</option><option value="405">405: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that reveals annotation problem…</option><option value="406">406: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that studies a teacher AI infer…</option><option value="407">407: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that tries to investigate LLMs’…</option><option value="408">408: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that uses data collected from t…</option><option value="409">409: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses prompt tuning in mult…</option><option value="410">410: [manual_acl] sp=1 qual=2 gr=1 — Is there any paper that uses token-level loss to e…</option><option value="412">412: [manual_acl] sp=1 qual=1 gr=1 — Is there any paper that utilizes graph structure t…</option><option value="413">413: [manual_acl] sp=0 qual=1 gr=1 — Is there any paper that utilizes masked language m…</option><option value="415">415: [manual_acl] sp=1 qual=2 gr=1 — Is there any work that allows large numbers of mod…</option><option value="416">416: [manual_acl] sp=1 qual=1 gr=1 — Is there any work that attacks language models in …</option><option value="418">418: [manual_acl] sp=1 qual=2 gr=1 — Is there commonsense reasoning dataset which gener…</option><option value="419">419: [manual_acl] sp=1 qual=2 gr=1 — Is there such a factuality evaluation dataset that…</option><option value="420">420: [manual_acl] sp=1 qual=2 gr=1 — Is there such a reading comprehension dataset in u…</option><option value="421">421: [manual_acl] sp=1 qual=1 gr=1 — Provide an example of a paper which proposes a met…</option><option value="422">422: [manual_acl] sp=1 qual=2 gr=1 — What are some data-efficient ways to learn text em…</option><option value="424">424: [manual_acl] sp=0 qual=1 gr=1 — What is a large event-coverage general-domain even…</option><option value="425">425: [manual_acl] sp=1 qual=1 gr=1 — What is the first paper to address the problem of …</option><option value="427">427: [manual_acl] sp=1 qual=2 gr=1 — What limitations do large language models have in …</option><option value="428">428: [manual_acl] sp=1 qual=2 gr=1 — What paper compares humans' and language models' n…</option><option value="429">429: [manual_acl] sp=1 qual=2 gr=1 — What work attempts to explore multi-hop reasoning …</option><option value="430">430: [manual_acl] sp=1 qual=1 gr=1 — Which article first proposed shuffled-group-whiten…</option><option value="431">431: [manual_acl] sp=1 qual=1 gr=1 — Which dataset supports narration generation and te…</option><option value="432">432: [manual_acl] sp=0 qual=1 gr=1 — Which family of model generally perform the best f…</option><option value="434">434: [manual_acl] sp=1 qual=2 gr=1 — Which knowledge graph completion method focuses on…</option><option value="435">435: [manual_acl] sp=1 qual=1 gr=1 — Which language model distillation paper that first…</option><option value="436">436: [manual_acl] sp=1 qual=2 gr=1 — Which numerical reasoning paper first published a …</option><option value="437">437: [manual_acl] sp=1 qual=2 gr=1 — Which paper about parameter-efficient finetuning f…</option><option value="438">438: [manual_acl] sp=1 qual=1 gr=1 — Which paper combines the advantages of different f…</option><option value="439">439: [manual_acl] sp=0 qual=1 gr=1 — Which paper did a comprehensive survey of the code…</option><option value="440">440: [manual_acl] sp=1 qual=2 gr=1 — Which paper employs a two-stage approach in genera…</option><option value="441">441: [manual_acl] sp=1 qual=1 gr=1 — Which paper enables interactive semantic parsing b…</option><option value="442">442: [manual_acl] sp=1 qual=1 gr=1 — Which paper explored training a GPT-2 for automati…</option><option value="443">443: [manual_acl] sp=1 qual=1 gr=1 — Which paper first aggregates statements to represe…</option><option value="444">444: [manual_acl] sp=1 qual=2 gr=1 — Which paper first applied the chain-of-thought tec…</option><option value="445">445: [manual_acl] sp=1 qual=2 gr=1 — Which paper first apply mixture of experts idea to…</option><option value="446">446: [manual_acl] sp=1 qual=2 gr=1 — Which paper first attempts to take potential depen…</option><option value="447">447: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines different methods for u…</option><option value="448">448: [manual_acl] sp=1 qual=1 gr=1 — Which paper first combines rewriting and expansion…</option><option value="449">449: [manual_acl] sp=1 qual=1 gr=1 — Which paper first conducted the positioned error t…</option><option value="450">450: [manual_acl] sp=1 qual=1 gr=1 — Which paper first construct large-scale corpus to …</option><option value="452">452: [manual_acl] sp=1 qual=1 gr=1 — Which paper first explored In-context learning in …</option><option value="453">453: [manual_acl] sp=1 qual=1 gr=1 — Which paper first found that multilingual models c…</option><option value="454">454: [manual_acl] sp=1 qual=2 gr=1 — Which paper first introduced document content as a…</option><option value="455">455: [manual_acl] sp=1 qual=1 gr=1 — Which paper first propose to mask positions to pre…</option><option value="456">456: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed a cross-domain language…</option><option value="457">457: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed extracting the pair of …</option><option value="459">459: [manual_acl] sp=1 qual=1 gr=1 — Which paper first proposed to combine pretrained m…</option><option value="460">460: [manual_acl] sp=0 qual=1 gr=1 — Which paper first proposed to only update some ori…</option><option value="461">461: [manual_acl] sp=1 qual=1 gr=1 — Which paper first published a real-world Chinese-E…</option><option value="462">462: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that it is possible to mai…</option><option value="463">463: [manual_acl] sp=1 qual=2 gr=1 — Which paper first shows that large language models…</option><option value="464">464: [manual_acl] sp=1 qual=2 gr=1 — Which paper first studied the efficiency robustnes…</option><option value="465">465: [manual_acl] sp=1 qual=2 gr=1 — Which paper first use the attention weights to gui…</option><option value="466">466: [manual_acl] sp=0 qual=1 gr=1 — Which paper first used structural information for …</option><option value="468">468: [manual_acl] sp=0 qual=1 gr=1 — Which paper highlights the need for leveraging all…</option><option value="469">469: [manual_acl] sp=1 qual=2 gr=1 — Which paper introduce a DRO (distribution robust o…</option><option value="470">470: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduced the human-evaluated timelin…</option><option value="471">471: [manual_acl] sp=1 qual=1 gr=1 — Which paper introduces the R-GCN technique into do…</option><option value="472">472: [manual_acl] sp=1 qual=1 gr=1 — Which paper investigates the influence of the dive…</option><option value="474">474: [manual_acl] sp=0 qual=1 gr=1 — Which paper is the first to comprehensively review…</option><option value="476">476: [manual_acl] sp=1 qual=2 gr=1 — Which paper measured how well the source-translati…</option><option value="477">477: [manual_acl] sp=1 qual=2 gr=1 — Which paper presents an easy to implement and high…</option><option value="478">478: [manual_acl] sp=1 qual=2 gr=1 — Which paper produces a dataset for text simplifica…</option><option value="479">479: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed a learning-based data augment…</option><option value="480">480: [manual_acl] sp=1 qual=2 gr=1 — Which paper proposed decomposing the logit update …</option><option value="481">481: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed dictionary-based Bayesian inf…</option><option value="482">482: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposed the integration of human tran…</option><option value="483">483: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes a memory-efficient optimizer …</option><option value="484">484: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes the two-stage training method…</option><option value="485">485: [manual_acl] sp=1 qual=1 gr=1 — Which paper proposes to use rewriting based approa…</option><option value="486">486: [manual_acl] sp=1 qual=2 gr=1 — Which paper showed that social relationships were …</option><option value="487">487: [manual_acl] sp=1 qual=2 gr=1 — Which paper shows assessment of training instabili…</option><option value="488">488: [manual_acl] sp=1 qual=1 gr=1 — Which paper shows that in instruction tuning, the …</option><option value="489">489: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies how current retrieval systems …</option><option value="490">490: [manual_acl] sp=1 qual=2 gr=1 — Which paper studies the concept of enhancing the c…</option><option value="491">491: [manual_acl] sp=1 qual=2 gr=1 — Which paper surveyed the datasets and tasks of ask…</option><option value="492">492: [manual_acl] sp=1 qual=2 gr=1 — Which paper used both automatically generated and …</option><option value="493">493: [manual_acl] sp=1 qual=2 gr=1 — Which paper utilizes language models to generate s…</option><option value="494">494: [manual_acl] sp=1 qual=1 gr=1 — Which paper was the first to propose combining hum…</option><option value="495">495: [manual_acl] sp=0 qual=2 gr=1 — Which papers develop methods to make in-context le…</option><option value="496">496: [manual_acl] sp=0 qual=1 gr=1 — Which papers were among the first to explore the t…</option><option value="497">497: [manual_acl] sp=1 qual=1 gr=1 — Which pre-trained model is specifically designed f…</option><option value="499">499: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model can demonstrate that v…</option><option value="500">500: [manual_acl] sp=1 qual=2 gr=1 — Which vision-language model paper in 2023 develope…</option><option value="501">501: [manual_acl] sp=1 qual=2 gr=1 — Which was the first paper to explore the online ad…</option><option value="503">503: [manual_acl] sp=1 qual=1 gr=1 — Which work proposes an approach to improve candida…</option><option value="504">504: [manual_acl] sp=1 qual=2 gr=1 — what's the first paper that manages to handle KBQA…</option><option value="505">505: [manual_acl] sp=1 qual=1 gr=1 — which paper first focuses on addressing the over-s…</option><option value="506">506: [manual_iclr] sp=1 qual=1 gr=1 — Can we reduce visual tokens in vision transformers…</option><option value="507">507: [manual_iclr] sp=1 qual=1 gr=1 — Can we learn to represent an image with arbitary n…</option><option value="508">508: [manual_iclr] sp=1 qual=2 gr=1 — Are there any papers that construct convolutional …</option><option value="509">509: [manual_iclr] sp=1 qual=1 gr=1 — Are there any papers that study whether you can id…</option><option value="511">511: [manual_iclr] sp=1 qual=1 gr=1 — Are there datasets and benchmarks available for me…</option><option value="512">512: [manual_iclr] sp=1 qual=2 gr=1 — Are there sequential learning guarantees for confi…</option><option value="513">513: [manual_iclr] sp=1 qual=2 gr=1 — Can we find the solution of the Bilevel optimizati…</option><option value="514">514: [manual_iclr] sp=0 qual=2 gr=1 — Can you find a dataset that shows LLM-based evalua…</option><option value="515">515: [manual_iclr] sp=1 qual=2 gr=1 — Can you find a research paper that discusses using…</option><option value="516">516: [manual_iclr] sp=1 qual=1 gr=1 — I'm using Local SGD with a decaying learning rate …</option><option value="517">517: [manual_iclr] sp=1 qual=1 gr=1 — In video diffusion models, is there any paper that…</option><option value="518">518: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper illustrating that pre-trained tra…</option><option value="519">519: [manual_iclr] sp=1 qual=2 gr=1 — Is there a paper that takes a mixed machine learni…</option><option value="520">520: [manual_iclr] sp=1 qual=1 gr=1 — Is there a paper which applies Bayesian optimizati…</option><option value="521">521: [manual_iclr] sp=0 qual=1 gr=1 — Is there a paper which proposes a general data sel…</option><option value="523">523: [manual_iclr] sp=0 qual=1 gr=1 — Is there a single GNN model that can inductively g…</option><option value="524">524: [manual_iclr] sp=1 qual=2 gr=1 — Is there a theory paper that explains why sometime…</option><option value="525">525: [manual_iclr] sp=1 qual=1 gr=1 — Is there an existing dataset of images with alt-te…</option><option value="526">526: [manual_iclr] sp=1 qual=1 gr=1 — Is there any generalizable NeRF paper that disenta…</option><option value="527">527: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper applies off-shelf GPT-2 model i…</option><option value="528">528: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper improves adversarial training b…</option><option value="529">529: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that explores ways to parameter…</option><option value="530">530: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that previously proposed to con…</option><option value="531">531: [manual_iclr] sp=1 qual=1 gr=1 — Is there any paper that seamlessly integrates the …</option><option value="532">532: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that theoretically explains why…</option><option value="533">533: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper that uses Lipschitz continuity …</option><option value="534">534: [manual_iclr] sp=1 qual=2 gr=1 — Is there any paper trying to improve MLE for auto-…</option><option value="535">535: [manual_iclr] sp=1 qual=1 gr=1 — Name a paper which proposes a probabilsitic formul…</option><option value="536">536: [manual_iclr] sp=1 qual=2 gr=1 — What are some evaluation benchmarks for LLM privac…</option><option value="537">537: [manual_iclr] sp=1 qual=1 gr=1 — What are the key advantages of coupling neural SDE…</option><option value="538">538: [manual_iclr] sp=1 qual=2 gr=1 — What is a paper studying data being collected in b…</option><option value="539">539: [manual_iclr] sp=1 qual=1 gr=1 — What is the first paper that theoretically studies…</option><option value="540">540: [manual_iclr] sp=1 qual=2 gr=1 — What is the first paper that uses the generalized …</option><option value="541">541: [manual_iclr] sp=1 qual=1 gr=1 — What molecular representation learning paper intro…</option><option value="542">542: [manual_iclr] sp=1 qual=2 gr=1 — What open-source dataset combined knowledge retrie…</option><option value="543">543: [manual_iclr] sp=0 qual=1 gr=1 — What paper considers sensitive data issue when pro…</option><option value="544">544: [manual_iclr] sp=0 qual=1 gr=1 — What paper evaluated the ability of visual few-sho…</option><option value="545">545: [manual_iclr] sp=1 qual=1 gr=1 — What paper first adapted ControlNet to generate co…</option><option value="546">546: [manual_iclr] sp=1 qual=1 gr=1 — What paper first associate the modeling frequency …</option><option value="547">547: [manual_iclr] sp=1 qual=2 gr=1 — What paper first extends rotary positional encodin…</option><option value="548">548: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposed a robust perceptual simi…</option><option value="549">549: [manual_iclr] sp=1 qual=2 gr=1 — What paper first proposes that simply reversing th…</option><option value="551">551: [manual_iclr] sp=1 qual=1 gr=1 — What paper first used the technique of prompt engi…</option><option value="552">552: [manual_iclr] sp=1 qual=1 gr=1 — What paper first uses decoupled workers in distrib…</option><option value="553">553: [manual_iclr] sp=1 qual=2 gr=1 — What paper investigated the effect of the relative…</option><option value="554">554: [manual_iclr] sp=1 qual=1 gr=1 — What paper is the first to prove finetuned LLM can…</option><option value="555">555: [manual_iclr] sp=1 qual=1 gr=1 — What paper mitigates language model sampling error…</option><option value="556">556: [manual_iclr] sp=1 qual=2 gr=1 — What paper mitigates the vocabulary size limitatio…</option><option value="558">558: [manual_iclr] sp=1 qual=1 gr=1 — What paper showed first that one can build a fully…</option><option value="559">559: [manual_iclr] sp=0 qual=1 gr=1 — What paper shows that RLAIF can fully replace RLHF…</option><option value="561">561: [manual_iclr] sp=1 qual=1 gr=1 — What work first uses LLM to code robotic simulatio…</option><option value="562">562: [manual_iclr] sp=1 qual=2 gr=1 — What work proposes a model to learn a latent regul…</option><option value="563">563: [manual_iclr] sp=1 qual=1 gr=1 — What work proposes to combine video foundation mod…</option><option value="565">565: [manual_iclr] sp=1 qual=1 gr=1 — Which foundation model paper first proposed a time…</option><option value="567">567: [manual_iclr] sp=1 qual=1 gr=1 — Which machine learning paper proposed certified ro…</option><option value="568">568: [manual_iclr] sp=0 qual=1 gr=1 — Which multimodal large language model represents v…</option><option value="569">569: [manual_iclr] sp=1 qual=1 gr=1 — Which neural theorem proving paper first attempted…</option><option value="570">570: [manual_iclr] sp=1 qual=2 gr=1 — Which paper considers both weights and activations…</option><option value="571">571: [manual_iclr] sp=1 qual=1 gr=1 — Which paper contains quantitative results demonstr…</option><option value="572">572: [manual_iclr] sp=1 qual=1 gr=1 — Which paper examined the scalability of instructio…</option><option value="573">573: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first applied the chain of thought con…</option><option value="574">574: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first derived online occupany estimati…</option><option value="575">575: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first found that REINFORCE works bette…</option><option value="576">576: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first found that when transformers are…</option><option value="577">577: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first investigates the knowledge prefe…</option><option value="578">578: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first proposes a unified framework for…</option><option value="579">579: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first proved that wide-enough transfor…</option><option value="580">580: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first showed that task-specific knowle…</option><option value="581">581: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first studied differential privacy for…</option><option value="582">582: [manual_iclr] sp=1 qual=1 gr=1 — Which paper first study POMDP with enhanced feedba…</option><option value="583">583: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first tried to fine-tune LLMs with cha…</option><option value="584">584: [manual_iclr] sp=1 qual=2 gr=1 — Which paper first used language models to emulate …</option><option value="585">585: [manual_iclr] sp=1 qual=1 gr=1 — Which paper formally defines the problem of model …</option><option value="586">586: [manual_iclr] sp=1 qual=2 gr=1 — Which paper found that using common character enco…</option><option value="587">587: [manual_iclr] sp=1 qual=1 gr=1 — Which paper in human motion generation can control…</option><option value="588">588: [manual_iclr] sp=1 qual=2 gr=1 — Which paper is the first to model the helpfulness …</option><option value="589">589: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes an alignment framework that s…</option><option value="590">590: [manual_iclr] sp=1 qual=1 gr=1 — Which paper proposes to integrate black-box LLMs w…</option><option value="591">591: [manual_iclr] sp=1 qual=1 gr=1 — Which paper studies how difficult is a policy lear…</option><option value="592">592: [manual_iclr] sp=1 qual=2 gr=1 — Which paper trains on linear regression to hypothe…</option><option value="593">593: [manual_iclr] sp=1 qual=1 gr=1 — Which paper uses the latent diffusion model for th…</option><option value="594">594: [manual_iclr] sp=1 qual=2 gr=1 — Which paper utilized MMD flows with Riesz kernels …</option><option value="596">596: [manual_iclr] sp=1 qual=2 gr=1 — Which paper systematically examed the input mismat…</option></select>
    <p id="queryText" class="small has-abs" data-abs="">In the context of machine translation, can you point me towards literature discussing the specifications for setting up encoder/decoder layers, attention heads, and other hyperparameters for a neural network model?</p>
    <p id="queryMeta" class="small">set:inline_acl | spec:0 | qual:1 | gr:0</p>

    <label>Relevant document (rank&nbsp;+&nbsp;title)</label>
    <select id="docSel"><option value="0" data-id="216144650" title="Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both oneto-many and many-to-many settings, and improves zero-shot performance by ∼10 BLEU, approaching conventional pivot-based methods. 1">0: rank=∞  Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</option></select>
    <p id="docTitle" class="small has-abs" data-abs="Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both oneto-many and many-to-many settings, and improves zero-shot performance by ∼10 BLEU, approaching conventional pivot-based methods. 1">0: rank=∞  Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</p>
  </div>

  <!-- PROMPT / LLM / ANNOTATION CARD -->
  <div class="card" style="flex:2 1 520px;">
    <h2>Prompt &amp; LLM setup</h2>

    <label>Prompt</label>
    <div style="display:flex;gap:.5rem;">
      <select id="promptSel" style="flex:1;"><option value="dummy">dummy</option><option value="full_text">full_text</option><option value="title_abstract">title_abstract</option></select>
      <button id="newPromptBtn">+&nbsp;New</button>
    </div>

    <label>Extractor</label>
    <select id="extSel"><option value="dummy">dummy</option><option value="json_list_extractor">json_list_extractor</option></select>

    <label>k (top‑k retrieval)</label>
    <input id="kInp" type="number" value="50" min="1">

    <label>Prompt text</label>
    <textarea id="promptBox"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="savePromptBtn">💾&nbsp;Save</button>
      <button id="reloadPromptBtn">⟳&nbsp;Reload</button>
    </div>

    <details style="margin-top:.75rem;">
      <summary><strong>LLM config</strong></summary>
      <label>API key</label><input id="apiKey" type="text" placeholder="sk‑…">
      <label>Model</label><input id="model" type="text" value="gpt-4o-mini">
      <label>Temperature</label><input id="temp" type="number" value="0" step=".1">
      <label>Max tokens</label><input id="maxTok" type="number" value="2048">
      <label style="display:flex;align-items:center;gap:.5rem;margin-top:.5rem;">
        <input id="wantJson" type="checkbox"> Expect JSON object response
      </label>
    </details>

    <label>Your annotation</label>
    <textarea id="noteBox" placeholder="Add notes about this run…"></textarea>
    <div style="display:flex;gap:.5rem;">
      <button id="saveNoteBtn" style="background:#0b63ff;color:#fff;">Save&nbsp;annotation</button>
      <button id="runBtn" style="background:#14a44d;color:#fff;">Run</button>
    </div>
  </div>
</section>

<!-- BEFORE / AFTER TABLES ---------------------------------------------->
<section class="flex">
  <div class="card">
    <h2>Before&nbsp;(original)</h2>
    <table id="beforeTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody><tr class="has-abs" data-abs="With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language, we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important for the encoder side than for the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well."><td>1</td><td>How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</td><td>0.460</td></tr><tr class="has-abs" data-abs="Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments."><td>2</td><td>Massive Exploration of Neural Machine Translation Architectures</td><td>0.480</td></tr><tr class="has-abs" data-abs="In this paper, we propose a multi-hop attention for the Transformer. It refines the attention for an output symbol by integrating that of each head, and consists of two hops. The first hop attention is the scaled dot-product attention which is the same attention mechanism used in the original Transformer. The second hop attention is a combination of multi-layer perceptron (MLP) attention and head gate, which efficiently increases the complexity of the model by adding dependencies between heads. We demonstrate that the translation accuracy of the proposed multi-hop attention outperforms the baseline Transformer significantly, +0.85 BLEU point for the IWSLT-2017 German-to-English task and +2.58 BLEU point for the WMT-2017 German-to-English task. We also find that the number of parameters required for a multi-hop attention is smaller than that for stacking another self-attention layer and the proposed model converges significantly faster than the original Transformer."><td>3</td><td>Attention over Heads: A Multi-Hop Attention for Neural Machine Translation</td><td>0.484</td></tr><tr class="has-abs" data-abs="Tensor2Tensor is a library for deep learning models that is very well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.Neural Machine Translation BackgroundMachine translation using deep neural networks achieved great success with sequence-tosequence models Sutskever et al. (2014); Bahdanau et al. (2014); Cho et al. (2014) that used recurrent neural networks (RNNs) with LSTM cells Hochreiter and Schmidhuber (1997). The basic sequence-to-sequence architecture is composed of an RNN encoder which reads the source sentence one token at a time and transforms it into a fixed-sized state vector. This is followed by an RNN decoder, which generates the target sentence, one token at a time, from the state vector.While a pure sequence-to-sequence recurrent neural network can already obtain good translation results Sutskever et al. (2014); Cho et al. (2014), it suffers from the fact that the whole input sentence needs to be encoded into a single fixed-size vector. This clearly manifests itself in the degradation of translation quality on longer sentences and was partially overcome in Bahdanau et al. (2014) by using a neural model of attention. Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from Kalchbrenner and Blunsom (2013) and later in Meng et al. (2015). These early models used a standard RNN on top of the convolution to generate the output, which creates a bottleneck and hurts performance. Fully convolutional neural machine translation without this bottleneck was first achieved in Kaiser and Bengio (2016) and Kalchbrenner et al. (2016). The model in Kaiser and Bengio (2016) (Extended Neural GPU) used a recurrent stack of gated convolutional layers, while the model in Kalchbrenner et al. (2016) (ByteNet) did away with recursion and used left-padded convolutions in the decoder. This idea, introduced in WaveNet van den Oord et al. (2016), significantly improves efficiency of the model. The same technique was improved in a number of neural translation models recently, including Gehring et al. (2017) and Kaiser et al. (2017)."><td>4</td><td>Tensor2Tensor for Neural Machine Translation</td><td>0.492</td></tr><tr class="has-abs" data-abs="Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L 0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU. 1"><td>5</td><td>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</td><td>0.492</td></tr><tr class="has-abs" data-abs="Much recent effort has been invested in non-autoregressive neural machine translation, which appears to be an efficient alternative to state-of-the-art autoregressive machine translation on modern GPUs. In contrast to the latter, where generation is sequential, the former allows generation to be parallelized across target token positions. Some of the latest non-autoregressive models have achieved impressive translation quality-speed tradeoffs compared to autoregressive baselines. In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a single-layer autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. We show that the speed disadvantage for autoregressive baselines compared to non-autoregressive methods has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. Our results establish a new protocol for future research toward fast, accurate machine translation. Our code is available at https://github.com/jungokasai/deep-shallow.Recent work proposed methods to mitigate this multimodality issue, including iterative refinement (e.g.,Lee et al., 2018;Ghazvininejad et al., 2019), and modeling with latent variables (e.g.,Ma et al., 2019;Shu et al., 2020). These approaches modify the decoder transformer to find a balance between decoding parallelism and translation quality. In this work, however, we adopt a different speed-quality tradeoff. Recent work byKim et al. (2019)in autoregressive machine translation (AR) suggests that better speed-quality tradeoffs can be achieved by having different depths in the encoder and the decoder. Here, we make a formal argument in favor of deep encoder, shallow decoder configurations and empirically demonstrate better speed-quality tradeoffs for the AR baselines. * Work partially done at Facebook AI."><td>6</td><td>Published as a conference paper at ICLR 2021 DEEP ENCODER, SHALLOW DECODER: REEVALUATING NON-AUTOREGRESSIVE MACHINE TRANSLATION</td><td>0.498</td></tr><tr class="has-abs" data-abs="Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model ofBahdanau et al. (2014)by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality."><td>7</td><td>Neural Machine Translation with Recurrent Attention Modeling</td><td>0.501</td></tr><tr class="has-abs" data-abs="Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of languageindependent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a &quot;black-box&quot; manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as &quot;variance&quot; or &quot;confidence&quot;, and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly."><td>8</td><td>Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?</td><td>0.501</td></tr><tr class="has-abs" data-abs="Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs."><td>9</td><td>Scaling Neural Machine Translation</td><td>0.509</td></tr><tr class="has-abs" data-abs="The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connections and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified averagebased self-attention sublayer and the encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. 1"><td>10</td><td>Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention</td><td>0.510</td></tr><tr class="has-abs" data-abs="Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4."><td>11</td><td>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</td><td>0.512</td></tr><tr class="has-abs" data-abs="Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways. 17.32 (+0.45)"><td>12</td><td>Document-Level Neural Machine Translation with Hierarchical Attention Networks</td><td>0.515</td></tr><tr class="has-abs" data-abs="Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data. This work is licensed under a Creative Commons Attribution 4.0 International License. License details:"><td>13</td><td>Multilingual Neural Machine Translation with Task-Specific Attention</td><td>0.515</td></tr><tr class="has-abs" data-abs="The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned after training. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training, instead of doing so on a fully converged model. Our experiments on machine translation show that it is possible to remove up to three-quarters of all attention heads from a transformer-big model with an average −0.1 change in BLEU for Turkish→English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. The method is complementary to other approaches, such as teacher-student, with our English→German student losing 0.2 BLEU at 75% encoder attention sparsity."><td>14</td><td>Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation</td><td>0.515</td></tr><tr class="has-abs" data-abs="Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed -non-learnable -attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios."><td>15</td><td>Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation</td><td>0.516</td></tr><tr class="has-abs" data-abs="The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then outperformed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English→French and English→German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets."><td>16</td><td>The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation</td><td>0.518</td></tr><tr class="has-abs" data-abs="Recent work has questioned the importance of the Transformer's multi-headed attention for achieving high translation quality. We push further in this direction by developing a &quot;hardcoded&quot; attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models."><td>17</td><td>Hard-Coded Gaussian Attention for Neural Machine Translation</td><td>0.519</td></tr><tr class="has-abs" data-abs="We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.Preprint. Under review."><td>18</td><td>Scaling Laws for Neural Machine Translation</td><td>0.521</td></tr><tr class="has-abs" data-abs="Neural language models (NLMs) have been able to improve machine translation (MT) thanks to their ability to generalize well to long contexts. Despite recent successes of deep neural networks in speech and vision, the general practice in MT is to incorporate NLMs with only one or two hidden layers and there have not been clear results on whether having more layers helps. In this paper, we demonstrate that deep NLMs with three or four layers outperform those with fewer layers in terms of both the perplexity and the translation quality. We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts. When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points. 1 . 2013. Decoding with large-scale neural language models improves translation. In EMNLP."><td>19</td><td>Deep Neural Language Models for Machine Translation</td><td>0.524</td></tr><tr class="has-abs" data-abs="Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure. We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence. In this way, in addition to the topmost encoder layer (referred to as the primary view), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder to maintain independent prediction. Consistency regularization based on KL divergence is used to encourage the two views to learn from each other. Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model."><td>20</td><td>Layer-Wise Multi-View Learning for Neural Machine Translation</td><td>0.527</td></tr></tbody></table>
  </div>
  <div class="card">
    <h2>After&nbsp;(augmented)</h2>
    <table id="afterTbl"><thead><tr><th>#</th><th>Document&nbsp;/&nbsp;Title</th><th>Dist</th></tr></thead><tbody></tbody></table>
  </div>
</section>

<!-- METRICS ------------------------------------------------------------->
<section id="metricsCard" class="card" style="">
  <h2>Metrics</h2>
  <div id="recallLine" class="small">Recall </div>

  <div class="tabs">
    <button id="tab1" class="">Δ recall</button>
    <button id="tab2" class="active">Rank maps</button>
  </div>

  <div id="body1" class="tab-body active">
    <p id="deltaLine" class="small"></p>
  </div>
  <div id="body2" class="tab-body active">
    <h3 class="small" style="margin-top:0;">ranks_before</h3>
    <pre id="rb" class="small">{
  "216144650": 9999999999
}</pre>
    <h3 class="small">ranks_after</h3>
    <pre id="ra" class="small"></pre>
  </div>
</section>




```
</body></html>